# Earth-Economy Devstack

This is the documentation for the Earth-Economy Devstack, which is the set of repositories and code tools used by the Johnson-Polasky Lab and NatCap TEEMs. This documentation starts with overall organization of the Earth-Economy Devstack and discusses common coding practices used among the 5 underlying repositories. Next, each of the repositories is discussed in turn.

# Overall Organization

## Getting the earth_economy_devstack repository

Assuming you have already setup your python environment and installed VS Code, the first step is to clone all of the relevant repositories into the exact-right location. Throughout this documentation, we refer to "EE Spec", or Earth-Economy Specification. This is simply the common conventions for things like naming files and organizing them so that it works for all participants. The EE Spec organization is to clone the `earth_economy_devstack` repository, available at <https://github.com/jandrewjohnson/earth_economy_devstack>, into your Users directory in a subdirectory called Files. The PC version is shown below.

![](images/paste-23.png)

To clone here, you can use the command line, navigate to the Files directory and use `git clone https://github.com/jandrewjohnson/earth_economy_devstack` . Alternatively you could use VS Code's Command Pallate \<ctrl-shift-s\> Git Clone command and navigate to this repo. This repository configures VS Code to work well with the other EE repos and, for instance, defines launch-configurations that will always use the latest from github.

## Get the other repositories

Next, create a folder for each of the five repositories in the Files directory, as in the picture above.

1.  hazelbean
2.  seals
3.  gtappy
4.  gtap_invest
5.  global_invest

Inside each of these folders, you will clone the corresponding repositories:

1.  <https://github.com/jandrewjohnson/hazelbean_dev>
2.  <https://github.com/jandrewjohnson/seals_dev>
3.  <https://github.com/jandrewjohnson/gtappy_dev>
4.  <https://github.com/jandrewjohnson/gtap_invest_dev>
5.  <https://github.com/jandrewjohnson/global_invest_dev>

If successful, you will have a new folder with `_dev` postpended, indicating that it is the repository itself (and is the dev, i.e., not yet public, version of it). For GTAPPy it should look like this:

![](images/paste-30.png)

All code will be stored in the \_dev repository. All files that you will generate when running the libraries, conversely, will be in a different Projects directory as above (This will be discussed more in the ProjectFlow section).

## Launching the devstack in VS Code

Navigate to the earth_economy_devstack directory. In there, you will find a file `earth_economy_devstack.code-workspace` (pictured below).

![](images/paste-31.png)

Double click it to load a preconfigured VS Code Workspace. You will know you have it all working if the explorer tab in VS Code shows all all SIX of the repositories.

![](images/paste-32.png)

## Launch and VS Code Configurations

The earth_economy_devstack.code-workspace configures VS Code so that it includes all of the necessary repositories for EE code to run. Specifically, this means that if you pull the most recent version of each repository from Github, your code will all work together seamlessly. In addition to this file, you will see in the .vs_code directory there is a `launch.json` file. This file defines how to launch the python Debugger so that it uses the correct versions of the repositories. To launch a specific python file, first make it the active editor window, then open the debugger tab in VS Code's left navbar, and in the Run and Debug dropdown box (pictured below) select the first run configuration, then hit the green Play triangle to launch the python file you had open. If all is setup correctly, your file you ran should be able to import all of the libraries in the devstack.

![](images/paste-34.png)

# How to contribute

## Version management with Git & GitHub repository

SEALS is available as open source software at GitHub, which not only allows users to download and run the code but also to develop it collaboratively. GitHub is based on the free and open source distributed version control system **git** (<https://git-scm.com/).> **git** allows users to track changes in code development, to work on different code branches and to revert changes to previous code versions, if necessary. Workflows using the version control system **git** can vary and there generally is no 'right' or 'wrong' in collaborative software development. However, some basic preferred workflows are outlined in the following.

![](images/git_workflow.png)

### Create a new fork of the original SEALS repo

Before developing the code it is recommended to create a [fork](https://docs.github.com/en/get-started/quickstart/fork-a-repo) of the original SEALS repository. This will create a new code repository at your own GitHub profile, which includes the same code base and visibility settings as the original 'upstream' repository. The fork can now be used for - code development or fixes - submitting a pull request to the original SEALS repository.

### Clone fork to your local machine

Although GitHub allows for some basic code changes, it is recommended to [clone](https://docs.github.com/en/get-started/quickstart/fork-a-repo#cloning-your-forked-repository) the forked repository at your local machine and do code developments using an integrated development environment (IDE), such as [VS Code](https://code.visualstudio.com/). To clone the repository on your local machine via the command line, navigate to the local folder, in which you want to clone the repository, and type

```         
git clone -b <name-of-branch>` <url-to-github-repo> <name-of-local-clone-folder>
```

For example, if you want to clone the develop branch of your SEALS fork type

```         
git clone -b develop https://github.com/<username/seals_dev seals_develop
```

### Code changes

For any code development it is recommended to use the clone of the code base at your local machine and an IDE. For larger code developments, a new code branch should also be created by using `git branch <name-of-new-branch>`. To let git know that you want to switch to and work on the new branch use `git checkout <name-of-new-branch>`.

Before making any changes, make sure that your fork and/or your local repository are up-to-date with the original SEALS repository. To pull changes from your fork use `git pull origin <name-of-branch>`. In order to pull the latest changes from the original SEALS repository you can set up a link to the original upstream repository with the command `git remote add upstream https://github.com/jandrewjohnson/seals_dev`. To pull the latest changes from the develop branch in the original upstream repository use `git pull upstream develop`.

During code development, any changes or fixes that should go back to the fork and/or the original SEALS repository need to be ['staged'](https://docs.github.com/en/get-started/quickstart/contributing-to-projects#making-and-pushing-changes) and then ['commited'](https://docs.github.com/en/get-started/quickstart/contributing-to-projects#making-and-pushing-changes) with a commit message that succinctly describes the changes. By staging changes, **git** is informed that these changes should become part of the next 'commit', which essentially takes a snapshot of all staged changes thus far.

To stage all changes in the current repository use the command `git add .`. If you only want to stage changes in a certain file use `git add <(relative)-path-to-file-in-repo>`.

To commit all staged changes use the command `git commit -m "a short description of the code changes"`.

After committing the changes, they can be pushed to your fork by using `git push origin <name-of-branch>`.

![](images/git_branches.png) \### Pull requests

In order to propose changes to the original SEALS repository, it is recommended to use [pull requests](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request-from-a-fork). Pull requests inform other users about your code changes and allow them to review these changes by comparing and highlighting the parts of the code that have have been altered. They also highlight potential [merge conflicts](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/addressing-merge-conflicts/about-merge-conflicts), which occur when git is trying to merge branches with competing commits within the same parts of the code. Pull requests can be done directly at GitHub by navigating to either the forked repository or the original SEALS repository and by clicking on 'Pull requests' and then 'New pull request'. This will open a new pull request page where changes can be described in more detail and code reviewers can be specfied. Pull requests should be reviewed by at least one of the main code developers.

## Iterating over many model assumptions

In many case, such as a standard GTAPPy run, we will iterate over different aggergations and scenarios (now renamed counterfactuals). This is done as expected with code like this:

```{python}
for aggregation_label in p.aggregation_labels:
     
    for experiment_label in p.experiment_labels:
        
        for n_years_counter, ending_year in enumerate(p.years):
            
            if n_years_counter == 0:
                starting_year = p.base_year
            else:
                starting_year = p.years[n_years_counter - 1]
                
            output_dir = p.get_path(os.path.join(aggregation_label, experiment_label, str(ending_year)))
```

But sometimes, this becomes DEEPLY nested and confusing. SEALS implements an API that reads these nested layers from a CSV. This is defined more fully in the SEALS user guide.

```{python}
for index, row in p.scenarios_df.iterrows():
    seals_utils.assign_df_row_to_object_attributes(p, row)
    
    if p.scenario_type != 'baseline':
                            
        for n_years_counter, ending_year in enumerate(p.years):

            if n_years_counter == 0:
                starting_year = p.base_year
            else:
                starting_year = p.years[n_years_counter - 1]
                
            current_run_dirs = os.path.join(p.exogenous_label, p.climate_label, p.model_label, p.counterfactual_label)
            output_dir = p.get_path(current_run_dirs, str(ending_year))
            expected_sl4_path = os.path.join(output_dir, p.counterfactual_label + '_Y' + str(ending_year) + '.sl4')

```

In this scenarios_df, which was loaded from scenarios_csv_path, there are multiple nested for loops implied, for `p.exogenous_label, p.climate_label, p.model_label, p.counterfactual_label`, each row has a unique value that would have been iterated over with the for loop above. Now, however, we are iterating just over scenario_df rows. Within each row pass, a project-level attribute is assigned via `seals_utils.assign_df_row_to_object_attributes(p, row)`. This is used instead of the nested for loop.

## Creating scenarios spreadsheets

Here. Explain why it writes the scenarios_csv FROM CODE rather than downloading it (keeps it up to date as code changes quickly). However, this gets convoluted when you also have to initialize the attributes before you write?!?

```{python}
    # If you want to run SEALS with the run.py file in a different directory (ie in the project dir)
    # then you need to add the path to the seals directory to the system path.
    custom_seals_path = None
    if custom_seals_path is not None: # G:/My Drive/Files/Research/seals/seals_dev/seals
        sys.path.insert(0, custom_seals_path)

    # SEALS will run based on the scenarios defined in a scenario_definitions.csv
    # If you have not run SEALS before, SEALS will generate it in your project's input_dir.
    # A useful way to get started is to to run SEALS on the test data without modification
    # and then edit the scenario_definitions.csv to your project needs.
    # Some of the other test files use different scenario definition csvs 
    # to illustrate the technique. If you point to one of these 
    # (or any one CSV that already exists), SEALS will not generate a new one.
    # The avalable example files in the default_inputs include:
    # - test_three_scenario_defininitions.csv
    # - test_scenario_defininitions_multi_coeffs.csvs
    
    p.scenario_definitions_path = os.path.join(p.input_dir, 'scenario_defininitions.csv')

    # Set defaults and generate the scenario_definitions.csv if it doesn't exist.
    if not hb.path_exists(p.scenario_definitions_path):
        # There are several possibilities for what you might want to set as the default.
        # Choose accordingly by uncommenting your desired one. The set of
        # supported options are
        # - set_attributes_to_dynamic_default (primary one)
        # - set_attributes_to_dynamic_many_year_default
        # - set_attributes_to_default # Deprecated

        gtap_invest_utils.set_attributes_to_dynamic_gtap_default(p) # Default option


        # # Optional overrides for us in intitla scenarios
        # p.aoi = 'RWA'

        # gtap_invest_utils.set_attributes_to_dynamic_default(p)
        # Once the attributes are set, generate the scenarios csv and put it in the input_dir.
        gtap_invest_utils.generate_gtap_invest_scenarios_csv_and_put_in_input_dir(p)
        p.scenarios_df = pd.read_csv(p.scenario_definitions_path)
    else:
        # Read in the scenarios csv and assign the first row to the attributes of this object (in order to setup additional 
        # project attributes like the resolutions of the fine scale and coarse scale data)
        p.scenarios_df = pd.read_csv(p.scenario_definitions_path)

        # Because we've only read the scenarios file, set the attributes
        # to what is in the first row.
        for index, row in p.scenarios_df.iterrows():
            seals_utils.assign_df_row_to_object_attributes(p, row)
            break # Just get first for initialization.

```

## Task Format

Project flow requires a consistent format for tasks. The following is an example of a task that creates a correspondence file from gtap11 regions to gtapaez11 regions. The task itself defined as a function that takes a `p` object as an argument. This `p` object is a `ProjectFlow` object that contains all the project-level variables, manages folders and files, and manages tasks and parallelization. `p` also includes documentation, which will be written directly into the task directory.

Also note that any project-level attribute defined in between the function start and the `if p.run_this:` component are the "project level variables" that are fair-game for use in other tasks. These paths are critical for high performance because they enable quick-skipping of completed tasks and determiniation of which parts of the task tree need rerunning.

Tasks should be named as a noun (this breaks Python pep8 style) referencing what will be stored in the tasks output dir. This might feel awkward at first, but it means that the resultant file structure is easier to interpret by a non-EE outsider.

```{python}
def gtap_aez_seals_correspondences(p):
    p.current_task_documentation = """
    Create correspondence CSVs from ISO3 countries to GTAPv11 160
    regions, and then to gtapaezv11 50ish regions, also put the classification
    for seals simplification and luh.  
    """
    p.gtap11_region_correspondence_input_path = os.path.join(p.base_data_dir, 'gtappy', 'aggregation_mappings', 'GTAP-ctry2reg.xlsx')
    p.gtap11_region_names_path = os.path.join(p.base_data_dir, 'gtappy', 'aggregation_mappings', 'gtap11_region_names.csv')
    p.gtap11_gtapaez11_region_correspondence_path = os.path.join(p.base_data_dir, 'gtappy', 'aggregation_mappings', 'gtap11_gtapaez11_region_correspondance.csv')    

    if p.run_this:
        
        "logic here"
```

## Automatic Directory Organization via Tasks

Hazelbean automatically defines directory organization as a function of the task tree. When the ProjectFlow object is created, it takes a directory as its only required input. This directory defines the root of the project. The other directory that needs to be referenced is the base_data_dir. When you initialize the p object, it notes this:

Created ProjectFlow object at C:\\Users\\jajohns\\Files\\gtap_invest\\projects\\cwon from script C:\\Users\\jajohns\\Files\\gtap_invest\\gtap_invest_dev\\gtap_invest\\run_cwon.py with base_data set at C:\\Users\\jajohns\\Files/base_data

`Created ProjectFlow object at C:\Users\jajohns\Files\gtap_invest\projects\cwon     from script C:\Users\jajohns\Files\gtap_invest\gtap_invest_dev\gtap_invest\run_cwon.py     with base_data set at C:\Users\jajohns\Files/base_data`

In the run file, the following line generates the task tree:

`gtap_invest_initialize_project.build_extract_and_run_aez_seals_task_tree(p)`

Which points to a builder function in the initialize file, looking something like this:

![](images/paste-28.png)

This would generate the following task tree:

![](images/paste-27.png)

Two notations are especially useful within this task tree.

1.  Within the function that defines a task, p.cur_dir points to the directory of that task. So for instance, the last task defined in the image above, in its code, you could reference p.cur_dir, and it would point to `<project_root>/econ_vizualization/econ_lcovercom`
2.  Outside of a given function's code, you can still refer to paths that were defined from within the functions code, but now (because you are outside the function) it is given a new reference. Using the example above, you could reference the same directory with `p.econ_lcovercom_dir` where the p attribute is named exactly as \<function_name\>\_dir

All of this setup enable another useful feature: automatic management of file generation, storage and downloading. This is done via the hazelbean function:

```{python, eval=FALSE}
useful_path = hb.get_path(relative_path)
```

This function will iteratively search multiple locations and return the most "useful" one. By default, the relative_path variable will first joined with the p.cur_dir. If the file exists, it returns it. If not, it checks the next location, which is p.input_dir, and then p.base_data_dir. If it doesn't find it anywhere, it will attempt to download it from google cloud (NYI) and save it in the p.cur_dir. If it is not available to download on google cloud, then it treats the path as something we will be generating within the task, and thus, get_path returns the first option above, namely joining the relative_path with p.cur_dir.

One important use-case that needs explaining is for tasks that generate files that will eventually be placed in the base_data_dir. The goal is to enable easy generation of it to the intermediate directory in the appropriate task_dir, but then have the ability to copy the files with the exact same relative path to the base_data_dir and have it still be found by p.get_path(). To do this, you will want to choose a path name relative to the tasks' cur_dir that matches the desired directory relative to the base data dir. So, for example, we include `'gtappy', 'aggregation_mappings'` at the beginning of the relative path for in the intermediate directory in the appropriate task_dir, but then we also will want to copy the files with the exact same relative path to the base_data_dir and have it still be found by p.get_path(). To do this, you will want to choose a path name relative to the tasks' cur_dir that matches that in the base_data_dir, for example `<base_data_dir>/'gtappy/aggregation_mappings/gadm_adm0.gpkg',`

```{python}
template_path = p.get_path(os.path.join('gtappy', 'aggregation_mappings', 'gadm_adm0.gpkg')) 
```

It can be hard deciding what counts as a base_data_generating task or not, but generally if it is a file that will not be used by other projects, you should not treat it as a base_data_generating task. Instead, you should just make it relative to the cur_dir (or wahtever makes sense), as below:

```{python}
output_path = p.get_path(os.path.join(aggregation_label + '_' + experiment_label + '_' + header + '_stacked_time_series.csv'))
```

One additional exception to the above is if you are calling get_path outside of a task/task_tree. One common example is in the run file before you build the task tree. In this case, the default_dirs will not make sense, and so you need to specify it manually as here:

```{python, eval=FALSE}
p.countries_iso3_path = p.get_path(os.path.join('cartographic', 'gadm', 'gadm_adm0_10sec.gpkg'), possible_dirs=[p.input_dir, p.base_data_dir])
```

## Validation of files

ProjectFlow is designed to calculate very fast while simultaneously validating that everything is approximately correct. It does this by checking for the existence of files (often combined with hb.get_path()). For example

```{python}
p.gadm_r263_gtapv7_r251_r160_r50_correspondence_vector_path = p.get_path(os.path.join('gtap_invest', 'region_boundaries', 'gadm_r263_gtapv7_r251_r160_r50_regions.gpkg'))     
if not hb.path_exists(p.gadm_r263_gtapv7_r251_r160_r50_correspondence_vector_path):         
    hb.log('Creating ' + p.gadm_r263_gtapv7_r251_r160_r50_correspondence_vector_path)         
    "computationally expensive thing here."
```

ProjectFlow very carefully defines whether or not you should run something based on the existence of specific paths. Usually this is just checking for each written path and only executing the code if it's missing, but in some cases where lots of files are created, it's possible to take the shortcut of just checking for the existence of the last-created path.

### Eliminating redundant calculation across projects

If you have a time consuming task that, or example, writes to

```{python}
big_file_path = hb.get_path('lulc', 'esa', 'seals7', 'convolutions', '2017', 'convolution_esa_seals7_2017_cropland_gaussian_5.tif' )
```

In this example, suppose you needed to create this file via your create_convolutions() task or something. When you first do this, it obviously won't exist yet, so get_path() will join that relative path in the p.cur_dir location. If you run the ProjectFlow again, it will see it's there and then instantly skip recalcualting it.

In addition to the 5 repos plus the EE repo, there is a managed base data set stored in teh same location

![](images/paste-25.png)

![](images/paste-26.png)

### Python Tips and Conventions

For large files that take a long time to load, use a string-\>dataframe/dataset substitution as below. Make a LOCAL variable to contain the loaded object, and have that be assigned to the correct project-level path string. In subsequent usages, check type and if it's still a string, then it hasn't been loaded yet, so do that. I'm debating making it a project level variable trick

```{python}

gadm = p.gadm_adm0_vector_input_path    

# Just load it on first pass
if type(gadm) is str:
    gadm = hb.read_vector(p.gadm_adm0_vector_input_path)
```

# GTAPPy

## Installation

### Cython Installation

![](images/paste-3.png)

![](images/paste-4.png)

![](images/paste-5.png)

![](images/paste-6.png)

![](images/paste-8.png)

![](images/paste-9.png)

![](images/paste-10.png)

### Run-GTAP installation simple for class

Go to <https://www.gtap.agecon.purdue.edu/databases/download.asp>

Follow step 1: 

Install GTAPAgg2: <https://www.gtap.agecon.purdue.edu/private/secured.asp?Sec_ID=1055>

Step 2: extract the .lic file from <https://www.gtap.agecon.purdue.edu/databases/download.asp> step 2. Put this in GTAPAgg2 dir

Install the GTAP Database itself from step 3 at : <https://www.gtap.agecon.purdue.edu/databases/download.asp>

Make sure to also get the AEZ version, which will also give a full pkg file.

![](images/paste-11.png)

Put this .pkg file in GTPAg2 dir. 

Launch GTPAg2, identify pkg file for both default and AEZ data

When running aggregations, make sure to choose the right product:

![](images/paste-12.png)

Install RunGTAP at https://www.gtap.agecon.purdue.edu/products/rungtap/default.asp

At <https://www.copsmodels.com/gpeidl.htm> download: <https://www.copsmodels.com/ftp/ei12dl/gpei-12.1.004-install.exe> 

OR FOR VERSION 12: https://www.copsmodels.com/ftp/ei12dl/gpei-12.0.004-install.exe

Install to default location

![](images/paste-13.png)

Proceed without selecting a license (to start the 6 month trial).

Running the unaggregated version

-   Open GTPAg2.exe

![](images/paste-14.png)

-   Use this to aggregate a 1-1 version.

    -   Create a 1-1 mapping

        -   View change regional aggregation, sector aggregation, setting to 1:1

        -   For factor aggregation, it defaults to 8-5, can set this to 8:8.

    -   Read aggregation scheme from file, loa default.agg is 10 by 10

    -   For factors, if using AEZ, there is land specified by each AEZ. Erwin is working to fix this. Because didn’t have it ready, went back to standard GTAP package of data.

    -   Used full 8 factors, make new things “mobile factors”.

<!-- -->

-   Save aggregation scheme to file to AggStore

![](images/paste-15.png)

Save aggregation scheme to file to AggStore

![](images/paste-17.png)

This could also be done by making the .agg file via text editing. 

-   Then finally Create aggregated database in GTAPAgg

![](images/paste-18.png)

Note that this creates two versions of the database, one for GTAP code v6.2, one for v7.0. The 7.0 is in a sub zip folder gtapv7.zip. This is the one we want.

“C:\\GTPAg2\\AggStore\\GTAP10A_GTAP_2014_65x141\\gtapv7”

**Getting the v7 code**

-   Extract from RunGTAP (lol)

-   Under Version, Change, set to NCORS3x3

-   Under Version, New, use wizard using same aggregation and simply copying.

![](images/paste-19.png)

-   This will create a new folder in c:/runGTAP375 named v7all.

-   Now we will replace the data files in v7all with the fully disaggregated database (in the v7 dir) we created above.

-   Notice also in the v7dir we have shock files, e.g. tinc.shk. 

    -   By default, these will be inherited from the old 3x3 version.

    -   Under Tools, Run Test Simulation. This will rewrite new shk files to match aggregation.

        -   ALSO NOTE, this is the full run that tested it all worked.

    -   To check that it worked, go to results, macros. 

    -   Or, could open results in ViewSOL. Use View -\> Results Using ViewSOL

        -   ViewSOL has the full results, whereas RunGTAP only has a subset by the limited mapping specific to RunGTAP. ViewSOL loads the whole sl4 file.

        -   Here you could go to e.g. pgdp to see that prices all went up by 10%, which is the default shock i guess?

    -   OR OR, from ViewSOL File, can open in ViewHAR, which gives even more power, such as dimensional sorting.

-   Now we can run a non-trivial shock. So for global ag productivity shock, let’s increase productivity.

    -   In RunGTAP, go to view RunCMFSTART file. Here we will define a new set for the new experiments. (CMFSTART files sets settings for runtime, but also which are the files that should be called, along with sets for the closure).

        -   To do this, go to RuinGTAP, View, Sets, enable advanced editing, Sets, View Set Library

            -   Here click on COMM, copy elements in tablo format). This will get the set definition in tablo format of ALL the commodities. From this you can pare down to which you want to shock,

            -   For the moment, we will create two sets, one for ag commodities, and one for a smaller subset of agg commodities.

            -   And will also define wha

![](images/paste-20.png)

Now that the sets are defined, can use them in defining the SHOCKS

![](images/paste-21.png)

-   Set the solution method to gragg 2-4-6

-   Save experiment.

-   Then Solve!

    -   Comment from TOM: If Gragg doesn’t work, use Euler with many steps 20-50

**From second call with Erwin on cmd and rungtap versions**

-   Note that now we have a tax, we do it on both domestic and imported, then write a new equation that makes them sum up: 

    -   E_tpm (all, c, COMM)(all, r, REG) tmp(c, r) = tpmall + tpreg + tpall

-   Note that we now set the rate% of beef tax to increase 50%. Before we were increasing it by a 50% POWER OF THE tariff.

-   First run testsim.bat.

-   Then run the simulations.

-   In rungtapv7.bat then, 

-   Simresults.bat last. Takes the sl4 files and converts it into a har file (.sol), and then combines them into 1 file, then splits to CSV.

-   RMAC is full dimension, RMCA is aggregated by chosen aggregation simplification.

    -   AggMap.har defines the aggregation.

-   Them Allres.CMF is run, which actually does the aggregation.

    -   Allres.cmf calls to allres.tab, which cleans the results. This would need to have new scenarios added to the top

-   Allres.EXP also needs to be updated, along with the scenario EXP files to h

-   ave thecorrect exogenous variables, such as tpdall

**alternatively just specify they use hb.get_path() to the private database.**

## Iterating over multiple aggregations

GTAPPY runs for multiple aggregations and follows the philosophy that for a well-built model, the results will be intuitively similar for different aggregations and thus it serves as a decent check. This is similar to "leave-one-out" testing in regression.

## Release notes

### v2023-12-18

We need to clarify how we go from a erwin-style mapping xlsx to EE spec. Below is what it looks like as downloaded from erwin.

GTAP-ctry2reg \[source from erwin converted to EE spec\].xlsx

![](images/paste-1.png)

Manually renamed to gtapv7_r251_r160_correspondence.xlsx (must be excel cause multi workbook). Also need to put the legend information somewhere else in a well-thought-out place (not attached to regions correspondnce)

Note that eg No. is used twice where the first is r160 and the second is r251. New input mapping should clarify

Similar for sectors

![](images/paste-2.png)

First note, the word "sector" is specific to the case when you aren't specifying if you're talking about COMM (commodities) or ACTS (activities) because I'm not quite sure of the differentiation at this point.

Notes from Erwin on GTAPPY

### v2023-12-15

Issues resolved:

1.  In the release's Data folder, the aggregation label gtapaez11-50 should be renamed v11-s26-r50, correct?

2.  Also note that I have decided to have the most recent release always have NO timestamp whereas dated versions of same-named models/aggregations should add on the timestamp of when they were first released

3.  Propose changing the names of the cmf files from cwon_bau.cmf to gtapv7-aez-rd_bau.cmf and cwon_bau-es.cmf to gtapv7-aez-rd_bau-es (note difference between hyphens (which imply same-variable) and underscores, which are used to split into list.)

4.  Propose not using set PROJ=cwon in the CMF as that is defined by the projectflow object.

5.  propose changing SIMRUN to just 'experiment_name' ie "bau" rather than "projname" + "bau"

6.  Reorganize this so that data is in the base_data_dir and the output is separated from the code release" set MODd=..\mod set CMFd=.\cmf

set SOLd=..\out set DATd=..\data%AGG%

THESE TWO STAY OUTSIDE THE RELEASE

7.  This basic idea now is that there is a release that someone downloads, and they could run it either by calling the bat file or by calling the python run script. This means i'm trying to make the two different file types as similar as possible. However, note that the bat file is only going to be able to replicate a rd run without ES, so technically the python script can contain a bat file but not vice versa.

8.  Renamce command line cmf options as tehy're referenced in the cmf file: \# CMF: experiment_label \# Rename BUT I understand this one might not be changeable because it appears to be defined by the filename of the CMF? \# p1: gtap_base_data_dir \# p2: starting_data_file_path \# Rename points to the correct starting har \# p3: output_dir \# Rename \# p4: starting_year \# Rename \# p5: ending_year \# Rename

9.  Simple question: Is there any way to read the raw GEMPACK output to get a sense of how close to complete you are? I would like to make an approximate progress bar.

-   When you say "automatic accuracy", you can.

    +++\> Beginning subinterval number 4.

    ---\> Beginning pass number 1 of 2-pass calculation, subinterval 4.

    Beginning pass number 6 of 6-pass calculation, subinterval 6

10. Would it be possible to not put a Y in front of years like Y2018? This can mess up string-\>int conversions.

keep Y, gempack can't have non-numeric characters at the start of a var

11. There is no bau-SUM_Y2050 (but ther is for VOL and WEL). Is this intentional?

NO! SUM describes the starting database.

Welfare not possibly in RD because no discount rate eg

12. Question: Is EVERYTHING stored in the SL4? I.e., are the other files redundant?

No, apparently things like the converting the sl4 to volume terms is not stored in the sl4, so that needs to come in on sltht or in ViewSOL

### v2023-11-01

#### Shocks implemented

1.  Different population file replacements
2.  GDP change
3.  Yield changes?

#### Example run file

![](images/2022-12-12-10-31-05.png)

At the top we create the project flow object. We will add tasks to this.

#### Example executable call

![](images/2022-12-12-10-33-44.png)

#### Harder part to do is figuring out how to have the shocks work

Need to create xSets, xSubsets, Shock. Could use READFROMFILE command in tablo cmf.

#### What are the different kinds of shocks

**Uniform** Shockaoall(AGCOM_SM, reg) = uniform 20;

Another more

aoall(ACTS, REG) = file select from file pointing to a list of all regions. 0 is no shock.

Everything in the exogenous list can be shocks.

Also can SWAP an initially endogenous with exogenous.

E.g. swap aoreg with GDP

What about changing an elasticity?

those are in gtapparm, so write a new .prm (this is not a shock but is just replacing an input.)

Notice that there are shocks vs data updates.

The elasticities are being calibrated??? if you change the basedata to basedata2, then a different default2.prm, with no shock, the model will replicate the initial database.

If it's a supply response elasticity (as in PNAS) that WILL affect it (unlike above).

needs to be percentage change over default POP. In base data. Read this in and process it against Eric's file.

Shock pop(REG) = SELECT FROM FILE filename header 4ltr header. Swap QGDP aoreg shock aoall (agcom_sm, reg) = select from yield file.

# SEALS

## From Readme (might be redundant)

SEALS is under active development and will change much as it moves from a personal research library to supported model. We have submitted this code for publication but are waiting on reviews. For installation and other details, see the [SEALS documentation](https://justinandrewjohnson.com/seals/documentation).

To run a minimal version of the model, open a terminal/console and navigate to the directory where run_test_seals.py is located. Then, simply run: \> python run_test_seals.py

In order for the above line to work, you will need to set the project directory and data directory lines in run_test_seals.py. To obtain the base_data necessary, see the SEALS manuscript for the download link.

To run a full version of the model, copy run_test_seals.py to a new file (i.e., run_seals.py) and set p.test_mode = False. You may also want to specify a new project directory to keep different runs separate.

## Release Notes

### Update v0.5.0

Downloading of base data now works.

### Update v0.4.0

Now all project flow objects can be set via a scenario_definitions.csv file, allowing for iteration over multiple projects.

If no scenario_definitions.csv is present, it will create the file based on the parameters set in the run file.

## Installation

-   Install Mambaforge from <https://github.com/conda-forge/miniforge#mambaforge>
-   For convenience, during installation, I select yes for "Add Mambaforge to my PATH environment Variable"
-   (PC) Open the Miniforge Prompt (search for it in the start menu) or (Mac) just type "mamba init"
-   Create a new mamba environment with the following commands (here it is named env2023a):

`mamba create -n env2023a -c conda-forge`

-   Activate the environment

`mamba activate research2023a`

-   Install libraries using the mamba command:

`mamba install -c conda-forge natcap.invest geopandas rasterstats netCDF4 cartopy xlrd markdown qtpy qtawesome plotly descartes pygeoprocessing taskgraph cython rioxarray dask google-cloud-datastore google-cloud-storage aenum anytree statsmodels openpyxl seaborn twine pyqt ipykernel imageio pandoc conda numba intake more-itertools`

-   And then finally, install non-conda distributions via pip:

`pip install mglearn pandoc datascience hazelbean`

## Numpy errors

If numpy throws "wrong size or changes size binary": upgrade numpy at the end of the installation process. See for details: <https://stackoverflow.com/questions/66060487/valueerror-numpy-ndarray-size-changed-may-indicate-binary-incompatibility-exp>

## Mac specific errors

Your python environment has to have permissions to access and write to the base data folder.

## Hazelbean

SEALS relies on the Hazelbean project, available via PIP. Hazelbean is a collection of geospatial processing tools based on gdal, numpy, scipy, cython, pygeoprocessing, taskgraph, natcap.invest, geopandas and many others to assist in common spatial analysis tasks in sustainability science, ecosystem service assessment, global integrated modelling assessment, natural capital accounting, and/or calculable general equilibrium modelling.

Note that for hazelbean to work, your computer will need to be configured to compile Cython files to C code. This workflow is tested in a Python 3.10, 64 bit Windows environment. It should work on other system environments, but this is not yet tested.

## Project Flow

One key component of Hazelbean is that it manages directories, base_data, etc. using a concept called ProjectFlow. ProjectFlow defines a tree of tasks that can easily be run in parallel where needed and keeping track of task-dependencies. ProjectFlow borrows heavily in concept (though not in code) from the task_graph library produced by Rich Sharp but adds a predefined file structure suited to research and exploration tasks.

## First run walkthrough tutorial

The simplest way to run SEALS is to clone the repository and then open run_test_seals.py in your preferred editor. Then, update the values in ENVIRONMENT SETTINGS near the top of run_test_seals.py for your local computer (ensuring this points to directories you have write-access for and is not a virtual/cloud directory).

``` python
    ### ------- ENVIRONMENT SETTINGS -------------------------------

    # Users should only need to edit lines in this ENVIRONMENT SETTINGS section
    # Everything is relative to these (or the source code dir).
    # Specifically,
    # 1. ensure that the project_dir makes sense for your machine
    # 2. ensure that the base_data_dir makes sense for your machine
    # 3. ensure that the data_credentials_path points to a valid credentials file
    # 4. ensure that the input_bucket_name points to a cloud bucket you have access to

    # A ProjectFlow object is created from the Hazelbean library to organize directories and enable parallel processing.
    # project-level variables are assigned as attributes to the p object (such as in p.base_data_dir = ... below)
    # The only agrument for a project flow object is where the project directory is relative to the current_working_directory.
    user_dir = os.path.expanduser('~')
    script_dir = os.path.dirname(os.path.realpath(__file__))

    project_name = 'test_seals_project'
    project_dir = os.path.join(user_dir,  'seals', 'projects', project_name)
    p = hb.ProjectFlow(project_dir)
```

The project name and the project dir will define the root directory where all files will be saved. This directory is given `hb.ProjectFlow()` to initalize the project (which will create the dirs). Once these are set, you should be able to run run_test_seals.py in your preferred way, ensuring that you are in the Conda environment discussed above. This could be achieved in VS Code by selecting the Conda environment in the bottom-right status bar and then selecting run. Alternatively, this could be done via the command line with the command `python run_test_seals.py` in the appropriate directory.

When SEALS is run in this way, it will use the default values for a test run on a small country (Rawanda). All of these values are set (and documented) in the run file (run_test_seals.py) in the SET DEFAULT VARIABLES section. For your first run, it is recommended to use the defaults. When run, a configuration file will be written into your project's input_dir named scenario_definitions.csv. This file is a table where each row is a "scenario" necessary to be defined for SEALS to run. In this minimal run, it must have 2 rows: one for the baseline condition (the starting LULC map) and one for a scenario of change that will indicate how much change of each LU class will happen in some coarse grid-cell or region/zone. Inspecting and/or modifying this file may give insights on how to customize a new run.

``` python
### ------- SET DEFAULT VARIABLES --------------------------------

# Set the path to the scenario definitions file. This is a CSV file that defines the scenarios to run.
# If this file exists, it will load all of the attributes from this file and overwrite the attributes
# set above. This is useful because adding new lines to to the scenario definitions file will allow
# you to run many different scenarios easily. If this file does not exist, it will be created based
# on the attributes set above and saved to the location in scenarios_definitions_path.
p.scenario_definitions_path = os.path.join(p.input_dir, 'scenario_defininitions.csv')

# IMPORTANT NOTE: If you set a scenario_definitions_path, then the attributes set in this file (such as p.scenario_label below)
# will be overwritten. Conversely, if you don't set a scenario_definitions_path, then the attributes set in this file will be used
# and will be written to a CSV file in your project's input dir.

# If you did not set a p.scenarios_definitions_path, the following default variables will be used
# and will be written to a scenarios csv in your project's input_dir for later use/editing/expansion.

# String that uniquely identifies the scenario. Will be referenced by other scenarios for comparison.
p.scenario_label = 'ssp2_rcp45_luh2-globio_bau'

# Scenario type determines if it is historical (baseline) or future (anything else) as well
# as what the scenario should be compared against. I.e., Policy minus BAU.
p.scenario_type = 'bau'
```

This computing stack also uses hazelbean to automatically download needed data at run time. In the code block below, notice the absolute path assigned to p.base_data_dir. Hazelbean will look here for certain files that are necessary and will download them from a cloud bucket if they are not present. This also lets you use the same base data across different projects.

In addition to defining a base_data_dir, you will need to For this to work, you need to also point SEALS to the correct data_credentials_path. If you don't have a credentils file, email jajohns\@umn.edu. The data are freely available but are very, very large (and thus expensive to host), so I limit access via credentials.

``` python
p.base_data_dir = os.path.join('G:/My Drive/Files/base_data')

p.data_credentials_path = '..\\api_key_credentials.json'
```

NOTE THAT the final directory has to be named base_data to match the naming convention on the google cloud bucket.

## Running the model

After doing the above steps, you should be ready to run `run_test_seals.py`. Upon starting, SEALS will report the "task tree" of steps that it will compute in the ProjectFlow environment. To understand SEALS in more depth, inspect each of the functions that define these tasks for more documention in the code.

Once the model is complete, go to your project directory, and then the intermediate directory. There you will see one directory for each of the tasks in the task tree. To get the final produce, go to the stitched_lulc_simplified_scenarios directory. There you will see the base_year lulc and the newly projected lulc map for the future year:

![](images/paste-3.png)

Open up the projected one (e.g., lulc_ssp2_rcp45_luh2-message_bau_2045.tif) in QGIS and enjoy your new, high-resolution land-use change projection!

## More SEALS details

### Linking paths

The logic used for paths in SEALS is confusing at first, but it enables efficient processing of very large files (and minimizes re-downloading or re-processing data. When a path to a file is specified in SEALS, often SEALS will call a function in hazelbean that checks multiple places for that file to exist, as in the code below.

``` python
path = hb.get_first_extant_path(path, [p.input_dir, p.base_data_dir])
```

If the path given is relative, this function will then iterate through the list of possible directories and join the path to each directory, continuing down the list until it finds an existing path. If it never finds it, the function will return the path joined with the first element in the list (presumably for later file creation). This functionality means that if a file is in your base data directory, hazelbean will not download it. As a protip, after the first time it's downloaded, you can move it from the input_dir to your base_data_dir (while keeping the exact same relative directory path) and then Hazelbean will not waste time re-downloading.

# Hazelbean

## Quickstart

Test that hazelbean imports correctly. Importing it will trigger compilation of the C files if they are not there.

``` python
import hazelbean as hb
```

## Library of useful functions

Explore examples of the useful spatial, statistical and economic functions in the examples section of this documentation.

## Project Flow

One key component of Hazelbean is that it manages directories, base_data, etc. using a concept called ProjectFlow. ProjectFlow defines a tree of tasks that can easily be run in parallel where needed and keeping track of task-dependencies. ProjectFlow borrows heavily in concept (though not in code) from the task_graph library produced by Rich Sharp but adds a predefined file structure suited to research and exploration tasks.

### Project Flow notes

Project Flow is intended to flow easily into the situation where you have coded a script that grows and grows until you think "oops, I should really make this modular." Thus, it has several modalities useful to researchers ranging from simple drop-in solution to complex scripting framework.

#### Notes

In run.py, initialize the project flow object. This is the only place where user supplied (possibly absolute but can be relative) path is stated. The p ProjectFlow object is the one global variable used throughout all parts of hazelbean.

``` python
import hazelbean as hb

if __name__ == '__main__':
    p = hb.ProjectFlow(r'C:\Files\Research\cge\gtap_invest\projects\feedback_policies_and_tipping_points')
```

In a multi-file setup, in the run.py you will need to import different scripts, such as main.py i.e.:

``` python
import visualizations.main
```

The script file mainpy can have whatever code, but in particular can include "task" functions. A task function, shown below, takes only p as an agrument and returns p (potentially modified). It also must have a conditional (if p.run_this:) to specify what always runs (and is assumed to run trivially fast, i.e., to specify file paths) just by nature of having it in the task tree and what is run only conditionally (based on the task.run attribute, or optionally based on satisfying a completed function.)

``` python
def example_task_function(p):
    """Fast function that creates several tiny geotiffs of gaussian-like kernels for later use in ffn_convolve."""

    if p.run_this:
        for i in computationally_intensive_loop:
            print(i)
```

**Important Non-Obvious Note**

Importing the script will define function(s) to add "tasks", which take the ProjectFlow object as an argument and returns it after potential modification.

``` python
def add_all_tasks_to_task_tree(p):
    p.generated_kernels_task = p.add_task(example_task_function)
```

# Global InVEST

NYI

# GTAP-InVEST

NYI.

# Naming conventions

### Id, index, label, name, description

In programming, `id` and `index` are two different concepts that are used in different contexts.

`id` refers to the unique identifier of an object. In this specification, it is an integer that is sorted by the ONE in the many-to-one correspondence, sorted alphabetically at generation time (though not necessarily to remain sorted given downstream correspondences).

`index` refers to the position of an element in a sequence (e.g. a list or a string). In the context of a correspondence file, this is the position of the row within the sorted spreadsheet, but is not assumed to be stable and shouldn't generally be used.

`labelheader` refers to an (exactly) 4 character string that is lowercase-alphanumeric with no special symbols besides underscore. Useful for the Header label in har files. Technically is case insensitive but we assume lowercase.

`labelshort` refers to an 8-character or less string that is lowercase-alphanumeric with no special symbols besides underscore. Useful for .har files.

`label` refers to a string that is lowercase-alphanumeric with no special symbols besides underscore

`name` refers to a string of any ascii characters with python-style escaping of special characters: `'She\'s a star!'` . It's assumed to be short enough to view as a column header or plot label

`Description`refers to a `name` of any length with detailed description, possibly even formatted MD text.

If there is a domain, described below, id, index, etc all should prepended with it to be eg gadm_id.

### Id, Index etc. in the context of vector data

Note that geopandas assumes the vector data are indexed with an FID. This is the order in which the geometries are added to the file and can get wonky when dealing with legacy file types (like ESRI Shapefiles). Additionally, when you write a GPKG to a CSV, it will not include the fid, so you might lose data. To fix this, EE spec requires that any GPKG when saved as a CSV have a new column, `id` added as the first col, which is generated starting at 1 and incrementing up by 1 after having sorted the data on the simplest non-fid label (e.g., iso3_label). See gtap_invest_generate_base_data.py.

### Labels files

Based on how the GTAP database is structured, EE spec defines several file types of files to systemetize how dimensions/sets are defined (and then used in e.g. figure plotting). A single dimension is first defined by a labels file. The labels file has at least 3 columns of domain_id, domain_label, domain_name and optionally a domain_description. If present, a column needs to be fully filled (no missing values). Label files are used in other contexts to, e.g., go from id to name for labeling an axis on a plot, as well as building the correspondence files below.

### Correspondences

Model linkages often require mapping many-to-one relationship in a consistent way. Correspondence files define this via a src-to-dst (source and destination). They are named according to a relatively complex pattern. Specifically, using the file path `gadm_r263_gtapv7_r251_r160_r50_correspondence`, we have a domain label `gadm` followed by a src dimension-size pair `r263` (where r is a label, short for region in this case, and 263 is the number of unique entries in that dimension). To be a correspondence,there needs to be at least one other dst dimension-size pair, where in this case there are 3 additional dimension-size pairs (`r251`, `r160`, and `r50`). However, the later three pairs are from a different domain, namely that of `gtapv7`. Each pair is identified with the domain most identified most closely prior. The dst dimension-size pairs are sorted in order of decreasing size. The dst dimension-size pairs are then followed by the word `correspondence`. This example creates a correspondence file that maps from the GADM 263 regions to the GTAPv7 251 regions, which are then mapped to the GTAPv7 160 regions, which are then mapped to the GTAPv7 50 regions.

An example of a 2-type correspondence is below. However, in this file, src and dst would have to be replaced with the specific domain names used.

| src_id | dst_id | src_label | dst_label | src_description | dst_description                |
|------------|------------|------------|------------|------------|------------|
| 1      | 1      | aus       | oceania   | Australia       | Oceania (including NZ and AUS) |
| 2      | 1      | nzl       | oceania   | New Zealand     | Oceania (including NZ and AUS) |

Here are the specific labels and corespondence files generated for gtapv7-aez-rd:

![](images/paste-22.png)

If defined exactly right, 2 dimensional correspondence files will work with Hazelbean via

`seals_utils.set_derived_attributes(p)`

and

```{python}
p.lulc_correspondence_dict = hb.utils.get_reclassification_dict_from_df(p.lulc_correspondence_path, 'src_id', 'dst_id', 'src_label', 'dst_label')}`

# return a very useful dictionary for various reclassification tasks:
```

```{python}
return_dict = {} return_dict['dst_to_src_reclassification_dict'] = dst_to_src_reclassification_dict  # Dict of one-to-many keys to lists of what each dst_key should be mapped to from each src_key. Useful when aggrigating multiple layers to a aggregated dest type 
return_dict['src_to_dst_reclassification_dict'] = src_to_dst_reclassification_dict # Useful when going to a specific value. 
return_dict['dst_to_src_labels_dict'] = dst_to_src_labels_dict # Dictionary of lists of labels that map to each dst label 
return_dict['src_ids'] = remove_duplicates_in_order(src_ids) # Unique set of src_ids r
eturn_dict['dst_ids'] = remove_duplicates_in_order(dst_ids) # Unique set of dst_ids 
return_dict['src_labels'] = remove_duplicates_in_order(src_labels) # Unique set of src_labels 
return_dict['dst_labels'] = remove_duplicates_in_order(dst_labels) # Unique set of dst_labels 
return_dict['src_ids_to_labels'] = {k: v for k, v in zip(return_dict['src_ids'], return_dict['src_labels'])} # one-to-one dictionary of src ids to labels 
return_dict['dst_ids_to_labels'] = {k: v for k, v in zip(return_dict['dst_ids'], return_dict['dst_labels'])} # one-to-one dictionary of dst ids to labels 
return_dict['src_labels_to_ids'] = {k: v for k, v in zip(return_dict['src_labels'], return_dict['src_ids'])} # one-to-one dictionary of src labels to ids 
return_dict['dst_labels_to_ids'] = {k: v for k, v in zip(return_dict['dst_labels'], return_dict['dst_ids'])} # one-to-one dictionary of dst labels to ids}
```

Among other possibilities, this could be used for reclassifying LULC geotiffs via

`{python} rules = p.lulc_correspondence_dict['src_to_dst_reclassification_dict'] hb.reclassify_raster_hb(raster_path, rules, output_path)}`

### Combined ids

One special case of ids is when two different ids are combined together leveraging their decimal position to compress data. For example, if you want a Region-AEZ-specific id stored in single column, you can do that by saying the joined id is 5 digits long, the first three correspond to ee_r264 and the final two correspond to aez18, as in this example:

![](images/paste-29.png)

In the event of a combined id, not that the \_id column above has two region-specifications combined (which otherwise violates the ee spec defined above for non combined ids).

### Correspondences with geometries

```{python}
# The rules for naming correspondences are a little different when adding geometry to the data. Because}     
# # only 1 geometry can be assigned per file, and becasue membership of aggregated regions and their     
# # members can get confusing, each correspondence file keeps all the labels but then is also saved along     
# # with a geometry file that drops the other labels (and the word correspondence in the filename).     
p.ee_r264_correspondence_vector_path = p.get_path(os.path.join('gtap_invest', 'region_boundaries', 'ee_r264_correspondence.gpkg'))     
p.ee_r264_vector_path = p.get_path(os.path.join('gtap_invest', 'region_boundaries', 'ee_r264.gpkg'))
```

### More on naming Conventions

The names of project-level variables are carefully defined. For example, in gtapv7_r251_r160_correspondence_input_path, the `input` when put right before the word path implies it is a raw asset we obtained from an external source but haven't processed it yet. This means that it can be a non-compliant XLSX file. Aslo, path implies that it is a string type that points to a location in a storage device. In this name, we see two other structures. First, the gtapv7 label indicates the "domain" of the correspondence defined in all of the following dimensions (until another domain label). Above, this means that it is the 251 regions, as defined by the gtapv7 domain, mapped to the 160 regions in the same domain.

You can have multiple mappings in a single correspondence file. For example, gtapv7_r251_s65_r50_s26_correspondence_input_path, we are mapping r251 to r50 and s (sectors) 65 to s26, all in the gtapv7 domain.

Appart from correspondence files, we also have **labels** files, such as gtapv7_r251_labels_path. Labels files are for a single set/variable/dimension but map together the synonymous categorizers. Specifically, it must have an id, label, and name, all filled out for every entry. It can optionally have others like description.

In the filename gtapv7_r251_labels_path we extract from the input_path and write a EE-compliant labels table for the gtapv7 r251 set. In the filename `gtap11_gtapaez11_region_correspondence_path`, we use these regions and then connect it to the gtapaez11 labels. We can infer also that it is from a mapping labeled gtap11 to gtapaez11 and that the variable in question is the regions while the word correspondence then indicates this is a many-to-one mapping file.

Note that the file path says "regions" while the column label in the CSV says "region".

```{python}
p.gtap11_region_correspondence_input_path = os.path.join(p.base_data_dir, 'gtappy', 'aggregation_mappings', 'GTAP-ctry2reg.xlsx')     
p.gtap11_region_names_path = os.path.join(p.base_data_dir, 'gtappy', 'aggregation_mappings', 'gtap11_region_names.csv')     
p.gtap11_gtapaez11_region_correspondence_path = os.path.join(p.base_data_dir, 'gtappy', 'aggregation_mappings', 'gtap11_gtapaez11_region_correspondance.csv')}
```

## Variable and Scenario Naming Conventions

To keep track of the MANY different filetypes, data processes, variables, scenarios, policies etc, please follow exactly the specifications below.

-   The word label refers to a relatively short string (preferably 8 characters long or less) with no spaces, underscores or punctuation (but may have hyphens). This is case-sensitive, but try to avoid capitalization.

-   The word short_label refers to a label that is strictly less or equal to 8 characters to ensure compatibility with HAR files.

-   The word name refers to a longer string that describes a specific label with a 1:1 correspondence. Will usually be defined via a correspondence dictionary.

-   The words index, indices or id refers to numerical data that describes a label with a 1:1 correspondence. Will usually be defined via a correspondence dictionary. If both are used, index/indices refer to a unique, ordered list of ints while an id/ids refer are unique but not necessarily ordered.

-   The word class refers to LULC class. Consider renaming this to lc-class?

-   Scenarios are defined in the following nested structure:

    -   Label (with no hyphens) for Exogenous Assumptions (e.g., which SSP, which GDP, which population). Typically this will be fully defined by the SSP.

    -   Label (with no hyphens) for Climate Assumption (which RCP)

    -   Label (can have hyphens) for which model is used (e.g., magpie, luh2-message). Only model is allowed to have hyphens (because they are used for multistep scenario processing of counterfactuals)

    -   Label for Counterfactual. This often represent policy Assumptions/Definition and can include BAU, which is a special counterfactual against which other policies are compared. Different counterfactuals correspond to different shockfiles in the econ model or different LUC projection priorities, etc.

        -   Counterfactuals may have multiple processing steps, which will be denoted by appending a hyphen and exactly 4 chars to the end of the base counterfactual label.

            -   IFor example, a run excludes consideration of ES, insert "-noes", at the end of the policy_name if it does include ES, postpend nothing (as this will be the one that is referenced by default)

    -   Year

        -   When the variable is singular, it must be an int. If it is plural, as is ints in a list. However, when either is stored in a dataframe, always type always type check as follows:

            -   If singular, do str(value), int(value) or float(value) as appropriate when reading from the df into a python variable.

            -   If plural, assume the df value is a space-delimited string that needs to be split, e.g. as \[int(i) for i in value.split(' ')\], or' '.join(values) if going into the DF

        -   Three types of years exist, including

            -   p.base_years, (which recall will always be a list even if there is a single entry because the variable name is plural)

-   Together, the labels above mean that the scenarios can be represented by directories as follows:

    -   'ssp2/rcp45/luh2-message/bau/filename_2050.tif'
        -   Note, the last layer of the hierarchy will be included as a the suffix of the filename start rather than as a directory (also see below for filename conventions)

-   For filenames, there are two possible conventions:

    -   Implied: This means that the directory structure above defines all the labels with the exception of year (which is postpended to the filename label) and the current variable name (such as lulc) which appears at the front of the filename.

        -   e.g., project/intermediate/convert_netcdf/ssp2/rcp45/luh2-message/bau/lulc_2050.tif

    -   Explicit: Even if a file is in a directory which implies its labels, explicit file naming will always include each label (and the variable label stays in front of the filename), so the above example is:

        -   project/intermediate/convert_netcdf/ssp2/rcp45/luh2-message/bau/lulc_ssp2_rcp45_bau_luh2-message_2050.tif

        -   And if there are no ES considered, it would be project/intermediate/convert_netcdf/ssp2/rcp45/luh2-message/bau/lulc_ssp2_rcp45_bau_luh2-message_2050.tif

-   Because labels have no spaces or underscores, it is possible to convert the nested structure above to a single string, as in filename_ssp2_rcp45_policyname_year.tif.

-   Filetypes that supported in this computation environment include

    -   Netcdf with the above listed dimensions in the same order

    -   A set of geotiffs embedded in directories. Each label gets a directory level expect for year, which by convention, will ALWAYS be the last 4 characters of a filename before the extension (with an underscore before it).

    -   A spreadsheet linkable to a geographic representation (e.g., a shapefile or a geopackage) in vertical format

-   Also we will create a set of tables to analyze results

    -   These will define Regions (shp) for quick result plotting

    -   Specifically, we will have a full vertically stacked CSV of results, then for each Report Archetype we would output 1 minimal info CSV and the corresponding Figure.

-   Miscellaneous:

    -   base_years is correct, never baseline_years (due to confusion between baseline and bau)

-   Scenario types

    -   Three scenario_types are supported: baseline, bau and policy
        -   Baseline assumes the year has data existing from observations (rather than modelled) and that these years are defined in p.years (and identically defined in p.base_years).
            -   One exception is when eg GTAP is used to update the base year from 2017 to 2023, and then policies are applied on 2023.
        -   BAU and policy scenarios assume the results are modelled and that their years are defined in p.years (but not p.base_years)
    -   Clarify what is the naming difference between src dst versus input output. Is the former only for file paths or can it also be e.g. array. OR does this have to do with if it is a function return.
        -   Proposed answer: src/dst is a pointer/reference to a thing and Input/Output is the thing itself. Esp useful for paths.
        -   Similarly, \_path and \_dir imply the string is a reference, so src_path and src_dir are common.
        -   You might often see e.g. input_array = hb.as_array(src_path), illustrating this difference.