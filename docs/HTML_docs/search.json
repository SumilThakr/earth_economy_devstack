[
  {
    "objectID": "conventions.html",
    "href": "conventions.html",
    "title": "Naming conventions",
    "section": "",
    "text": "In programming, id and index are two different concepts that are used in different contexts.\nid refers to the unique identifier of an object. In this specification, it is an integer that is sorted by the ONE in the many-to-one correspondence, sorted alphabetically at generation time (though not necessarily to remain sorted given downstream correspondences).\nindex refers to the position of an element in a sequence (e.g. a list or a string). In the context of a correspondence file, this is the position of the row within the sorted spreadsheet, but is not assumed to be stable and shouldn’t generally be used.\nlabelheader refers to an (exactly) 4 character string that is lowercase-alphanumeric with no special symbols besides underscore. Useful for the Header label in har files. Technically is case insensitive but we assume lowercase.\nlabelshort refers to an 8-character or less string that is lowercase-alphanumeric with no special symbols besides underscore. Useful for .har files.\nlabel refers to a string that is lowercase-alphanumeric with no special symbols besides underscore\nname refers to a string of any ascii characters with python-style escaping of special characters: 'She\\'s a star!' . It’s assumed to be short enough to view as a column header or plot label\nDescriptionrefers to a name of any length with detailed description, possibly even formatted MD text.\nIf there is a domain, described below, id, index, etc all should prepended with it to be eg gadm_id."
  },
  {
    "objectID": "conventions.html#variable-and-scenario-naming-conventions",
    "href": "conventions.html#variable-and-scenario-naming-conventions",
    "title": "Naming conventions",
    "section": "Variable and Scenario Naming Conventions",
    "text": "Variable and Scenario Naming Conventions\nTo keep track of the MANY different filetypes, data processes, variables, scenarios, policies etc, please follow exactly the specifications below.\n\nThe word label refers to a relatively short string (preferably 8 characters long or less) with no spaces, underscores or punctuation (but may have hyphens). This is case-sensitive, but try to avoid capitalization.\nThe word short_label refers to a label that is strictly less or equal to 8 characters to ensure compatibility with HAR files.\nThe word name refers to a longer string that describes a specific label with a 1:1 correspondence. Will usually be defined via a correspondence dictionary.\nThe words index, indices or id refers to numerical data that describes a label with a 1:1 correspondence. Will usually be defined via a correspondence dictionary. If both are used, index/indices refer to a unique, ordered list of ints while an id/ids refer are unique but not necessarily ordered.\nThe word class refers to LULC class. Consider renaming this to lc-class?\nScenarios are defined in the following nested structure:\n\nLabel (with no hyphens) for Exogenous Assumptions (e.g., which SSP, which GDP, which population). Typically this will be fully defined by the SSP.\nLabel (with no hyphens) for Climate Assumption (which RCP)\nLabel (can have hyphens) for which model is used (e.g., magpie, luh2-message). Only model is allowed to have hyphens (because they are used for multistep scenario processing of counterfactuals)\nLabel for Counterfactual. This often represent policy Assumptions/Definition and can include BAU, which is a special counterfactual against which other policies are compared. Different counterfactuals correspond to different shockfiles in the econ model or different LUC projection priorities, etc.\n\nCounterfactuals may have multiple processing steps, which will be denoted by appending a hyphen and exactly 4 chars to the end of the base counterfactual label.\n\nIFor example, a run excludes consideration of ES, insert “-noes”, at the end of the policy_name if it does include ES, postpend nothing (as this will be the one that is referenced by default)\n\n\nYear\n\nWhen the variable is singular, it must be an int. If it is plural, as is ints in a list. However, when either is stored in a dataframe, always type always type check as follows:\n\nIf singular, do str(value), int(value) or float(value) as appropriate when reading from the df into a python variable.\nIf plural, assume the df value is a space-delimited string that needs to be split, e.g. as [int(i) for i in value.split(’ ‘)], or’ ’.join(values) if going into the DF\n\nThree types of years exist, including\n\np.base_years, (which recall will always be a list even if there is a single entry because the variable name is plural)\n\n\n\nTogether, the labels above mean that the scenarios can be represented by directories as follows:\n\nssp2/rcp45/luh2-message/bau/filename_2050.tif\n\nNote, the last layer of the hierarchy will be included as a the suffix of the filename start rather than as a directory (also see below for filename conventions)\n\n\nFor filenames, there are two possible conventions:\n\nImplied: This means that the directory structure above defines all the labels with the exception of year (which is postpended to the filename label) and the current variable name (such as lulc) which appears at the front of the filename.\n\ne.g., project/intermediate/convert_netcdf/ssp2/rcp45/luh2-message/bau/lulc_2050.tif\n\nExplicit: Even if a file is in a directory which implies its labels, explicit file naming will always include each label (and the variable label stays in front of the filename), so the above example is:\n\nproject/intermediate/convert_netcdf/ssp2/rcp45/luh2-message/bau/lulc_ssp2_rcp45_bau_luh2-message_2050.tif\nAnd if there are no ES considered, it would be project/intermediate/convert_netcdf/ssp2/rcp45/luh2-message/bau/lulc_ssp2_rcp45_bau_luh2-message_2050.tif\n\n\nBecause labels have no spaces or underscores, it is possible to convert the nested structure above to a single string, as in filename_ssp2_rcp45_policyname_year.tif.\nFiletypes that supported in this computation environment include\n\nNetcdf with the above listed dimensions in the same order\nA set of geotiffs embedded in directories. Each label gets a directory level expect for year, which by convention, will ALWAYS be the last 4 characters of a filename before the extension (with an underscore before it).\nA spreadsheet linkable to a geographic representation (e.g., a shapefile or a geopackage) in vertical format\n\nAlso we will create a set of tables to analyze results\n\nThese will define Regions (shp) for quick result plotting\nSpecifically, we will have a full vertically stacked CSV of results, then for each Report Archetype we would output 1 minimal info CSV and the corresponding Figure.\n\nMiscellaneous:\n\nbase_years is correct, never baseline_years (due to confusion between baseline and bau)\n\nScenario types\n\nThree scenario_types are supported: baseline, bau and policy\n\nBaseline assumes the year has data existing from observations (rather than modelled) and that these years are defined in p.years (and identically defined in p.base_years).\n\nOne exception is when eg GTAP is used to update the base year from 2017 to 2023, and then policies are applied on 2023.\n\nBAU and policy scenarios assume the results are modelled and that their years are defined in p.years (but not p.base_years)\n\nClarify what is the naming difference between src dst versus input output. Is the former only for file paths or can it also be e.g. array. OR does this have to do with if it is a function return.\n\nProposed answer: src/dst is a pointer/reference to a thing and Input/Output is the thing itself. Esp useful for paths.\nSimilarly, _path and _dir imply the string is a reference, so src_path and src_dir are common.\nYou might often see e.g. input_array = hb.as_array(src_path), illustrating this difference."
  },
  {
    "objectID": "earth_economy_devstack.html",
    "href": "earth_economy_devstack.html",
    "title": "DEPRACATED - SEE INDIVIDUAL FILES - Earth-Economy Devstack",
    "section": "",
    "text": "This is the documentation for the Earth-Economy Devstack, which is the set of repositories and code tools used by the Johnson-Polasky Lab and NatCap TEEMs. This documentation starts with overall organization of the Earth-Economy Devstack and discusses common coding practices used among the 5 underlying repositories. Next, each of the repositories is discussed in turn."
  },
  {
    "objectID": "earth_economy_devstack.html#getting-the-earth_economy_devstack-repository",
    "href": "earth_economy_devstack.html#getting-the-earth_economy_devstack-repository",
    "title": "DEPRACATED - SEE INDIVIDUAL FILES - Earth-Economy Devstack",
    "section": "Getting the earth_economy_devstack repository",
    "text": "Getting the earth_economy_devstack repository\nAssuming you have already setup your python environment and installed VS Code, the first step is to clone all of the relevant repositories into the exact-right location. Throughout this documentation, we refer to “EE Spec”, or Earth-Economy Specification. This is simply the common conventions for things like naming files and organizing them so that it works for all participants. The EE Spec organization is to clone the earth_economy_devstack repository, available at https://github.com/jandrewjohnson/earth_economy_devstack, into your Users directory in a subdirectory called Files. The PC version is shown below.\n\nTo clone here, you can use the command line, navigate to the Files directory and use git clone https://github.com/jandrewjohnson/earth_economy_devstack . Alternatively you could use VS Code’s Command Pallate &lt;ctrl-shift-s&gt; Git Clone command and navigate to this repo. This repository configures VS Code to work well with the other EE repos and, for instance, defines launch-configurations that will always use the latest from github."
  },
  {
    "objectID": "earth_economy_devstack.html#get-the-other-repositories",
    "href": "earth_economy_devstack.html#get-the-other-repositories",
    "title": "DEPRACATED - SEE INDIVIDUAL FILES - Earth-Economy Devstack",
    "section": "Get the other repositories",
    "text": "Get the other repositories\nNext, create a folder for each of the five repositories in the Files directory, as in the picture above.\n\nhazelbean\nseals\ngtappy\ngtap_invest\nglobal_invest\n\nInside each of these folders, you will clone the corresponding repositories:\n\nhttps://github.com/jandrewjohnson/hazelbean_dev\nhttps://github.com/jandrewjohnson/seals_dev\nhttps://github.com/jandrewjohnson/gtappy_dev\nhttps://github.com/jandrewjohnson/gtap_invest_dev\nhttps://github.com/jandrewjohnson/global_invest_dev\n\nIf successful, you will have a new folder with _dev postpended, indicating that it is the repository itself (and is the dev, i.e., not yet public, version of it). For GTAPPy it should look like this:\n\nAll code will be stored in the _dev repository. All files that you will generate when running the libraries, conversely, will be in a different Projects directory as above (This will be discussed more in the ProjectFlow section)."
  },
  {
    "objectID": "earth_economy_devstack.html#launching-the-devstack-in-vs-code",
    "href": "earth_economy_devstack.html#launching-the-devstack-in-vs-code",
    "title": "DEPRACATED - SEE INDIVIDUAL FILES - Earth-Economy Devstack",
    "section": "Launching the devstack in VS Code",
    "text": "Launching the devstack in VS Code\nNavigate to the earth_economy_devstack directory. In there, you will find a file earth_economy_devstack.code-workspace (pictured below).\n\nDouble click it to load a preconfigured VS Code Workspace. You will know you have it all working if the explorer tab in VS Code shows all all SIX of the repositories."
  },
  {
    "objectID": "earth_economy_devstack.html#launch-and-vs-code-configurations",
    "href": "earth_economy_devstack.html#launch-and-vs-code-configurations",
    "title": "DEPRACATED - SEE INDIVIDUAL FILES - Earth-Economy Devstack",
    "section": "Launch and VS Code Configurations",
    "text": "Launch and VS Code Configurations\nThe earth_economy_devstack.code-workspace configures VS Code so that it includes all of the necessary repositories for EE code to run. Specifically, this means that if you pull the most recent version of each repository from Github, your code will all work together seamlessly. In addition to this file, you will see in the .vs_code directory there is a launch.json file. This file defines how to launch the python Debugger so that it uses the correct versions of the repositories. To launch a specific python file, first make it the active editor window, then open the debugger tab in VS Code’s left navbar, and in the Run and Debug dropdown box (pictured below) select the first run configuration, then hit the green Play triangle to launch the python file you had open. If all is setup correctly, your file you ran should be able to import all of the libraries in the devstack."
  },
  {
    "objectID": "earth_economy_devstack.html#version-management-with-git-github-repository",
    "href": "earth_economy_devstack.html#version-management-with-git-github-repository",
    "title": "DEPRACATED - SEE INDIVIDUAL FILES - Earth-Economy Devstack",
    "section": "Version management with Git & GitHub repository",
    "text": "Version management with Git & GitHub repository\nSEALS is available as open source software at GitHub, which not only allows users to download and run the code but also to develop it collaboratively. GitHub is based on the free and open source distributed version control system git (https://git-scm.com/). git allows users to track changes in code development, to work on different code branches and to revert changes to previous code versions, if necessary. Workflows using the version control system git can vary and there generally is no ‘right’ or ‘wrong’ in collaborative software development.\nThere are two different ways to interact with a github repository: 1. making a fork; 2; working on a branch. This section describes both options, however internal members of TEEMs will usually use branches.\n\nForking\nHowever, some basic preferred workflows are outlined in the following.\n\n\nCreate a new fork of the original SEALS repo\nBefore developing the code it is recommended to create a fork of the original SEALS repository. This will create a new code repository at your own GitHub profile, which includes the same code base and visibility settings as the original ‘upstream’ repository. The fork can now be used for - code development or fixes - submitting a pull request to the original SEALS repository.\n\n\nClone fork to your local machine\nAlthough GitHub allows for some basic code changes, it is recommended to clone the forked repository at your local machine and do code developments using an integrated development environment (IDE), such as VS Code. To clone the repository on your local machine via the command line, navigate to the local folder, in which you want to clone the repository, and type\ngit clone -b &lt;name-of-branch&gt;` &lt;url-to-github-repo&gt; &lt;name-of-local-clone-folder&gt;\nFor example, if you want to clone the develop branch of your SEALS fork type\ngit clone -b develop https://github.com/&lt;username/seals_dev seals_develop\n\n\n\nBranching\nWheter you are working on a Fork or on the seals repository itself, we will organize contributions with branches. On the command line, you can create new code branch by using git branch &lt;name-of-new-branch&gt;. To let git know that you want to switch to and work on the new branch use git checkout &lt;name-of-new-branch&gt;. There are many other graphical user interfaces that help with git commands (so you don’t have to memorize all of the commands), including Github Desktop, Sourcetree and others. We also will use a VS Code plug called GitGraph to manage our repository. To install GitGraph, go the the Extensions left tab in VS Code and search for it. Once installed, you will find a button on the status bar to launch it. Using the gtap_invest_dev repository as an example, it should look like this:\n\nThis is an example that has multiple branches that add new content, but are then merged back into one.\nBefore making any changes, make sure that your fork and/or your local repository are up-to-date with the original SEALS repository. To pull changes from your fork use git pull origin &lt;name-of-branch&gt;. In order to pull the latest changes from the original SEALS repository you can set up a link to the original upstream repository with the command git remote add upstream https://github.com/jandrewjohnson/seals_dev. To pull the latest changes from the develop branch in the original upstream repository use git pull upstream develop.\nDuring code development, any changes or fixes that should go back to the fork and/or the original SEALS repository need to be ‘staged’ and then ‘commited’ with a commit message that succinctly describes the changes. By staging changes, git is informed that these changes should become part of the next ‘commit’, which essentially takes a snapshot of all staged changes thus far.\nTo stage all changes in the current repository use the command git add .. If you only want to stage changes in a certain file use git add &lt;(relative)-path-to-file-in-repo&gt;.\nTo commit all staged changes use the command git commit -m \"a short description of the code changes\".\nAfter committing the changes, they can be pushed to your fork by using git push origin &lt;name-of-branch&gt;.\n ### Pull requests\nIn order to propose changes to the original SEALS repository, it is recommended to use pull requests. Pull requests inform other users about your code changes and allow them to review these changes by comparing and highlighting the parts of the code that have have been altered. They also highlight potential merge conflicts, which occur when git is trying to merge branches with competing commits within the same parts of the code. Pull requests can be done directly at GitHub by navigating to either the forked repository or the original SEALS repository and by clicking on ‘Pull requests’ and then ‘New pull request’. This will open a new pull request page where changes can be described in more detail and code reviewers can be specfied. Pull requests should be reviewed by at least one of the main code developers."
  },
  {
    "objectID": "earth_economy_devstack.html#iterating-over-many-model-assumptions",
    "href": "earth_economy_devstack.html#iterating-over-many-model-assumptions",
    "title": "DEPRACATED - SEE INDIVIDUAL FILES - Earth-Economy Devstack",
    "section": "Iterating over many model assumptions",
    "text": "Iterating over many model assumptions\nIn many case, such as a standard GTAPPy run, we will iterate over different aggergations and scenarios (now renamed counterfactuals). This is done as expected with code like this:\n\nfor aggregation_label in p.aggregation_labels:\n     \n    for experiment_label in p.experiment_labels:\n        \n        for n_years_counter, ending_year in enumerate(p.years):\n            \n            if n_years_counter == 0:\n                starting_year = p.base_year\n            else:\n                starting_year = p.years[n_years_counter - 1]\n                \n            output_dir = p.get_path(os.path.join(aggregation_label, experiment_label, str(ending_year)))\n\nBut sometimes, this becomes DEEPLY nested and confusing. SEALS implements an API that reads these nested layers from a CSV. This is defined more fully in the SEALS user guide.\n\nfor index, row in p.scenarios_df.iterrows():\n    seals_utils.assign_df_row_to_object_attributes(p, row)\n    \n    if p.scenario_type != 'baseline':\n                            \n        for n_years_counter, ending_year in enumerate(p.years):\n\n            if n_years_counter == 0:\n                starting_year = p.base_year\n            else:\n                starting_year = p.years[n_years_counter - 1]\n                \n            current_run_dirs = os.path.join(p.exogenous_label, p.climate_label, p.model_label, p.counterfactual_label)\n            output_dir = p.get_path(current_run_dirs, str(ending_year))\n            expected_sl4_path = os.path.join(output_dir, p.counterfactual_label + '_Y' + str(ending_year) + '.sl4')\n\nIn this scenarios_df, which was loaded from scenarios_csv_path, there are multiple nested for loops implied, for p.exogenous_label, p.climate_label, p.model_label, p.counterfactual_label, each row has a unique value that would have been iterated over with the for loop above. Now, however, we are iterating just over scenario_df rows. Within each row pass, a project-level attribute is assigned via seals_utils.assign_df_row_to_object_attributes(p, row). This is used instead of the nested for loop."
  },
  {
    "objectID": "earth_economy_devstack.html#creating-scenarios-spreadsheets",
    "href": "earth_economy_devstack.html#creating-scenarios-spreadsheets",
    "title": "DEPRACATED - SEE INDIVIDUAL FILES - Earth-Economy Devstack",
    "section": "Creating scenarios spreadsheets",
    "text": "Creating scenarios spreadsheets\nHere. Explain why it writes the scenarios_csv FROM CODE rather than downloading it (keeps it up to date as code changes quickly). However, this gets convoluted when you also have to initialize the attributes before you write?!?\n\n    # If you want to run SEALS with the run.py file in a different directory (ie in the project dir)\n    # then you need to add the path to the seals directory to the system path.\n    custom_seals_path = None\n    if custom_seals_path is not None: # G:/My Drive/Files/Research/seals/seals_dev/seals\n        sys.path.insert(0, custom_seals_path)\n\n    # SEALS will run based on the scenarios defined in a scenario_definitions.csv\n    # If you have not run SEALS before, SEALS will generate it in your project's input_dir.\n    # A useful way to get started is to to run SEALS on the test data without modification\n    # and then edit the scenario_definitions.csv to your project needs.\n    # Some of the other test files use different scenario definition csvs \n    # to illustrate the technique. If you point to one of these \n    # (or any one CSV that already exists), SEALS will not generate a new one.\n    # The avalable example files in the default_inputs include:\n    # - test_three_scenario_defininitions.csv\n    # - test_scenario_defininitions_multi_coeffs.csvs\n    \n    p.scenario_definitions_path = os.path.join(p.input_dir, 'scenario_defininitions.csv')\n\n    # Set defaults and generate the scenario_definitions.csv if it doesn't exist.\n    if not hb.path_exists(p.scenario_definitions_path):\n        # There are several possibilities for what you might want to set as the default.\n        # Choose accordingly by uncommenting your desired one. The set of\n        # supported options are\n        # - set_attributes_to_dynamic_default (primary one)\n        # - set_attributes_to_dynamic_many_year_default\n        # - set_attributes_to_default # Deprecated\n\n        gtap_invest_utils.set_attributes_to_dynamic_gtap_default(p) # Default option\n\n\n        # # Optional overrides for us in intitla scenarios\n        # p.aoi = 'RWA'\n\n        # gtap_invest_utils.set_attributes_to_dynamic_default(p)\n        # Once the attributes are set, generate the scenarios csv and put it in the input_dir.\n        gtap_invest_utils.generate_gtap_invest_scenarios_csv_and_put_in_input_dir(p)\n        p.scenarios_df = pd.read_csv(p.scenario_definitions_path)\n    else:\n        # Read in the scenarios csv and assign the first row to the attributes of this object (in order to setup additional \n        # project attributes like the resolutions of the fine scale and coarse scale data)\n        p.scenarios_df = pd.read_csv(p.scenario_definitions_path)\n\n        # Because we've only read the scenarios file, set the attributes\n        # to what is in the first row.\n        for index, row in p.scenarios_df.iterrows():\n            seals_utils.assign_df_row_to_object_attributes(p, row)\n            break # Just get first for initialization."
  },
  {
    "objectID": "earth_economy_devstack.html#task-format",
    "href": "earth_economy_devstack.html#task-format",
    "title": "DEPRACATED - SEE INDIVIDUAL FILES - Earth-Economy Devstack",
    "section": "Task Format",
    "text": "Task Format\nProject flow requires a consistent format for tasks. The following is an example of a task that creates a correspondence file from gtap11 regions to gtapaez11 regions. The task itself defined as a function that takes a p object as an argument. This p object is a ProjectFlow object that contains all the project-level variables, manages folders and files, and manages tasks and parallelization. p also includes documentation, which will be written directly into the task directory.\nAlso note that any project-level attribute defined in between the function start and the if p.run_this: component are the “project level variables” that are fair-game for use in other tasks. These paths are critical for high performance because they enable quick-skipping of completed tasks and determiniation of which parts of the task tree need rerunning.\nTasks should be named as a noun (this breaks Python pep8 style) referencing what will be stored in the tasks output dir. This might feel awkward at first, but it means that the resultant file structure is easier to interpret by a non-EE outsider.\n\ndef gtap_aez_seals_correspondences(p):\n    p.current_task_documentation = \"\"\"\n    Create correspondence CSVs from ISO3 countries to GTAPv11 160\n    regions, and then to gtapaezv11 50ish regions, also put the classification\n    for seals simplification and luh.  \n    \"\"\"\n    p.gtap11_region_correspondence_input_path = os.path.join(p.base_data_dir, 'gtappy', 'aggregation_mappings', 'GTAP-ctry2reg.xlsx')\n    p.gtap11_region_names_path = os.path.join(p.base_data_dir, 'gtappy', 'aggregation_mappings', 'gtap11_region_names.csv')\n    p.gtap11_gtapaez11_region_correspondence_path = os.path.join(p.base_data_dir, 'gtappy', 'aggregation_mappings', 'gtap11_gtapaez11_region_correspondance.csv')    \n\n    if p.run_this:\n        \n        \"logic here\""
  },
  {
    "objectID": "earth_economy_devstack.html#automatic-directory-organization-via-tasks",
    "href": "earth_economy_devstack.html#automatic-directory-organization-via-tasks",
    "title": "DEPRACATED - SEE INDIVIDUAL FILES - Earth-Economy Devstack",
    "section": "Automatic Directory Organization via Tasks",
    "text": "Automatic Directory Organization via Tasks\nHazelbean automatically defines directory organization as a function of the task tree. When the ProjectFlow object is created, it takes a directory as its only required input. This directory defines the root of the project. The other directory that needs to be referenced is the base_data_dir. When you initialize the p object, it notes this:\nCreated ProjectFlow object at C:\\Users\\jajohns\\Files\\gtap_invest\\projects\\cwon     from script C:\\Users\\jajohns\\Files\\gtap_invest\\gtap_invest_dev\\gtap_invest\\run_cwon.py     with base_data set at C:\\Users\\jajohns\\Files/base_data\nIn the run file, the following line generates the task tree:\ngtap_invest_initialize_project.build_extract_and_run_aez_seals_task_tree(p)\nWhich points to a builder function in the initialize file, looking something like this:\n\nThis would generate the following task tree:\n\nTwo notations are especially useful within this task tree.\n\nWithin the function that defines a task, p.cur_dir points to the directory of that task. So for instance, the last task defined in the image above, in its code, you could reference p.cur_dir, and it would point to &lt;project_root&gt;/econ_visualization/econ_lcovercom\nOutside of a given function’s code, you can still refer to paths that were defined from within the functions code, but now (because you are outside the function) it is given a new reference. Using the example above, you could reference the same directory with p.econ_lcovercom_dir where the p attribute is named exactly as &lt;function_name&gt;_dir\n\nAll of this setup enable another useful feature: automatic management of file generation, storage and downloading. This is done via the hazelbean function:\n\nuseful_path = hb.get_path(relative_path)\n\nThis function will iteratively search multiple locations and return the most “useful” one. By default, the relative_path variable will first joined with the p.cur_dir. If the file exists, it returns it. If not, it checks the next location, which is p.input_dir, and then p.base_data_dir. If it doesn’t find it anywhere, it will attempt to download it from google cloud (NYI) and save it in the p.cur_dir. If it is not available to download on google cloud, then it treats the path as something we will be generating within the task, and thus, get_path returns the first option above, namely joining the relative_path with p.cur_dir.\nOne important use-case that needs explaining is for tasks that generate files that will eventually be placed in the base_data_dir. The goal is to enable easy generation of it to the intermediate directory in the appropriate task_dir, but then have the ability to copy the files with the exact same relative path to the base_data_dir and have it still be found by p.get_path(). To do this, you will want to choose a path name relative to the tasks’ cur_dir that matches the desired directory relative to the base data dir. So, for example, we include 'gtappy', 'aggregation_mappings' at the beginning of the relative path for in the intermediate directory in the appropriate task_dir, but then we also will want to copy the files with the exact same relative path to the base_data_dir and have it still be found by p.get_path(). To do this, you will want to choose a path name relative to the tasks’ cur_dir that matches that in the base_data_dir, for example &lt;base_data_dir&gt;/'gtappy/aggregation_mappings/gadm_adm0.gpkg',\n\ntemplate_path = p.get_path(os.path.join('gtappy', 'aggregation_mappings', 'gadm_adm0.gpkg')) \n\nIt can be hard deciding what counts as a base_data_generating task or not, but generally if it is a file that will not be used by other projects, you should not treat it as a base_data_generating task. Instead, you should just make it relative to the cur_dir (or wahtever makes sense), as below:\n\noutput_path = p.get_path(os.path.join(aggregation_label + '_' + experiment_label + '_' + header + '_stacked_time_series.csv'))\n\nOne additional exception to the above is if you are calling get_path outside of a task/task_tree. One common example is in the run file before you build the task tree. In this case, the default_dirs will not make sense, and so you need to specify it manually as here:\n\np.countries_iso3_path = p.get_path(os.path.join('cartographic', 'gadm', 'gadm_adm0_10sec.gpkg'), possible_dirs=[p.input_dir, p.base_data_dir])"
  },
  {
    "objectID": "earth_economy_devstack.html#validation-of-files",
    "href": "earth_economy_devstack.html#validation-of-files",
    "title": "DEPRACATED - SEE INDIVIDUAL FILES - Earth-Economy Devstack",
    "section": "Validation of files",
    "text": "Validation of files\nProjectFlow is designed to calculate very fast while simultaneously validating that everything is approximately correct. It does this by checking for the existence of files (often combined with hb.get_path()). For example\n\np.gadm_r263_gtapv7_r251_r160_r50_correspondence_vector_path = p.get_path(os.path.join('gtap_invest', 'region_boundaries', 'gadm_r263_gtapv7_r251_r160_r50_regions.gpkg'))     \nif not hb.path_exists(p.gadm_r263_gtapv7_r251_r160_r50_correspondence_vector_path):         \n    hb.log('Creating ' + p.gadm_r263_gtapv7_r251_r160_r50_correspondence_vector_path)         \n    \"computationally expensive thing here.\"\n\nProjectFlow very carefully defines whether or not you should run something based on the existence of specific paths. Usually this is just checking for each written path and only executing the code if it’s missing, but in some cases where lots of files are created, it’s possible to take the shortcut of just checking for the existence of the last-created path.\n\nEliminating redundant calculation across projects\nIf you have a time consuming task that, or example, writes to\n\nbig_file_path = hb.get_path('lulc', 'esa', 'seals7', 'convolutions', '2017', 'convolution_esa_seals7_2017_cropland_gaussian_5.tif' )\n\nIn this example, suppose you needed to create this file via your create_convolutions() task or something. When you first do this, it obviously won’t exist yet, so get_path() will join that relative path in the p.cur_dir location. If you run the ProjectFlow again, it will see it’s there and then instantly skip recalcualting it.\nIn addition to the 5 repos plus the EE repo, there is a managed base data set stored in teh same location\n\nA ProjectFlow object must have a base_data_dir set (I think…). This is because the p.get_path() will look in this folder for it, and/or will download to it.\n\n\n\nPython Tips and Conventions\nFor large files that take a long time to load, use a string-&gt;dataframe/dataset substitution as below. Make a LOCAL variable to contain the loaded object, and have that be assigned to the correct project-level path string. In subsequent usages, check type and if it’s still a string, then it hasn’t been loaded yet, so do that. I’m debating making it a project level variable trick\n\ngadm = p.gadm_adm0_vector_input_path    \n\n# Just load it on first pass\nif type(gadm) is str:\n    gadm = hb.read_vector(p.gadm_adm0_vector_input_path)"
  },
  {
    "objectID": "earth_economy_devstack.html#quickstart",
    "href": "earth_economy_devstack.html#quickstart",
    "title": "DEPRACATED - SEE INDIVIDUAL FILES - Earth-Economy Devstack",
    "section": "Quickstart",
    "text": "Quickstart\nTest that hazelbean imports correctly. Importing it will trigger compilation of the C files if they are not there.\n\nimport hazelbean as hb"
  },
  {
    "objectID": "earth_economy_devstack.html#library-of-useful-functions",
    "href": "earth_economy_devstack.html#library-of-useful-functions",
    "title": "DEPRACATED - SEE INDIVIDUAL FILES - Earth-Economy Devstack",
    "section": "Library of useful functions",
    "text": "Library of useful functions\nExplore examples of the useful spatial, statistical and economic functions in the examples section of this documentation."
  },
  {
    "objectID": "earth_economy_devstack.html#project-flow",
    "href": "earth_economy_devstack.html#project-flow",
    "title": "DEPRACATED - SEE INDIVIDUAL FILES - Earth-Economy Devstack",
    "section": "Project Flow",
    "text": "Project Flow\nOne key component of Hazelbean is that it manages directories, base_data, etc. using a concept called ProjectFlow. ProjectFlow defines a tree of tasks that can easily be run in parallel where needed and keeping track of task-dependencies. ProjectFlow borrows heavily in concept (though not in code) from the task_graph library produced by Rich Sharp but adds a predefined file structure suited to research and exploration tasks.\n\nProject Flow notes\nProject Flow is intended to flow easily into the situation where you have coded a script that grows and grows until you think “oops, I should really make this modular.” Thus, it has several modalities useful to researchers ranging from simple drop-in solution to complex scripting framework.\n\nNotes\nIn run.py, initialize the project flow object. This is the only place where user supplied (possibly absolute but can be relative) path is stated. The p ProjectFlow object is the one global variable used throughout all parts of hazelbean.\n\nimport hazelbean as hb\n\nif __name__ == '__main__':\n    p = hb.ProjectFlow(r'C:\\Files\\Research\\cge\\gtap_invest\\projects\\feedback_policies_and_tipping_points')\n\nIn a multi-file setup, in the run.py you will need to import different scripts, such as main.py i.e.:\n\nimport visualizations.main\n\nThe script file mainpy can have whatever code, but in particular can include “task” functions. A task function, shown below, takes only p as an agrument and returns p (potentially modified). It also must have a conditional (if p.run_this:) to specify what always runs (and is assumed to run trivially fast, i.e., to specify file paths) just by nature of having it in the task tree and what is run only conditionally (based on the task.run attribute, or optionally based on satisfying a completed function.)\n\ndef example_task_function(p):\n    \"\"\"Fast function that creates several tiny geotiffs of gaussian-like kernels for later use in ffn_convolve.\"\"\"\n\n    if p.run_this:\n        for i in computationally_intensive_loop:\n            print(i)\n\nImportant Non-Obvious Note\nImporting the script will define function(s) to add “tasks”, which take the ProjectFlow object as an argument and returns it after potential modification.\n\ndef add_all_tasks_to_task_tree(p):\n    p.generated_kernels_task = p.add_task(example_task_function)"
  },
  {
    "objectID": "earth_economy_devstack.html#installation",
    "href": "earth_economy_devstack.html#installation",
    "title": "DEPRACATED - SEE INDIVIDUAL FILES - Earth-Economy Devstack",
    "section": "Installation",
    "text": "Installation\n\nCython Installation\n\n\n\n\n\n\n\n\n\nRun-GTAP installation simple for class\nStep 1:\n\nInstall RunGTAP at https://www.gtap.agecon.purdue.edu/products/rungtap/default.asp\nAt https://www.copsmodels.com/gpeidl.htm download: https://www.copsmodels.com/ftp/ei12dl/gpei-12.1.004-install.exe \n\nOR FOR VERSION 12: https://www.copsmodels.com/ftp/ei12dl/gpei-12.0.004-install.exe\n\nInstall to default location\n\n\nProceed without selecting a license (to start the 6 month trial).\nStep 2: Install GTAPAgg2: https://www.gtap.agecon.purdue.edu/private/secured.asp?Sec_ID=1055\nTO PROCEED, YOU NEED TO HAVE A ACCESS TO THE GTAP DATABASE. OR YOU CAN ACCESS IT THROUGH ee_internal DATABASE.\n\nStep 2.1: - Extract the .lic file from https://www.gtap.agecon.purdue.edu/databases/download.asp step 2. Put this in GTAPAgg2 dir\nStep 2.2: - Install the GTAP Database itself from at : https://www.gtap.agecon.purdue.edu/databases/download.asp\n\nMake sure to also get the AEZ version, which will also give a full pkg file.\n\n\n\nPut this .pkg file in GTPAg2 dir. \nLaunch GTPAg2, identify pkg file for both default and AEZ data\n\nWhen running aggregations, make sure to choose the right product:\n\nStep 2.3: Running the unaggregated version - Open GTPAg2.exe\n\n\nUse this to aggregate a 1-1 version.\n\nCreate a 1-1 mapping\n\nView change regional aggregation, sector aggregation, setting to 1:1\nFor factor aggregation, it defaults to 8-5, can set this to 8:8.\n\nRead aggregation scheme from file, loa default.agg is 10 by 10\nFor factors, if using AEZ, there is land specified by each AEZ. Erwin is working to fix this. Because didn’t have it ready, went back to standard GTAP package of data.\nUsed full 8 factors, make new things “mobile factors”.\n\n\n\n\nSave aggregation scheme to file to AggStore\n\n\nSave aggregation scheme to file to AggStore\n\nThis could also be done by making the .agg file via text editing. \n\nThen finally Create aggregated database in GTAPAgg\n\n\nNote that this creates two versions of the database, one for GTAP code v6.2, one for v7.0. The 7.0 is in a sub zip folder gtapv7.zip. This is the one we want."
  },
  {
    "objectID": "earth_economy_devstack.html#cgtpag2aggstoregtap10a_gtap_2014_65x141gtapv7",
    "href": "earth_economy_devstack.html#cgtpag2aggstoregtap10a_gtap_2014_65x141gtapv7",
    "title": "DEPRACATED - SEE INDIVIDUAL FILES - Earth-Economy Devstack",
    "section": "C:\\\\GTPAg2\\\\AggStore\\\\GTAP10A_GTAP_2014_65x141\\\\gtapv7",
    "text": "C:\\\\GTPAg2\\\\AggStore\\\\GTAP10A_GTAP_2014_65x141\\\\gtapv7\nGetting the v7 code\n\nExtract from RunGTAP (lol)\nUnder Version, Change, set to NCORS3x3\nUnder Version, New, use wizard using same aggregation and simply copying.\n\n\n\nThis will create a new folder in c:/runGTAP375 named v7all.\nNow we will replace the data files in v7all with the fully disaggregated database (in the v7 dir) we created above.\nNotice also in the v7dir we have shock files, e.g. tinc.shk. \n\nBy default, these will be inherited from the old 3x3 version.\nUnder Tools, Run Test Simulation. This will rewrite new shk files to match aggregation.\n\nALSO NOTE, this is the full run that tested it all worked.\n\nTo check that it worked, go to results, macros. \nOr, could open results in ViewSOL. Use View -&gt; Results Using ViewSOL\n\nViewSOL has the full results, whereas RunGTAP only has a subset by the limited mapping specific to RunGTAP. ViewSOL loads the whole sl4 file.\nHere you could go to e.g. pgdp to see that prices all went up by 10%, which is the default shock i guess?\n\nOR, from ViewSOL File, can open in ViewHAR, which gives even more power, such as dimensional sorting.\n\nNow we can run a non-trivial shock. So for global ag productivity shock, let’s increase productivity.\n\nIn RunGTAP, go to view RunCMFSTART file. Here we will define a new set for the new experiments. (CMFSTART files sets settings for runtime, but also which are the files that should be called, along with sets for the closure).\n\nTo do this, go to RunGTAP-\\&gt;View-\\&gt;Sets, enable advanced editing, Sets-\\&gt;View Set Library\n\nHere click on COMM, copy elements in tablo format. This will get the set definition in tablo format of ALL the commodities. From this you can pare down to which you want to shock,\nFor the moment, we will create two sets, one for ag commodities (ag_comm), and one for a smaller subset of agg commodities (ag_comm_sm).\nAnd will also define what is the complementary set (xag_comm_sm) of ag_comm_sm.\n\n\n\n\n\nNow that the sets are defined, can use them in defining the SHOCKS\n\n\nSet the solution method to gragg 2-4-6\nSave experiment.\nThen Solve!\n\nComment from TOM: If Gragg doesn’t work, use Euler with many steps 20-50\n\n\nFrom second call with Erwin on cmd and rungtap versions\n\nNote that now we have a tax, we do it on both domestic and imported, then write a new equation that makes them sum up: \n\nE_tpm (all, c, COMM)(all, r, REG) tmp(c, r) = tpmall + tpreg + tpall\n\nNote that we now set the rate% of beef tax to increase 50%. Before we were increasing it by a 50% POWER OF THE tariff.\nFirst run testsim.bat.\nThen run the simulations.\nIn rungtapv7.bat then, \nSimresults.bat last. Takes the sl4 files and converts it into a har file (.sol), and then combines them into 1 file, then splits to CSV.\nRMAC is full dimension, RMCA is aggregated by chosen aggregation simplification.\n\nAggMap.har defines the aggregation.\n\nThem Allres.CMF is run, which actually does the aggregation.\n\nAllres.cmf calls to allres.tab, which cleans the results. This would need to have new scenarios added to the top\n\nAllres.EXP also needs to be updated, along with the scenario EXP files to h\nave thecorrect exogenous variables, such as tpdall\n\nalternatively just specify they use hb.get_path() to the private database."
  },
  {
    "objectID": "earth_economy_devstack.html#iterating-over-multiple-aggregations",
    "href": "earth_economy_devstack.html#iterating-over-multiple-aggregations",
    "title": "DEPRACATED - SEE INDIVIDUAL FILES - Earth-Economy Devstack",
    "section": "Iterating over multiple aggregations",
    "text": "Iterating over multiple aggregations\nGTAPPY runs for multiple aggregations and follows the philosophy that for a well-built model, the results will be intuitively similar for different aggregations and thus it serves as a decent check. This is similar to “leave-one-out” testing in regression."
  },
  {
    "objectID": "earth_economy_devstack.html#release-notes",
    "href": "earth_economy_devstack.html#release-notes",
    "title": "DEPRACATED - SEE INDIVIDUAL FILES - Earth-Economy Devstack",
    "section": "Release notes",
    "text": "Release notes\n\nv2023-12-18\nWe need to clarify how we go from a erwin-style mapping xlsx to EE spec. Below is what it looks like as downloaded from erwin.\nGTAP-ctry2reg [source from erwin converted to EE spec].xlsx\n\nManually renamed to gtapv7_r251_r160_correspondence.xlsx (must be excel cause multi workbook). Also need to put the legend information somewhere else in a well-thought-out place (not attached to regions correspondnce)\nNote that eg No. is used twice where the first is r160 and the second is r251. New input mapping should clarify\nSimilar for sectors\n\nFirst note, the word “sector” is specific to the case when you aren’t specifying if you’re talking about COMM (commodities) or ACTS (activities) because I’m not quite sure of the differentiation at this point.\nNotes from Erwin on GTAPPY\n\n\nv2023-12-15\nIssues resolved:\n\nIn the release’s Data folder, the aggregation label gtapaez11-50 should be renamed v11-s26-r50, correct?\nAlso note that I have decided to have the most recent release always have NO timestamp whereas dated versions of same-named models/aggregations should add on the timestamp of when they were first released\nPropose changing the names of the cmf files from cwon_bau.cmf to gtapv7-aez-rd_bau.cmf and cwon_bau-es.cmf to gtapv7-aez-rd_bau-es (note difference between hyphens (which imply same-variable) and underscores, which are used to split into list.)\nPropose not using set PROJ=cwon in the CMF as that is defined by the projectflow object.\npropose changing SIMRUN to just ‘experiment_name’ ie “bau” rather than “projname” + “bau”\nReorganize this so that data is in the base_data_dir and the output is separated from the code release” set MODd=..set CMFd=.\n\nset SOLd=..set DATd=..%AGG%\nTHESE TWO STAY OUTSIDE THE RELEASE\n\nThis basic idea now is that there is a release that someone downloads, and they could run it either by calling the bat file or by calling the python run script. This means i’m trying to make the two different file types as similar as possible. However, note that the bat file is only going to be able to replicate a rd run without ES, so technically the python script can contain a bat file but not vice versa.\nRenamce command line cmf options as tehy’re referenced in the cmf file: # CMF: experiment_label # Rename BUT I understand this one might not be changeable because it appears to be defined by the filename of the CMF? # p1: gtap_base_data_dir # p2: starting_data_file_path # Rename points to the correct starting har # p3: output_dir # Rename # p4: starting_year # Rename # p5: ending_year # Rename\nSimple question: Is there any way to read the raw GEMPACK output to get a sense of how close to complete you are? I would like to make an approximate progress bar.\n\n\nWhen you say “automatic accuracy”, you can.\n+++&gt; Beginning subinterval number 4.\n—&gt; Beginning pass number 1 of 2-pass calculation, subinterval 4.\nBeginning pass number 6 of 6-pass calculation, subinterval 6\n\n\nWould it be possible to not put a Y in front of years like Y2018? This can mess up string-&gt;int conversions.\n\nkeep Y, gempack can’t have non-numeric characters at the start of a var\n\nThere is no bau-SUM_Y2050 (but ther is for VOL and WEL). Is this intentional?\n\nNO! SUM describes the starting database.\nWelfare not possibly in RD because no discount rate eg\n\nQuestion: Is EVERYTHING stored in the SL4? I.e., are the other files redundant?\n\nNo, apparently things like the converting the sl4 to volume terms is not stored in the sl4, so that needs to come in on sltht or in ViewSOL\n\n\nv2023-11-01\n\nShocks implemented\n\nDifferent population file replacements\nGDP change\nYield changes?\n\n\n\nExample run file\n\nAt the top we create the project flow object. We will add tasks to this.\n\n\nExample executable call\n\n\n\nHarder part to do is figuring out how to have the shocks work\nNeed to create xSets, xSubsets, Shock. Could use READFROMFILE command in tablo cmf.\n\n\nWhat are the different kinds of shocks\nUniform Shockaoall(AGCOM_SM, reg) = uniform 20;\nAnother more\naoall(ACTS, REG) = file select from file pointing to a list of all regions. 0 is no shock.\nEverything in the exogenous list can be shocks.\nAlso can SWAP an initially endogenous with exogenous.\nE.g. swap aoreg with GDP\nWhat about changing an elasticity?\nthose are in gtapparm, so write a new .prm (this is not a shock but is just replacing an input.)\nNotice that there are shocks vs data updates.\nThe elasticities are being calibrated??? if you change the basedata to basedata2, then a different default2.prm, with no shock, the model will replicate the initial database.\nIf it’s a supply response elasticity (as in PNAS) that WILL affect it (unlike above).\nneeds to be percentage change over default POP. In base data. Read this in and process it against Eric’s file.\nShock pop(REG) = SELECT FROM FILE filename header 4ltr header. Swap QGDP aoreg shock aoall (agcom_sm, reg) = select from yield file."
  },
  {
    "objectID": "earth_economy_devstack.html#from-readme-might-be-redundant",
    "href": "earth_economy_devstack.html#from-readme-might-be-redundant",
    "title": "DEPRACATED - SEE INDIVIDUAL FILES - Earth-Economy Devstack",
    "section": "From Readme (might be redundant)",
    "text": "From Readme (might be redundant)\nSEALS is under active development and will change much as it moves from a personal research library to supported model. We have submitted this code for publication but are waiting on reviews. For installation and other details, see the SEALS documentation.\nTo run a minimal version of the model, open a terminal/console and navigate to the directory where run_test_seals.py is located. Then, simply run: &gt; python run_test_seals.py\nIn order for the above line to work, you will need to set the project directory and data directory lines in run_test_seals.py. To obtain the base_data necessary, see the SEALS manuscript for the download link.\nTo run a full version of the model, copy run_test_seals.py to a new file (i.e., run_seals.py) and set p.test_mode = False. You may also want to specify a new project directory to keep different runs separate.\n\nimages/2024-01-18-08-25-09.png"
  },
  {
    "objectID": "earth_economy_devstack.html#release-notes-1",
    "href": "earth_economy_devstack.html#release-notes-1",
    "title": "DEPRACATED - SEE INDIVIDUAL FILES - Earth-Economy Devstack",
    "section": "Release Notes",
    "text": "Release Notes\n\nUpdate v0.5.0\nDownloading of base data now works.\n\n\nUpdate v0.4.0\nNow all project flow objects can be set via a scenario_definitions.csv file, allowing for iteration over multiple projects.\nIf no scenario_definitions.csv is present, it will create the file based on the parameters set in the run file."
  },
  {
    "objectID": "earth_economy_devstack.html#numpy-errors",
    "href": "earth_economy_devstack.html#numpy-errors",
    "title": "DEPRACATED - SEE INDIVIDUAL FILES - Earth-Economy Devstack",
    "section": "Numpy errors",
    "text": "Numpy errors\nIf numpy throws “wrong size or changes size binary”: upgrade numpy at the end of the installation process. See for details: https://stackoverflow.com/questions/66060487/valueerror-numpy-ndarray-size-changed-may-indicate-binary-incompatibility-exp"
  },
  {
    "objectID": "earth_economy_devstack.html#mac-specific-errors",
    "href": "earth_economy_devstack.html#mac-specific-errors",
    "title": "DEPRACATED - SEE INDIVIDUAL FILES - Earth-Economy Devstack",
    "section": "Mac specific errors",
    "text": "Mac specific errors\nYour python environment has to have permissions to access and write to the base data folder."
  },
  {
    "objectID": "earth_economy_devstack.html#hazelbean-1",
    "href": "earth_economy_devstack.html#hazelbean-1",
    "title": "DEPRACATED - SEE INDIVIDUAL FILES - Earth-Economy Devstack",
    "section": "Hazelbean",
    "text": "Hazelbean\nSEALS relies on the Hazelbean project, available via PIP. Hazelbean is a collection of geospatial processing tools based on gdal, numpy, scipy, cython, pygeoprocessing, taskgraph, natcap.invest, geopandas and many others to assist in common spatial analysis tasks in sustainability science, ecosystem service assessment, global integrated modelling assessment, natural capital accounting, and/or calculable general equilibrium modelling.\nNote that for hazelbean to work, your computer will need to be configured to compile Cython files to C code. This workflow is tested in a Python 3.10, 64 bit Windows environment. It should work on other system environments, but this is not yet tested."
  },
  {
    "objectID": "earth_economy_devstack.html#project-flow-1",
    "href": "earth_economy_devstack.html#project-flow-1",
    "title": "DEPRACATED - SEE INDIVIDUAL FILES - Earth-Economy Devstack",
    "section": "Project Flow",
    "text": "Project Flow\nOne key component of Hazelbean is that it manages directories, base_data, etc. using a concept called ProjectFlow. ProjectFlow defines a tree of tasks that can easily be run in parallel where needed and keeping track of task-dependencies. ProjectFlow borrows heavily in concept (though not in code) from the task_graph library produced by Rich Sharp but adds a predefined file structure suited to research and exploration tasks."
  },
  {
    "objectID": "earth_economy_devstack.html#first-run-walkthrough-tutorial",
    "href": "earth_economy_devstack.html#first-run-walkthrough-tutorial",
    "title": "DEPRACATED - SEE INDIVIDUAL FILES - Earth-Economy Devstack",
    "section": "First run walkthrough tutorial",
    "text": "First run walkthrough tutorial\nThe simplest way to run SEALS is to clone the repository and then open run_test_seals.py in your preferred editor. Then, update the values in ENVIRONMENT SETTINGS near the top of run_test_seals.py for your local computer (ensuring this points to directories you have write-access for and is not a virtual/cloud directory).\n\n    ### ------- ENVIRONMENT SETTINGS -------------------------------\n\n    # Users should only need to edit lines in this ENVIRONMENT SETTINGS section\n    # Everything is relative to these (or the source code dir).\n    # Specifically,\n    # 1. ensure that the project_dir makes sense for your machine\n    # 2. ensure that the base_data_dir makes sense for your machine\n    # 3. ensure that the data_credentials_path points to a valid credentials file\n    # 4. ensure that the input_bucket_name points to a cloud bucket you have access to\n\n    # A ProjectFlow object is created from the Hazelbean library to organize directories and enable parallel processing.\n    # project-level variables are assigned as attributes to the p object (such as in p.base_data_dir = ... below)\n    # The only agrument for a project flow object is where the project directory is relative to the current_working_directory.\n    user_dir = os.path.expanduser('~')\n    script_dir = os.path.dirname(os.path.realpath(__file__))\n\n    project_name = 'test_seals_project'\n    project_dir = os.path.join(user_dir,  'seals', 'projects', project_name)\n    p = hb.ProjectFlow(project_dir)\n\nThe project name and the project dir will define the root directory where all files will be saved. This directory is given hb.ProjectFlow() to initalize the project (which will create the dirs). Once these are set, you should be able to run run_test_seals.py in your preferred way, ensuring that you are in the Conda environment discussed above. This could be achieved in VS Code by selecting the Conda environment in the bottom-right status bar and then selecting run. Alternatively, this could be done via the command line with the command python run_test_seals.py in the appropriate directory.\nWhen SEALS is run in this way, it will use the default values for a test run on a small country (Rawanda). All of these values are set (and documented) in the run file (run_test_seals.py) in the SET DEFAULT VARIABLES section. For your first run, it is recommended to use the defaults. When run, a configuration file will be written into your project’s input_dir named scenario_definitions.csv. This file is a table where each row is a “scenario” necessary to be defined for SEALS to run. In this minimal run, it must have 2 rows: one for the baseline condition (the starting LULC map) and one for a scenario of change that will indicate how much change of each LU class will happen in some coarse grid-cell or region/zone. Inspecting and/or modifying this file may give insights on how to customize a new run.\n\n### ------- SET DEFAULT VARIABLES --------------------------------\n\n# Set the path to the scenario definitions file. This is a CSV file that defines the scenarios to run.\n# If this file exists, it will load all of the attributes from this file and overwrite the attributes\n# set above. This is useful because adding new lines to to the scenario definitions file will allow\n# you to run many different scenarios easily. If this file does not exist, it will be created based\n# on the attributes set above and saved to the location in scenarios_definitions_path.\np.scenario_definitions_path = os.path.join(p.input_dir, 'scenario_defininitions.csv')\n\n# IMPORTANT NOTE: If you set a scenario_definitions_path, then the attributes set in this file (such as p.scenario_label below)\n# will be overwritten. Conversely, if you don't set a scenario_definitions_path, then the attributes set in this file will be used\n# and will be written to a CSV file in your project's input dir.\n\n# If you did not set a p.scenarios_definitions_path, the following default variables will be used\n# and will be written to a scenarios csv in your project's input_dir for later use/editing/expansion.\n\n# String that uniquely identifies the scenario. Will be referenced by other scenarios for comparison.\np.scenario_label = 'ssp2_rcp45_luh2-globio_bau'\n\n# Scenario type determines if it is historical (baseline) or future (anything else) as well\n# as what the scenario should be compared against. I.e., Policy minus BAU.\np.scenario_type = 'bau'\n\nThis computing stack also uses hazelbean to automatically download needed data at run time. In the code block below, notice the absolute path assigned to p.base_data_dir. Hazelbean will look here for certain files that are necessary and will download them from a cloud bucket if they are not present. This also lets you use the same base data across different projects.\nIn addition to defining a base_data_dir, you will need to For this to work, you need to also point SEALS to the correct data_credentials_path. If you don’t have a credentils file, email jajohns@umn.edu. The data are freely available but are very, very large (and thus expensive to host), so I limit access via credentials.\n\np.base_data_dir = os.path.join('G:/My Drive/Files/base_data')\n\np.data_credentials_path = '..\\\\api_key_credentials.json'\n\nNOTE THAT the final directory has to be named base_data to match the naming convention on the google cloud bucket."
  },
  {
    "objectID": "earth_economy_devstack.html#running-the-model",
    "href": "earth_economy_devstack.html#running-the-model",
    "title": "DEPRACATED - SEE INDIVIDUAL FILES - Earth-Economy Devstack",
    "section": "Running the model",
    "text": "Running the model\nAfter doing the above steps, you should be ready to run run_test_seals.py. Upon starting, SEALS will report the “task tree” of steps that it will compute in the ProjectFlow environment. To understand SEALS in more depth, inspect each of the functions that define these tasks for more documention in the code.\nOnce the model is complete, go to your project directory, and then the intermediate directory. There you will see one directory for each of the tasks in the task tree. To get the final produce, go to the stitched_lulc_simplified_scenarios directory. There you will see the base_year lulc and the newly projected lulc map for the future year:\n [THIS IS NOT THE CORRECT IMAGE]\nOpen up the projected one (e.g., lulc_ssp2_rcp45_luh2-message_bau_2045.tif) in QGIS and enjoy your new, high-resolution land-use change projection!"
  },
  {
    "objectID": "earth_economy_devstack.html#more-seals-details",
    "href": "earth_economy_devstack.html#more-seals-details",
    "title": "DEPRACATED - SEE INDIVIDUAL FILES - Earth-Economy Devstack",
    "section": "More SEALS details",
    "text": "More SEALS details\n\nLinking paths\nThe logic used for paths in SEALS is confusing at first, but it enables efficient processing of very large files (and minimizes re-downloading or re-processing data. When a path to a file is specified in SEALS, often SEALS will call a function in hazelbean that checks multiple places for that file to exist, as in the code below.\n\npath = hb.get_first_extant_path(path, [p.input_dir, p.base_data_dir])\n\nIf the path given is relative, this function will then iterate through the list of possible directories and join the path to each directory, continuing down the list until it finds an existing path. If it never finds it, the function will return the path joined with the first element in the list (presumably for later file creation). This functionality means that if a file is in your base data directory, hazelbean will not download it. As a protip, after the first time it’s downloaded, you can move it from the input_dir to your base_data_dir (while keeping the exact same relative directory path) and then Hazelbean will not waste time re-downloading."
  },
  {
    "objectID": "earth_economy_devstack.html#variable-and-scenario-naming-conventions",
    "href": "earth_economy_devstack.html#variable-and-scenario-naming-conventions",
    "title": "DEPRACATED - SEE INDIVIDUAL FILES - Earth-Economy Devstack",
    "section": "Variable and Scenario Naming Conventions",
    "text": "Variable and Scenario Naming Conventions\nTo keep track of the MANY different filetypes, data processes, variables, scenarios, policies etc, please follow exactly the specifications below.\n\nThe word label refers to a relatively short string (preferably 8 characters long or less) with no spaces, underscores or punctuation (but may have hyphens). This is case-sensitive, but try to avoid capitalization.\nThe word short_label refers to a label that is strictly less or equal to 8 characters to ensure compatibility with HAR files.\nThe word name refers to a longer string that describes a specific label with a 1:1 correspondence. Will usually be defined via a correspondence dictionary.\nThe words index, indices or id refers to numerical data that describes a label with a 1:1 correspondence. Will usually be defined via a correspondence dictionary. If both are used, index/indices refer to a unique, ordered list of ints while an id/ids refer are unique but not necessarily ordered.\nThe word class refers to LULC class. Consider renaming this to lc-class?\nScenarios are defined in the following nested structure:\n\nLabel (with no hyphens) for Exogenous Assumptions (e.g., which SSP, which GDP, which population). Typically this will be fully defined by the SSP.\nLabel (with no hyphens) for Climate Assumption (which RCP)\nLabel (can have hyphens) for which model is used (e.g., magpie, luh2-message). Only model is allowed to have hyphens (because they are used for multistep scenario processing of counterfactuals)\nLabel for Counterfactual. This often represent policy Assumptions/Definition and can include BAU, which is a special counterfactual against which other policies are compared. Different counterfactuals correspond to different shockfiles in the econ model or different LUC projection priorities, etc.\n\nCounterfactuals may have multiple processing steps, which will be denoted by appending a hyphen and exactly 4 chars to the end of the base counterfactual label.\n\nIFor example, a run excludes consideration of ES, insert “-noes”, at the end of the policy_name if it does include ES, postpend nothing (as this will be the one that is referenced by default)\n\n\nYear\n\nWhen the variable is singular, it must be an int. If it is plural, as is ints in a list. However, when either is stored in a dataframe, always type always type check as follows:\n\nIf singular, do str(value), int(value) or float(value) as appropriate when reading from the df into a python variable.\nIf plural, assume the df value is a space-delimited string that needs to be split, e.g. as [int(i) for i in value.split(’ ‘)], or’ ’.join(values) if going into the DF\n\nThree types of years exist, including\n\np.base_years, (which recall will always be a list even if there is a single entry because the variable name is plural)\n\n\n\nTogether, the labels above mean that the scenarios can be represented by directories as follows:\n\nssp2/rcp45/luh2-message/bau/filename_2050.tif\n\nNote, the last layer of the hierarchy will be included as a the suffix of the filename start rather than as a directory (also see below for filename conventions)\n\n\nFor filenames, there are two possible conventions:\n\nImplied: This means that the directory structure above defines all the labels with the exception of year (which is postpended to the filename label) and the current variable name (such as lulc) which appears at the front of the filename.\n\ne.g., project/intermediate/convert_netcdf/ssp2/rcp45/luh2-message/bau/lulc_2050.tif\n\nExplicit: Even if a file is in a directory which implies its labels, explicit file naming will always include each label (and the variable label stays in front of the filename), so the above example is:\n\nproject/intermediate/convert_netcdf/ssp2/rcp45/luh2-message/bau/lulc_ssp2_rcp45_bau_luh2-message_2050.tif\nAnd if there are no ES considered, it would be project/intermediate/convert_netcdf/ssp2/rcp45/luh2-message/bau/lulc_ssp2_rcp45_bau_luh2-message_2050.tif\n\n\nBecause labels have no spaces or underscores, it is possible to convert the nested structure above to a single string, as in filename_ssp2_rcp45_policyname_year.tif.\nFiletypes that supported in this computation environment include\n\nNetcdf with the above listed dimensions in the same order\nA set of geotiffs embedded in directories. Each label gets a directory level expect for year, which by convention, will ALWAYS be the last 4 characters of a filename before the extension (with an underscore before it).\nA spreadsheet linkable to a geographic representation (e.g., a shapefile or a geopackage) in vertical format\n\nAlso we will create a set of tables to analyze results\n\nThese will define Regions (shp) for quick result plotting\nSpecifically, we will have a full vertically stacked CSV of results, then for each Report Archetype we would output 1 minimal info CSV and the corresponding Figure.\n\nMiscellaneous:\n\nbase_years is correct, never baseline_years (due to confusion between baseline and bau)\n\nScenario types\n\nThree scenario_types are supported: baseline, bau and policy\n\nBaseline assumes the year has data existing from observations (rather than modelled) and that these years are defined in p.years (and identically defined in p.base_years).\n\nOne exception is when eg GTAP is used to update the base year from 2017 to 2023, and then policies are applied on 2023.\n\nBAU and policy scenarios assume the results are modelled and that their years are defined in p.years (but not p.base_years)\n\nClarify what is the naming difference between src dst versus input output. Is the former only for file paths or can it also be e.g. array. OR does this have to do with if it is a function return.\n\nProposed answer: src/dst is a pointer/reference to a thing and Input/Output is the thing itself. Esp useful for paths.\nSimilarly, _path and _dir imply the string is a reference, so src_path and src_dir are common.\nYou might often see e.g. input_array = hb.as_array(src_path), illustrating this difference."
  },
  {
    "objectID": "global_invest.html",
    "href": "global_invest.html",
    "title": "Global InVEST Introduction",
    "section": "",
    "text": "Global InVEST Introduction\nNYI"
  },
  {
    "objectID": "gtappy.html",
    "href": "gtappy.html",
    "title": "GTAPPy",
    "section": "",
    "text": "Step 1:\n\nInstall RunGTAP at https://www.gtap.agecon.purdue.edu/products/rungtap/default.asp\nAt https://www.copsmodels.com/gpeidl.htm download: https://www.copsmodels.com/ftp/ei12dl/gpei-12.1.004-install.exe \n\nOR FOR VERSION 12: https://www.copsmodels.com/ftp/ei12dl/gpei-12.0.004-install.exe\n\nInstall to default location\n\n\nProceed without selecting a license (to start the 6 month trial).\nStep 2: Install GTAPAgg2: https://www.gtap.agecon.purdue.edu/private/secured.asp?Sec_ID=1055\nTO PROCEED, YOU NEED TO HAVE A ACCESS TO THE GTAP DATABASE. OR YOU CAN ACCESS IT THROUGH ee_internal DATABASE.\n\nStep 2.1: - Extract the .lic file from https://www.gtap.agecon.purdue.edu/databases/download.asp step 2. Put this in GTAPAgg2 dir\nStep 2.2: - Install the GTAP Database itself from at : https://www.gtap.agecon.purdue.edu/databases/download.asp\n\nMake sure to also get the AEZ version, which will also give a full pkg file.\n\n\n\nPut this .pkg file in GTPAg2 dir. \nLaunch GTPAg2, identify pkg file for both default and AEZ data\n\nWhen running aggregations, make sure to choose the right product:\n\nStep 2.3: Running the unaggregated version - Open GTPAg2.exe\n\n\nUse this to aggregate a 1-1 version.\n\nCreate a 1-1 mapping\n\nView change regional aggregation, sector aggregation, setting to 1:1\nFor factor aggregation, it defaults to 8-5, can set this to 8:8.\n\nRead aggregation scheme from file, loa default.agg is 10 by 10\nFor factors, if using AEZ, there is land specified by each AEZ. Erwin is working to fix this. Because didn’t have it ready, went back to standard GTAP package of data.\nUsed full 8 factors, make new things “mobile factors”.\n\n\n\n\nSave aggregation scheme to file to AggStore\n\n\nSave aggregation scheme to file to AggStore\n\nThis could also be done by making the .agg file via text editing. \n\nThen finally Create aggregated database in GTAPAgg\n\n\nNote that this creates two versions of the database, one for GTAP code v6.2, one for v7.0. The 7.0 is in a sub zip folder gtapv7.zip. This is the one we want.\n\n\n\n\nGetting the v7 code\n\nExtract from RunGTAP (lol)\nUnder Version, Change, set to NCORS3x3\nUnder Version, New, use wizard using same aggregation and simply copying.\n\n\n\nThis will create a new folder in c:/runGTAP375 named v7all.\nNow we will replace the data files in v7all with the fully disaggregated database (in the v7 dir) we created above.\nNotice also in the v7dir we have shock files, e.g. tinc.shk. \n\nBy default, these will be inherited from the old 3x3 version.\nUnder Tools, Run Test Simulation. This will rewrite new shk files to match aggregation.\n\nALSO NOTE, this is the full run that tested it all worked.\n\nTo check that it worked, go to results, macros. \nOr, could open results in ViewSOL. Use View -&gt; Results Using ViewSOL\n\nViewSOL has the full results, whereas RunGTAP only has a subset by the limited mapping specific to RunGTAP. ViewSOL loads the whole sl4 file.\nHere you could go to e.g. pgdp to see that prices all went up by 10%, which is the default shock i guess?\n\nOR, from ViewSOL File, can open in ViewHAR, which gives even more power, such as dimensional sorting.\n\nNow we can run a non-trivial shock. So for global ag productivity shock, let’s increase productivity.\n\nIn RunGTAP, go to view RunCMFSTART file. Here we will define a new set for the new experiments. (CMFSTART files sets settings for runtime, but also which are the files that should be called, along with sets for the closure).\n\nTo do this, go to RunGTAP-\\&gt;View-\\&gt;Sets, enable advanced editing, Sets-\\&gt;View Set Library\n\nHere click on COMM, copy elements in tablo format. This will get the set definition in tablo format of ALL the commodities. From this you can pare down to which you want to shock,\nFor the moment, we will create two sets, one for ag commodities (ag_comm), and one for a smaller subset of agg commodities (ag_comm_sm).\nAnd will also define what is the complementary set (xag_comm_sm) of ag_comm_sm.\n\n\n\n\n\nNow that the sets are defined, can use them in defining the SHOCKS\n\n\nSet the solution method to gragg 2-4-6\nSave experiment.\nThen Solve!\n\nComment from TOM: If Gragg doesn’t work, use Euler with many steps 20-50\n\n\nFrom second call with Erwin on cmd and rungtap versions\n\nNote that now we have a tax, we do it on both domestic and imported, then write a new equation that makes them sum up: \n\nE_tpm (all, c, COMM)(all, r, REG) tmp(c, r) = tpmall + tpreg + tpall\n\nNote that we now set the rate% of beef tax to increase 50%. Before we were increasing it by a 50% POWER OF THE tariff.\nFirst run testsim.bat.\nThen run the simulations.\nIn rungtapv7.bat then, \nSimresults.bat last. Takes the sl4 files and converts it into a har file (.sol), and then combines them into 1 file, then splits to CSV.\nRMAC is full dimension, RMCA is aggregated by chosen aggregation simplification.\n\nAggMap.har defines the aggregation.\n\nThem Allres.CMF is run, which actually does the aggregation.\n\nAllres.cmf calls to allres.tab, which cleans the results. This would need to have new scenarios added to the top\n\nAllres.EXP also needs to be updated, along with the scenario EXP files to h\nave thecorrect exogenous variables, such as tpdall\n\nalternatively just specify they use hb.get_path() to the private database.\n\n\n\nGTAPPY runs for multiple aggregations and follows the philosophy that for a well-built model, the results will be intuitively similar for different aggregations and thus it serves as a decent check. This is similar to “leave-one-out” testing in regression.\n\n\n\n\n\nWe need to clarify how we go from a erwin-style mapping xlsx to EE spec. Below is what it looks like as downloaded from erwin.\nGTAP-ctry2reg [source from erwin converted to EE spec].xlsx\n\nManually renamed to gtapv7_r251_r160_correspondence.xlsx (must be excel cause multi workbook). Also need to put the legend information somewhere else in a well-thought-out place (not attached to regions correspondnce)\nNote that eg No. is used twice where the first is r160 and the second is r251. New input mapping should clarify\nSimilar for sectors\n\nFirst note, the word “sector” is specific to the case when you aren’t specifying if you’re talking about COMM (commodities) or ACTS (activities) because I’m not quite sure of the differentiation at this point.\nNotes from Erwin on GTAPPY\n\n\n\nIssues resolved:\n\nIn the release’s Data folder, the aggregation label gtapaez11-50 should be renamed v11-s26-r50, correct?\nAlso note that I have decided to have the most recent release always have NO timestamp whereas dated versions of same-named models/aggregations should add on the timestamp of when they were first released\nPropose changing the names of the cmf files from cwon_bau.cmf to gtapv7-aez-rd_bau.cmf and cwon_bau-es.cmf to gtapv7-aez-rd_bau-es (note difference between hyphens (which imply same-variable) and underscores, which are used to split into list.)\nPropose not using set PROJ=cwon in the CMF as that is defined by the projectflow object.\npropose changing SIMRUN to just ‘experiment_name’ ie “bau” rather than “projname” + “bau”\nReorganize this so that data is in the base_data_dir and the output is separated from the code release” set MODd=..set CMFd=.\n\nset SOLd=..set DATd=..%AGG%\nTHESE TWO STAY OUTSIDE THE RELEASE\n\nThis basic idea now is that there is a release that someone downloads, and they could run it either by calling the bat file or by calling the python run script. This means i’m trying to make the two different file types as similar as possible. However, note that the bat file is only going to be able to replicate a rd run without ES, so technically the python script can contain a bat file but not vice versa.\nRenamce command line cmf options as tehy’re referenced in the cmf file: # CMF: experiment_label # Rename BUT I understand this one might not be changeable because it appears to be defined by the filename of the CMF? # p1: gtap_base_data_dir # p2: starting_data_file_path # Rename points to the correct starting har # p3: output_dir # Rename # p4: starting_year # Rename # p5: ending_year # Rename\nSimple question: Is there any way to read the raw GEMPACK output to get a sense of how close to complete you are? I would like to make an approximate progress bar.\n\n\nWhen you say “automatic accuracy”, you can.\n+++&gt; Beginning subinterval number 4.\n—&gt; Beginning pass number 1 of 2-pass calculation, subinterval 4.\nBeginning pass number 6 of 6-pass calculation, subinterval 6\n\n\nWould it be possible to not put a Y in front of years like Y2018? This can mess up string-&gt;int conversions.\n\nkeep Y, gempack can’t have non-numeric characters at the start of a var\n\nThere is no bau-SUM_Y2050 (but ther is for VOL and WEL). Is this intentional?\n\nNO! SUM describes the starting database.\nWelfare not possibly in RD because no discount rate eg\n\nQuestion: Is EVERYTHING stored in the SL4? I.e., are the other files redundant?\n\nNo, apparently things like the converting the sl4 to volume terms is not stored in the sl4, so that needs to come in on sltht or in ViewSOL\n\n\n\n\n\n\nDifferent population file replacements\nGDP change\nYield changes?\n\n\n\n\n\nAt the top we create the project flow object. We will add tasks to this.\n\n\n\n\n\n\n\nNeed to create xSets, xSubsets, Shock. Could use READFROMFILE command in tablo cmf.\n\n\n\nUniform Shockaoall(AGCOM_SM, reg) = uniform 20;\nAnother more\naoall(ACTS, REG) = file select from file pointing to a list of all regions. 0 is no shock.\nEverything in the exogenous list can be shocks.\nAlso can SWAP an initially endogenous with exogenous.\nE.g. swap aoreg with GDP\nWhat about changing an elasticity?\nthose are in gtapparm, so write a new .prm (this is not a shock but is just replacing an input.)\nNotice that there are shocks vs data updates.\nThe elasticities are being calibrated??? if you change the basedata to basedata2, then a different default2.prm, with no shock, the model will replicate the initial database.\nIf it’s a supply response elasticity (as in PNAS) that WILL affect it (unlike above).\nneeds to be percentage change over default POP. In base data. Read this in and process it against Eric’s file.\nShock pop(REG) = SELECT FROM FILE filename header 4ltr header. Swap QGDP aoreg shock aoall (agcom_sm, reg) = select from yield file."
  },
  {
    "objectID": "gtappy.html#installation",
    "href": "gtappy.html#installation",
    "title": "GTAPPy",
    "section": "",
    "text": "Step 1:\n\nInstall RunGTAP at https://www.gtap.agecon.purdue.edu/products/rungtap/default.asp\nAt https://www.copsmodels.com/gpeidl.htm download: https://www.copsmodels.com/ftp/ei12dl/gpei-12.1.004-install.exe \n\nOR FOR VERSION 12: https://www.copsmodels.com/ftp/ei12dl/gpei-12.0.004-install.exe\n\nInstall to default location\n\n\nProceed without selecting a license (to start the 6 month trial).\nStep 2: Install GTAPAgg2: https://www.gtap.agecon.purdue.edu/private/secured.asp?Sec_ID=1055\nTO PROCEED, YOU NEED TO HAVE A ACCESS TO THE GTAP DATABASE. OR YOU CAN ACCESS IT THROUGH ee_internal DATABASE.\n\nStep 2.1: - Extract the .lic file from https://www.gtap.agecon.purdue.edu/databases/download.asp step 2. Put this in GTAPAgg2 dir\nStep 2.2: - Install the GTAP Database itself from at : https://www.gtap.agecon.purdue.edu/databases/download.asp\n\nMake sure to also get the AEZ version, which will also give a full pkg file.\n\n\n\nPut this .pkg file in GTPAg2 dir. \nLaunch GTPAg2, identify pkg file for both default and AEZ data\n\nWhen running aggregations, make sure to choose the right product:\n\nStep 2.3: Running the unaggregated version - Open GTPAg2.exe\n\n\nUse this to aggregate a 1-1 version.\n\nCreate a 1-1 mapping\n\nView change regional aggregation, sector aggregation, setting to 1:1\nFor factor aggregation, it defaults to 8-5, can set this to 8:8.\n\nRead aggregation scheme from file, loa default.agg is 10 by 10\nFor factors, if using AEZ, there is land specified by each AEZ. Erwin is working to fix this. Because didn’t have it ready, went back to standard GTAP package of data.\nUsed full 8 factors, make new things “mobile factors”.\n\n\n\n\nSave aggregation scheme to file to AggStore\n\n\nSave aggregation scheme to file to AggStore\n\nThis could also be done by making the .agg file via text editing. \n\nThen finally Create aggregated database in GTAPAgg\n\n\nNote that this creates two versions of the database, one for GTAP code v6.2, one for v7.0. The 7.0 is in a sub zip folder gtapv7.zip. This is the one we want."
  },
  {
    "objectID": "gtappy.html#cgtpag2aggstoregtap10a_gtap_2014_65x141gtapv7",
    "href": "gtappy.html#cgtpag2aggstoregtap10a_gtap_2014_65x141gtapv7",
    "title": "GTAPPy",
    "section": "",
    "text": "Getting the v7 code\n\nExtract from RunGTAP (lol)\nUnder Version, Change, set to NCORS3x3\nUnder Version, New, use wizard using same aggregation and simply copying.\n\n\n\nThis will create a new folder in c:/runGTAP375 named v7all.\nNow we will replace the data files in v7all with the fully disaggregated database (in the v7 dir) we created above.\nNotice also in the v7dir we have shock files, e.g. tinc.shk. \n\nBy default, these will be inherited from the old 3x3 version.\nUnder Tools, Run Test Simulation. This will rewrite new shk files to match aggregation.\n\nALSO NOTE, this is the full run that tested it all worked.\n\nTo check that it worked, go to results, macros. \nOr, could open results in ViewSOL. Use View -&gt; Results Using ViewSOL\n\nViewSOL has the full results, whereas RunGTAP only has a subset by the limited mapping specific to RunGTAP. ViewSOL loads the whole sl4 file.\nHere you could go to e.g. pgdp to see that prices all went up by 10%, which is the default shock i guess?\n\nOR, from ViewSOL File, can open in ViewHAR, which gives even more power, such as dimensional sorting.\n\nNow we can run a non-trivial shock. So for global ag productivity shock, let’s increase productivity.\n\nIn RunGTAP, go to view RunCMFSTART file. Here we will define a new set for the new experiments. (CMFSTART files sets settings for runtime, but also which are the files that should be called, along with sets for the closure).\n\nTo do this, go to RunGTAP-\\&gt;View-\\&gt;Sets, enable advanced editing, Sets-\\&gt;View Set Library\n\nHere click on COMM, copy elements in tablo format. This will get the set definition in tablo format of ALL the commodities. From this you can pare down to which you want to shock,\nFor the moment, we will create two sets, one for ag commodities (ag_comm), and one for a smaller subset of agg commodities (ag_comm_sm).\nAnd will also define what is the complementary set (xag_comm_sm) of ag_comm_sm.\n\n\n\n\n\nNow that the sets are defined, can use them in defining the SHOCKS\n\n\nSet the solution method to gragg 2-4-6\nSave experiment.\nThen Solve!\n\nComment from TOM: If Gragg doesn’t work, use Euler with many steps 20-50\n\n\nFrom second call with Erwin on cmd and rungtap versions\n\nNote that now we have a tax, we do it on both domestic and imported, then write a new equation that makes them sum up: \n\nE_tpm (all, c, COMM)(all, r, REG) tmp(c, r) = tpmall + tpreg + tpall\n\nNote that we now set the rate% of beef tax to increase 50%. Before we were increasing it by a 50% POWER OF THE tariff.\nFirst run testsim.bat.\nThen run the simulations.\nIn rungtapv7.bat then, \nSimresults.bat last. Takes the sl4 files and converts it into a har file (.sol), and then combines them into 1 file, then splits to CSV.\nRMAC is full dimension, RMCA is aggregated by chosen aggregation simplification.\n\nAggMap.har defines the aggregation.\n\nThem Allres.CMF is run, which actually does the aggregation.\n\nAllres.cmf calls to allres.tab, which cleans the results. This would need to have new scenarios added to the top\n\nAllres.EXP also needs to be updated, along with the scenario EXP files to h\nave thecorrect exogenous variables, such as tpdall\n\nalternatively just specify they use hb.get_path() to the private database."
  },
  {
    "objectID": "gtappy.html#iterating-over-multiple-aggregations",
    "href": "gtappy.html#iterating-over-multiple-aggregations",
    "title": "GTAPPy",
    "section": "",
    "text": "GTAPPY runs for multiple aggregations and follows the philosophy that for a well-built model, the results will be intuitively similar for different aggregations and thus it serves as a decent check. This is similar to “leave-one-out” testing in regression."
  },
  {
    "objectID": "gtappy.html#release-notes",
    "href": "gtappy.html#release-notes",
    "title": "GTAPPy",
    "section": "",
    "text": "We need to clarify how we go from a erwin-style mapping xlsx to EE spec. Below is what it looks like as downloaded from erwin.\nGTAP-ctry2reg [source from erwin converted to EE spec].xlsx\n\nManually renamed to gtapv7_r251_r160_correspondence.xlsx (must be excel cause multi workbook). Also need to put the legend information somewhere else in a well-thought-out place (not attached to regions correspondnce)\nNote that eg No. is used twice where the first is r160 and the second is r251. New input mapping should clarify\nSimilar for sectors\n\nFirst note, the word “sector” is specific to the case when you aren’t specifying if you’re talking about COMM (commodities) or ACTS (activities) because I’m not quite sure of the differentiation at this point.\nNotes from Erwin on GTAPPY\n\n\n\nIssues resolved:\n\nIn the release’s Data folder, the aggregation label gtapaez11-50 should be renamed v11-s26-r50, correct?\nAlso note that I have decided to have the most recent release always have NO timestamp whereas dated versions of same-named models/aggregations should add on the timestamp of when they were first released\nPropose changing the names of the cmf files from cwon_bau.cmf to gtapv7-aez-rd_bau.cmf and cwon_bau-es.cmf to gtapv7-aez-rd_bau-es (note difference between hyphens (which imply same-variable) and underscores, which are used to split into list.)\nPropose not using set PROJ=cwon in the CMF as that is defined by the projectflow object.\npropose changing SIMRUN to just ‘experiment_name’ ie “bau” rather than “projname” + “bau”\nReorganize this so that data is in the base_data_dir and the output is separated from the code release” set MODd=..set CMFd=.\n\nset SOLd=..set DATd=..%AGG%\nTHESE TWO STAY OUTSIDE THE RELEASE\n\nThis basic idea now is that there is a release that someone downloads, and they could run it either by calling the bat file or by calling the python run script. This means i’m trying to make the two different file types as similar as possible. However, note that the bat file is only going to be able to replicate a rd run without ES, so technically the python script can contain a bat file but not vice versa.\nRenamce command line cmf options as tehy’re referenced in the cmf file: # CMF: experiment_label # Rename BUT I understand this one might not be changeable because it appears to be defined by the filename of the CMF? # p1: gtap_base_data_dir # p2: starting_data_file_path # Rename points to the correct starting har # p3: output_dir # Rename # p4: starting_year # Rename # p5: ending_year # Rename\nSimple question: Is there any way to read the raw GEMPACK output to get a sense of how close to complete you are? I would like to make an approximate progress bar.\n\n\nWhen you say “automatic accuracy”, you can.\n+++&gt; Beginning subinterval number 4.\n—&gt; Beginning pass number 1 of 2-pass calculation, subinterval 4.\nBeginning pass number 6 of 6-pass calculation, subinterval 6\n\n\nWould it be possible to not put a Y in front of years like Y2018? This can mess up string-&gt;int conversions.\n\nkeep Y, gempack can’t have non-numeric characters at the start of a var\n\nThere is no bau-SUM_Y2050 (but ther is for VOL and WEL). Is this intentional?\n\nNO! SUM describes the starting database.\nWelfare not possibly in RD because no discount rate eg\n\nQuestion: Is EVERYTHING stored in the SL4? I.e., are the other files redundant?\n\nNo, apparently things like the converting the sl4 to volume terms is not stored in the sl4, so that needs to come in on sltht or in ViewSOL\n\n\n\n\n\n\nDifferent population file replacements\nGDP change\nYield changes?\n\n\n\n\n\nAt the top we create the project flow object. We will add tasks to this.\n\n\n\n\n\n\n\nNeed to create xSets, xSubsets, Shock. Could use READFROMFILE command in tablo cmf.\n\n\n\nUniform Shockaoall(AGCOM_SM, reg) = uniform 20;\nAnother more\naoall(ACTS, REG) = file select from file pointing to a list of all regions. 0 is no shock.\nEverything in the exogenous list can be shocks.\nAlso can SWAP an initially endogenous with exogenous.\nE.g. swap aoreg with GDP\nWhat about changing an elasticity?\nthose are in gtapparm, so write a new .prm (this is not a shock but is just replacing an input.)\nNotice that there are shocks vs data updates.\nThe elasticities are being calibrated??? if you change the basedata to basedata2, then a different default2.prm, with no shock, the model will replicate the initial database.\nIf it’s a supply response elasticity (as in PNAS) that WILL affect it (unlike above).\nneeds to be percentage change over default POP. In base data. Read this in and process it against Eric’s file.\nShock pop(REG) = SELECT FROM FILE filename header 4ltr header. Swap QGDP aoreg shock aoall (agcom_sm, reg) = select from yield file."
  },
  {
    "objectID": "hazelbean.html",
    "href": "hazelbean.html",
    "title": "Hazelbean Introduction",
    "section": "",
    "text": "Hazelbean is a collection of geospatial processing tools based on gdal, numpy, scipy, cython, pygeoprocessing, taskgraph, natcap.invest, geopandas and many others to assist in common spatial analysis tasks in sustainability science, ecosystem service assessment, global integrated modelling assessment, natural capital accounting, and/or calculable general equilibrium modelling.\nNote that for all of the features of hazelbean to work, your computer will need to be configured to compile Cython files to C code. This workflow is tested in a Python 3.10, 64 bit Windows environment. It should work on other system environments, but this is not yet tested."
  },
  {
    "objectID": "hazelbean.html#installation",
    "href": "hazelbean.html#installation",
    "title": "Hazelbean Introduction",
    "section": "Installation",
    "text": "Installation\nFollow the instructions in the Earth-Economy Devstack repository."
  },
  {
    "objectID": "hazelbean.html#quickstart",
    "href": "hazelbean.html#quickstart",
    "title": "Hazelbean Introduction",
    "section": "Quickstart",
    "text": "Quickstart\nTest that hazelbean imports correctly. Importing it will trigger compilation of the C files if they are not there.\n\nimport hazelbean as hb\n\nFrom here, explore examples of the useful spatial, statistical and economic functions in the examples section of this documentation. A good starting example would be the zonal_statistics function."
  },
  {
    "objectID": "hazelbean.html#project-complexity-level-1-simple-question-answered-well",
    "href": "hazelbean.html#project-complexity-level-1-simple-question-answered-well",
    "title": "Hazelbean Introduction",
    "section": "Project complexity level 1: Simple question answered well",
    "text": "Project complexity level 1: Simple question answered well\nHere is an example script that you might write as an Earth-economy researcher. Suppose your adviser asks you “what is the total caloric yield on earth per hectare?” You might write a script like this:\n\nimport os\nimport numpy as np\nimport gdal\n\nyield_per_hectare_raster_path = os.path.join('data', 'yield_per_cell.tif')\nyield_per_hectare_raster = gdal.Open(yield_per_hectare_raster_path)\nyield_per_hectare_array = yield_per_hectare_raster.ReadAsArray()\n\nsum_of_yield = np.sum(yield_per_hectare_array)\n\nprint('The total caloric yield on earth per hectare is: ' + str(sum_of_yield))"
  },
  {
    "objectID": "hazelbean.html#project-complexity-level-2-many-similar-questions.-creates-a-very-long-list.",
    "href": "hazelbean.html#project-complexity-level-2-many-similar-questions.-creates-a-very-long-list.",
    "title": "Hazelbean Introduction",
    "section": "Project complexity level 2: Many similar questions. Creates a very long list.",
    "text": "Project complexity level 2: Many similar questions. Creates a very long list.\nThis is where most reserach code goes to die, in my experience. Suppose your advisor now asks okay do this for the a bunch of different datasets on yield. The classic coder response is to make a longer script!\n\nimport os\nimport numpy as np\nimport gdal\n\nyield_per_hectare_raster_path_1 = os.path.join('data', 'yield_per_cell_1.tif')\nyield_per_hectare_raster_1 = gdal.Open(yield_per_hectare_raster_path_1)\nyield_per_hectare_array_1 = yield_per_hectare_raster_1.ReadAsArray()\n\nsum_of_yield_1 = np.sum(yield_per_hectare_array_1)\n\nprint('The total caloric yield on earth per hectare for dataset 1 is: ' + str(sum_of_yield_1))\n\nyield_per_hectare_raster_path_2 = os.path.join('data', 'yield_per_cell_2.tif')\nyield_per_hectare_raster_2 = gdal.Open(yield_per_hectare_raster_path_2)\nyield_per_hectare_array_2 = yield_per_hectare_raster_2.ReadAsArray()\n\nsum_of_yield_2 = np.sum(yield_per_hectare_array_2)\n\nprint('The total caloric yield on earth per hectare for dataset 2 is: ' + str(sum_of_yield_2))\n\nyield_per_hectare_raster_path_3 = os.path.join('data', 'yield_per_cell_3.tif')\nyield_per_hectare_raster_3 = gdal.Open(yield_per_hectare_raster_path_3)\nyield_per_hectare_array_3 = yield_per_hectare_raster_3.ReadAsArray()\n\nsum_of_yield_3 = np.sum(yield_per_hectare_array_2)\n\nprint('The total caloric yield on earth per hectare for dataset 3 is: ' + str(sum_of_yield_3))\n\nyield_per_hectare_raster_path_4 = os.path.join('data', 'yield_per_cell_4.tif')\nyield_per_hectare_raster_4 = gdal.Open(yield_per_hectare_raster_path_4)\nyield_per_hectare_array_4 = yield_per_hectare_raster_4.ReadAsArray()\n\nsum_of_yield_4 = np.sum(yield_per_hectare_array_4)\n\nprint('The total caloric yield on earth per hectare for dataset 4 is: ' + str(sum_of_yield_4))\n\nThis style of coding works, but will quickly cause you to lose your sanity. Who can find the reason the above code will cause your article to be retracted? Also, what if each of those summations takes a long time and you want to make a small change? You have to rerun the whole thing. This is bad."
  },
  {
    "objectID": "hazelbean.html#project-complexity-level-3-starting-to-deal-with-performance-and-generalization.",
    "href": "hazelbean.html#project-complexity-level-3-starting-to-deal-with-performance-and-generalization.",
    "title": "Hazelbean Introduction",
    "section": "Project complexity level 3: Starting to deal with performance and generalization.",
    "text": "Project complexity level 3: Starting to deal with performance and generalization.\nBelow is a real-life script I created in around 2017 to calculate something for Johnson et al. 2016. Do not even attempt to run this, but just appreciate how awful it is. Please skim past it quickly to save me the personal embarassment!\n\n# import logging\n# import os\n# import csv\n# import math, time, random\n\n# import numpy\n# np = numpy\n# from osgeo import gdal, gdalconst\n# import geoecon_utils.geoecon_utils as gu\n# from gdal import *\n# import pyximport\n# pyximport.install(setup_args={\"script_args\":[\"--compiler=mingw32\"],\"include_dirs\":numpy.get_include()}, reload_support=True)\n# import geoecon_utils.geoecon_cython_utils as gcu\n\n# log_id = gu.pretty_time()\n# LOGGER = logging.getLogger('ag_tradeoffs')\n# LOGGER.setLevel(logging.WARN) # warn includes the final output of the whole model, carbon saved.\n# file_handler = logging.FileHandler('logs/ag_tradeoffs_log_' + log_id + '.log')\n# LOGGER.addHandler(file_handler)\n\n# # Set Input files\n# workspace = 'E:/bulk_data/reusch_and_gibbs_carbon/data/datasets/c_1km/'\n# c_1km_file = 'c_1km.tif'\n# c_1km_uri = workspace + c_1km_file\n# ha_per_cell_30s_file = 'ha_per_cell_30s.tif'\n# ha_per_cell_30s_uri = workspace + ha_per_cell_30s_file\n# ha_per_cell_5m_file = 'ha_per_cell_5m.tif'\n# ha_per_cell_5m_uri = workspace + ha_per_cell_5m_file\n# ha_per_cell_5m = gu.as_array(ha_per_cell_5m_uri)\n\n\n\n# # Do the resample, or load from previously made to speed things up.\n# #sOLD NOTE: and resample it, then rescale it, then test to make sure teh averageing method doesn't undereestimate on the south shores of london, then use it.'\n# do_30s_resample = False\n# if do_30s_resample:\n#     # Define desired resample details. In this case, I am converting to 10x resolution of 5 min data (30 sec)\n#     desired_geotrans = (-180.0, 0.008333333333333, 0.0, 90.0, 0.0, -0.008333333333333)\n#     c_30s_unscaled_uri = workspace + 'c_30s_unscaled_' + gu.pretty_time() + '.tif'\n#     gu.aggregate_geotiff(c_1km_uri, c_30s_unscaled_uri, desired_geotrans)\n#     #print gu.desc(c_30s_unscaled_uri)\n# else:\n#     c_30s_unscaled_uri = workspace + 'c_30s_unscaled.tif'\n\n# do_rescale = False\n# if do_rescale:\n#     c_30s_pre_uri = workspace + 'c_30s_pre_' + gu.pretty_time() + '.tif'\n#     gcu.multiply_two_large_geotiffs(c_30s_unscaled_uri, ha_per_cell_uri, c_30s_pre_uri)\n#     c_30s_uri = workspace + 'c_30s_' + gu.pretty_time() + '.tif'\n#     gcu.multiply_large_geotiff_by_float(c_30s_pre_uri, c_30s_uri, 0.01)\n#     gcu_sum = gcu.sum_geotiff(c_30s_uri)\n# else:\n#     c_30s_uri = workspace + 'c_30s.tif'\n\n# compare_resampled_to_input = False\n# if compare_resampled_to_input:\n#     c_5m_uri = 'E:/bulk_data/reusch_and_gibbs_carbon/data/datasets/c_5m/w001001.adf'\n#     c_5m = gu.as_array(c_5m_uri)\n#     c_5m[c_5m&lt;0]=0\n#     c_5m_from_30s_uri = 'E:/bulk_data/reusch_and_gibbs_carbon/data/datasets/c_1km/c_5m_from_30s.tif'\n#     desired_geotrans = (-180.0, 0.08333333333333, 0.0, 90.0, 0.0, -0.08333333333333)\n#     c_5m_from_30s = gu.as_array(c_5m_from_30s_uri)\n#     difference = np.where(c_5m_from_30s &gt; 0, (c_5m * gu.as_array('E:/bulk_data/reusch_and_gibbs_carbon/data/datasets/c_1km/ha_per_cell_5m.tif')) / c_5m_from_30s, 0)\n#     gu.show_array(difference,output_uri=c_5m_from_30s_uri.replace('.tif','_difference.jpg'), vmin = 0, vmax = 222)\n#     print np.sum(c_5m)\n#     print np.sum(c_5m * gu.as_array('E:/bulk_data/reusch_and_gibbs_carbon/data/datasets/c_1km/ha_per_cell_5m.tif'))\n\n# literal_aggregation_to_5m_cell = False\n# if literal_aggregation_to_5m_cell:\n#     factor =  10\n#     shape = (2160, 4320)\n#     c_30s_tr = gu.Tr(c_30s_uri, chunkshape = shape)\n#     cell_sum_5m = np.zeros((c_30s_tr.num_rows / factor, c_30s_tr.num_cols / factor))\n#     cell_sum_5m_uri = os.path.split(ha_per_cell_5m_uri)[0] + '/c_5m_literal_aggregation_' + gu.pretty_time() + '.tif'\n#     for tile in c_30s_tr.tr_frame:\n#         tile_array = c_30s_tr.tile_to_array(tile)\n#         print 'Aggregating tile', tile\n#         for row in range(c_30s_tr.chunkshape[0] / factor):\n#             for col in range(c_30s_tr.chunkshape[1] / factor):\n#                 cell_sum = np.sum(tile_array[row * factor : (row + 1) * factor, col * factor: (col + 1) * factor])\n#                 cell_sum_5m[tile[0] / factor + row, tile[1] / factor + col] = cell_sum\n#     cell_sum_5m /= 100 #because 30s is in c per ha and i want c per 5min gridcell\n#     print gu.desc(cell_sum_5m)\n#     gu.save_array_as_geotiff(cell_sum_5m, cell_sum_5m_uri, ha_per_cell_5m_uri)\n# else:\n#     cell_sum_5m_uri = os.path.split(ha_per_cell_5m_uri)[0] + '/c_5m_literal_aggregation.tif'\n\n\n# calculate_pnvc = True\n# if calculate_pnvc:\n#     factor = 10\n#     shape = (2160, 4320)\n#     c_30s_tr = gu.Tr(c_30s_uri, chunkshape = shape)\n\n#     sum_carbon_in_uncultivated_land_5m = np.zeros((c_30s_tr.num_rows / factor, c_30s_tr.num_cols / factor))\n#     potential_carbon_if_no_cultivation_5m = np.zeros((c_30s_tr.num_rows / factor, c_30s_tr.num_cols / factor))\n\n#     sum_carbon_in_uncultivated_land_5m_uri = os.path.split(ha_per_cell_5m_uri)[0] + '/sum_carbon_in_uncultivated_land_' + gu.pretty_time() + '.tif'\n#     potential_carbon_if_no_cultivation_5m_uri = os.path.split(ha_per_cell_5m_uri)[0] + '/potential_carbon_if_no_cultivation_' + gu.pretty_time() + '.tif'\n#     carbon_loss_per_new_ha_cultivated_5m_uri = os.path.split(ha_per_cell_5m_uri)[0] + '/carbon_loss_per_new_ha_cultivated_' + gu.pretty_time() + '.tif'\n\n#     proportion_cultivated = gu.as_array(workspace + 'proportion_cultivated.tif')\n#     ha_cultivated = ha_per_cell_5m * proportion_cultivated\n#     for tile in c_30s_tr.tr_frame:\n#         chunk = c_30s_tr.tile_to_array(tile)\n#         print 'Calculating on tile', tile\n#         for row in range(c_30s_tr.chunkshape[0] / factor):\n#             for col in range(c_30s_tr.chunkshape[1] / factor):\n\n#                 # Determine number of cells in a 5m tile of 30s data that are uncultivated by assuming that every cell\n#                 # not in cultivation is uncultivated.\n#                 cells_uncultivated = 100 - math.ceil(proportion_cultivated[tile[0] / factor + row, tile[1] / factor + col] * 100)\n\n#                 # Sort the current chunk\n#                 sorted_array = np.sort(chunk[row * factor: (row + 1) * factor, col * factor: (col + 1) * factor], axis = None)\n\n#                 # once the array is sorted, select the N highest ranked cells\n#                 # where N = the number of uncultivated cells present, based on proportion cultivated.\n#                 sum_carbon_in_uncultivated_land = np.sum(sorted_array[100 - cells_uncultivated:])\n\n#                 # The previous step gave the carbon present in uncultivated cells. Now, take teh average of that and fill\n#                 # in to all the other cells.\n#                 if cells_uncultivated &gt; 0:\n#                     potential_carbon_if_no_cultivation = sum_carbon_in_uncultivated_land + (sum_carbon_in_uncultivated_land / cells_uncultivated) * (100 - cells_uncultivated)\n#                 else:\n#                     # Because some cells have 100% cultivation, and given that this is usually either a data eror or a\n#                     # situation of agroforestry or other high-carbon systems (at least, higher than monoculture cereals)\n#                     # i set the carbon here to the carbon present in the top 10% highest carbon in the cell. This\n#                     # won't be a problem becuase the DeltaC calculation includes the carbon content of the crop in a different\n#                     # component.\n#                     potential_carbon_if_no_cultivation = np.mean(sorted_array[90:99]) * 100\n\n#                 # Fill into the appropriate spot of the 5m data\n#                 sum_carbon_in_uncultivated_land_5m[tile[0] / factor + row, tile[1] / factor + col] = sum_carbon_in_uncultivated_land\n#                 potential_carbon_if_no_cultivation_5m[tile[0] / factor + row, tile[1] / factor + col] = potential_carbon_if_no_cultivation\n\n#     carbon_loss_per_new_ha_cultivated_5m = potential_carbon_if_no_cultivation_5m / ha_per_cell_5m\n\n#     gu.save_array_as_geotiff(sum_carbon_in_uncultivated_land_5m, sum_carbon_in_uncultivated_land_5m_uri, ha_per_cell_5m_uri, verbose=True)\n#     gu.save_array_as_geotiff(potential_carbon_if_no_cultivation_5m, potential_carbon_if_no_cultivation_5m_uri, ha_per_cell_5m_uri, verbose=True)\n#     gu.save_array_as_geotiff(carbon_loss_per_new_ha_cultivated_5m, carbon_loss_per_new_ha_cultivated_5m_uri, ha_per_cell_5m_uri, verbose=True)\n# else:\n#     sum_carbon_in_uncultivated_land_5m_uri = os.path.split(ha_per_cell_5m_uri)[0] + '/sum_carbon_in_uncultivated_land_.tif'\n#     potential_carbon_if_no_cultivation_5m_uri = os.path.split(ha_per_cell_5m_uri)[0] + '/potential_carbon_if_no_cultivation.tif'\n#     carbon_loss_per_new_ha_cultivated_5m_uri = os.path.split(ha_per_cell_5m_uri)[0] + '/carbon_loss_per_new_ha_cultivated.tif'\n\nActually important part of this step\nThere are two important things to note:\n\nI start to deal with not recomputing things that have already been computed to save time, as in\n\n\ndo_30s_resample = False\nif do_30s_resample:\n    'do some stuff'\n\n\nI start to generalize my code on the frequently used functions. For instance:\n\n\n# import geoecon_utils.geoecon_utils as gu\n# gu.save_array_as_geotiff()"
  },
  {
    "objectID": "hazelbean.html#project-complexity-level-4-starting-to-deal-with-parallelization-and-file-management.",
    "href": "hazelbean.html#project-complexity-level-4-starting-to-deal-with-parallelization-and-file-management.",
    "title": "Hazelbean Introduction",
    "section": "Project complexity level 4: Starting to deal with parallelization and file management.",
    "text": "Project complexity level 4: Starting to deal with parallelization and file management.\nNYI, so lets’ skip to the solution, but I might actually write out all these steps!"
  },
  {
    "objectID": "hazelbean.html#project-complexity-level-5-simple-projectflow-implementation",
    "href": "hazelbean.html#project-complexity-level-5-simple-projectflow-implementation",
    "title": "Hazelbean Introduction",
    "section": "Project complexity level 5: Simple ProjectFlow Implementation",
    "text": "Project complexity level 5: Simple ProjectFlow Implementation\nSo you would like to contribute to the TEEMs repository. Suppose you say to me or Steve, “I think that the ecosystem services value of urban greenspace is very high” and “I want to calculate it at a global scale.” I might respond to you with “Great! Let’s add it to GTAP-InVEST as a new ProjectFlow task.” Let’s talk about this example. Because we actually will be running this, please open the file projectflow_quickstart.ipynb"
  },
  {
    "objectID": "how_to_contribute.html",
    "href": "how_to_contribute.html",
    "title": "How to contribute",
    "section": "",
    "text": "The Earth Economy Devstack and all of its published article code is available as open source software at GitHub, which not only allows users to download and run the code but also to develop it collaboratively. GitHub is based on the free and open source distributed version control system git (https://git-scm.com/). git allows users to track changes in code development, to work on different code branches and to revert changes to previous code versions, if necessary. Workflows using the version control system git can vary and there generally is no ‘right’ or ‘wrong’ in collaborative software development.\nThere are two different ways to interact with a github repository: 1. making a fork; 2; working on a branch. This section describes both options, however internal members of TEEMs will usually use branches.\n\n\nHowever, some basic preferred workflows are outlined in the following.\n\n\n\nBefore developing the code it is recommended to create a fork of the original SEALS repository. This will create a new code repository at your own GitHub profile, which includes the same code base and visibility settings as the original ‘upstream’ repository. The fork can now be used for - code development or fixes - submitting a pull request to the original SEALS repository.\n\n\n\nAlthough GitHub allows for some basic code changes, it is recommended to clone the forked repository at your local machine and do code developments using an integrated development environment (IDE), such as VS Code. To clone the repository on your local machine via the command line, navigate to the local folder, in which you want to clone the repository, and type\ngit clone -b &lt;name-of-branch&gt;` &lt;url-to-github-repo&gt; &lt;name-of-local-clone-folder&gt;\nFor example, if you want to clone the develop branch of your SEALS fork type\ngit clone -b develop https://github.com/&lt;username/seals_dev seals_develop\nBefore making any changes, make sure that your fork and/or your local repository are up-to-date with the original SEALS repository. To pull changes from your fork use git pull origin &lt;name-of-branch&gt;. In order to pull the latest changes from the original SEALS repository you can set up a link to the original upstream repository with the command git remote add upstream https://github.com/jandrewjohnson/seals_dev. To pull the latest changes from the develop branch in the original upstream repository use git pull upstream develop.\nDuring code development, any changes or fixes that should go back to the fork and/or the original SEALS repository need to be ‘staged’ and then ‘commited’ with a commit message that succinctly describes the changes. By staging changes, git is informed that these changes should become part of the next ‘commit’, which essentially takes a snapshot of all staged changes thus far.\nTo stage all changes in the current repository use the command git add .. If you only want to stage changes in a certain file use git add &lt;(relative)-path-to-file-in-repo&gt;.\nTo commit all staged changes use the command git commit -m \"a short description of the code changes\".\nAfter committing the changes, they can be pushed to your fork by using git push origin &lt;name-of-branch&gt;.\n\n\n\n\n\nIn order to propose changes to the original SEALS repository, it is recommended to use pull requests. Pull requests inform other users about your code changes and allow them to review these changes by comparing and highlighting the parts of the code that have have been altered. They also highlight potential merge conflicts, which occur when git is trying to merge branches with competing commits within the same parts of the code. Pull requests can be done directly at GitHub by navigating to either the forked repository or the original SEALS repository and by clicking on ‘Pull requests’ and then ‘New pull request’. This will open a new pull request page where changes can be described in more detail and code reviewers can be specfied. Pull requests should be reviewed by at least one of the main code developers.\n\n\n\nWheter you are working on a Fork or on the seals repository itself, we will organize contributions with branches. On the command line, you can create new code branch by using git branch &lt;name-of-new-branch&gt;. To let git know that you want to switch to and work on the new branch use git checkout &lt;name-of-new-branch&gt;. There are many other graphical user interfaces that help with git commands (so you don’t have to memorize all of the commands), including Github Desktop, Sourcetree and others. We also will use a VS Code plug called GitGraph to manage our repository. To install GitGraph, go the the Extensions left tab in VS Code and search for it. Once installed, you will find a button on the status bar to launch it. Using the gtap_invest_dev repository as an example, it should look like this:\n\n\nSuppose you want to make a change to the hazelbean_dev repository. We will use this example to explain how we use Git Graph to make this contribution. First, let’s take a look at the Git Graph interface.\n\nFirst note the Repo dropdown box indicating we are looking at the hazelbean_dev repo. Here you could switch to other Repos that have been added to your workspace. The next thing to note are the three different branch tages (Blue, Magenta and Green boxes on the Git Graph tree view). These are the three branches that are currently in the hazelbean_dev repository. The blue branch is the main branch, the magenta branch is the develop branch and the green branch is a branch specific to the user and/or the feature. In this case, it is develop_justin branch. The lines and dots indicates the history of how files were edited and then how the different branches were merged back together after an edit. By default, when you clone a repository, you will be on the main branch. To switch to another branch, right-click the tag and select checkout branch. When you do this, the files on your harddrive will be changed by Git to match the status of the branch you just checked out. In the image below, we can see which is the current branch because it has an open-circle dot in the graph and the name is bold. Other things to notice is that after the name of the branch there is the word origin. This indicates that the branch is synced with the remote repository (named origin by convention).\nTo make a change, you will first want to create your own branch. First, make absolutely certain you currently have checked out the develop branch (unless you know why to do otherwise). We will use main branch only to contain working “releases”. Once develop is checked out, use the Command Pallate (ctrl-shift-p) and search for create branch.\n\nChoose the right repository among those loaded in your workspace, hazelbean_dev in this example.\n\nThen select it and give the new branch a name, either develop_&lt;your_name&gt; or feature_&lt;your_feature_name&gt;. Below we have created a new feature branch called feature_test branched off of the develop branch. It should look like this\n\nYou’ll notice that the tag is bold, indicating you have checked it out and it is your current branch. Also notice though that it does not have the tag origin after it, indicating that it is not synced with the remote repository. To sync it, you will need to push it to the remote repository. To do this, right-click the tag and select push branch. This will push the branch to the remote repository and it will be available to other users.\nAnother way to push the branch is using the Version Control tab in VS Code. Click the Version Control tab on the Lefthand bar. There you will see all of the repos you have loaded into our workspace. For hazelbean_dev, you will see it has a publish branch button. Clicking this will have the same effect as the push branch command in Git Graph.\n\nRegardless of how you pushed it to the remote repository, you will now see the branch has the ‘origin’ tag after it, indicating it is synced with the remote repository. It is now also the checked-out branch and so all changes you make will be made to this branch.\n\nNow, we’re going to make a small change to the code. In the VS Code file explorer, I will open up file_io.py in the hazelbean module and scroll to the function I want to edit.\n\nOn lines 778-779, you’ll see VS Code has greyed the variables out indicating they are not used, so I will remove them. Once gone, you’ll see a blue bar on the left side of the editor. This is Git and VS Code indicating to you you made a change. Blue indicates a modification where as green indicates a new addition and red indicates a deletion.\n\nTo illustrate a new addition, I will add a line at to space out our for loop. Once I do this, you’ll see the green bar on the left side of the editor. In the image you can also see that the file_io.py is now a different color and has an M next to it. This indicates that the file has been modified.\n\nAnother useful thing you can do is click on the Blue or Green edit bar to see more details, as below.\n\nHere, you can see the exact changes you made. Additionally, there are buttons at the top of this new box that let you revert the change to go back to how it was before.\nBefore we commit our changes, look at the Git Graph view. You’ll see that we now have a new grey line coming from the feature_test branch, showing that we have uncommitted changes. You could click on the uncommitted changes link to see the edits we made.\n\nWe are now going to commit our changes. To do this, go to the Version Control Tab. You will now see that there is a change listed under the hazelbean_dev repository. Click on the change to see the details. You will see a more detailed “diff editor” that lets you understand (or change) what was edited. To accept these changes, we will click the commit button. But first write a short commit message. Click Commit (but don’t yet click the next button to Sync changes). After committing, look back at the Git Graph view. You’ll see that the grey line is now gone and the blue feature_test tag is at the top of our commit tree along with our comitt message.\n\nNotice though that the tag for origin/feature_test is not yet up at the new location. This is because we have not yet pushed our changes to the remote repository. To do this, click the Sync button in the bottom right of the VS Code window. This will push your changes to the remote repository and update the tag to the new location, like this.\n\nYour code is now on GitHub and other contributors with access could check it out and try it. But, this code will be different than their code and if you made non-trivial changes, it could be hard to keep straight what is going on. To clarify this, we are going to merge our feature_test branch back into develop.\nTo do this, first you must make sure you have the most recent version of the develop branch. To do this, first we will use the command palette and search for git pull. This will pull the most recent changes from the remote repository and update your local develop branch. Next, we want to make sure that any changes in develop are merged with what we just did to our feature_test branch. If there were no changes, this is not strictly necessary, but it’s a good precation to take to avoid future merge conflicts. To do this, right click on the develop tag and select merge into current branch. A popup will ask you to confirm this, which you do want to (with the default options).\n\nNow that we know for sure we have the most up-to-date develop branch details, we can merge our new feature into the develop branch. However, we will protect the develop branch so that only features that pass unit tests can be merged in. Thus, you will make a pull request, as described above, to get me to merge your work in the develop branch. For completeness, here we discuss how one would to that. To do this, first right-click on the develop tag and select Checkout branch. Our git tree will now show we have develop checked out:\n\nWith develop checked out, now right click on feature_test and select merge into current branch. Select confirm in the popup box. Click Sync Changes in the Version Control Tab.\n\nthe feature_test and develop branches are now identical. You could now delete feature_test and nothing would be lost. Now, the develop branch is ready for a user to use and/or make new branches off of. # End"
  },
  {
    "objectID": "how_to_contribute.html#version-management-with-git-github-repository",
    "href": "how_to_contribute.html#version-management-with-git-github-repository",
    "title": "How to contribute",
    "section": "",
    "text": "The Earth Economy Devstack and all of its published article code is available as open source software at GitHub, which not only allows users to download and run the code but also to develop it collaboratively. GitHub is based on the free and open source distributed version control system git (https://git-scm.com/). git allows users to track changes in code development, to work on different code branches and to revert changes to previous code versions, if necessary. Workflows using the version control system git can vary and there generally is no ‘right’ or ‘wrong’ in collaborative software development.\nThere are two different ways to interact with a github repository: 1. making a fork; 2; working on a branch. This section describes both options, however internal members of TEEMs will usually use branches.\n\n\nHowever, some basic preferred workflows are outlined in the following.\n\n\n\nBefore developing the code it is recommended to create a fork of the original SEALS repository. This will create a new code repository at your own GitHub profile, which includes the same code base and visibility settings as the original ‘upstream’ repository. The fork can now be used for - code development or fixes - submitting a pull request to the original SEALS repository.\n\n\n\nAlthough GitHub allows for some basic code changes, it is recommended to clone the forked repository at your local machine and do code developments using an integrated development environment (IDE), such as VS Code. To clone the repository on your local machine via the command line, navigate to the local folder, in which you want to clone the repository, and type\ngit clone -b &lt;name-of-branch&gt;` &lt;url-to-github-repo&gt; &lt;name-of-local-clone-folder&gt;\nFor example, if you want to clone the develop branch of your SEALS fork type\ngit clone -b develop https://github.com/&lt;username/seals_dev seals_develop\nBefore making any changes, make sure that your fork and/or your local repository are up-to-date with the original SEALS repository. To pull changes from your fork use git pull origin &lt;name-of-branch&gt;. In order to pull the latest changes from the original SEALS repository you can set up a link to the original upstream repository with the command git remote add upstream https://github.com/jandrewjohnson/seals_dev. To pull the latest changes from the develop branch in the original upstream repository use git pull upstream develop.\nDuring code development, any changes or fixes that should go back to the fork and/or the original SEALS repository need to be ‘staged’ and then ‘commited’ with a commit message that succinctly describes the changes. By staging changes, git is informed that these changes should become part of the next ‘commit’, which essentially takes a snapshot of all staged changes thus far.\nTo stage all changes in the current repository use the command git add .. If you only want to stage changes in a certain file use git add &lt;(relative)-path-to-file-in-repo&gt;.\nTo commit all staged changes use the command git commit -m \"a short description of the code changes\".\nAfter committing the changes, they can be pushed to your fork by using git push origin &lt;name-of-branch&gt;.\n\n\n\n\n\nIn order to propose changes to the original SEALS repository, it is recommended to use pull requests. Pull requests inform other users about your code changes and allow them to review these changes by comparing and highlighting the parts of the code that have have been altered. They also highlight potential merge conflicts, which occur when git is trying to merge branches with competing commits within the same parts of the code. Pull requests can be done directly at GitHub by navigating to either the forked repository or the original SEALS repository and by clicking on ‘Pull requests’ and then ‘New pull request’. This will open a new pull request page where changes can be described in more detail and code reviewers can be specfied. Pull requests should be reviewed by at least one of the main code developers.\n\n\n\nWheter you are working on a Fork or on the seals repository itself, we will organize contributions with branches. On the command line, you can create new code branch by using git branch &lt;name-of-new-branch&gt;. To let git know that you want to switch to and work on the new branch use git checkout &lt;name-of-new-branch&gt;. There are many other graphical user interfaces that help with git commands (so you don’t have to memorize all of the commands), including Github Desktop, Sourcetree and others. We also will use a VS Code plug called GitGraph to manage our repository. To install GitGraph, go the the Extensions left tab in VS Code and search for it. Once installed, you will find a button on the status bar to launch it. Using the gtap_invest_dev repository as an example, it should look like this:\n\n\nSuppose you want to make a change to the hazelbean_dev repository. We will use this example to explain how we use Git Graph to make this contribution. First, let’s take a look at the Git Graph interface.\n\nFirst note the Repo dropdown box indicating we are looking at the hazelbean_dev repo. Here you could switch to other Repos that have been added to your workspace. The next thing to note are the three different branch tages (Blue, Magenta and Green boxes on the Git Graph tree view). These are the three branches that are currently in the hazelbean_dev repository. The blue branch is the main branch, the magenta branch is the develop branch and the green branch is a branch specific to the user and/or the feature. In this case, it is develop_justin branch. The lines and dots indicates the history of how files were edited and then how the different branches were merged back together after an edit. By default, when you clone a repository, you will be on the main branch. To switch to another branch, right-click the tag and select checkout branch. When you do this, the files on your harddrive will be changed by Git to match the status of the branch you just checked out. In the image below, we can see which is the current branch because it has an open-circle dot in the graph and the name is bold. Other things to notice is that after the name of the branch there is the word origin. This indicates that the branch is synced with the remote repository (named origin by convention).\nTo make a change, you will first want to create your own branch. First, make absolutely certain you currently have checked out the develop branch (unless you know why to do otherwise). We will use main branch only to contain working “releases”. Once develop is checked out, use the Command Pallate (ctrl-shift-p) and search for create branch.\n\nChoose the right repository among those loaded in your workspace, hazelbean_dev in this example.\n\nThen select it and give the new branch a name, either develop_&lt;your_name&gt; or feature_&lt;your_feature_name&gt;. Below we have created a new feature branch called feature_test branched off of the develop branch. It should look like this\n\nYou’ll notice that the tag is bold, indicating you have checked it out and it is your current branch. Also notice though that it does not have the tag origin after it, indicating that it is not synced with the remote repository. To sync it, you will need to push it to the remote repository. To do this, right-click the tag and select push branch. This will push the branch to the remote repository and it will be available to other users.\nAnother way to push the branch is using the Version Control tab in VS Code. Click the Version Control tab on the Lefthand bar. There you will see all of the repos you have loaded into our workspace. For hazelbean_dev, you will see it has a publish branch button. Clicking this will have the same effect as the push branch command in Git Graph.\n\nRegardless of how you pushed it to the remote repository, you will now see the branch has the ‘origin’ tag after it, indicating it is synced with the remote repository. It is now also the checked-out branch and so all changes you make will be made to this branch.\n\nNow, we’re going to make a small change to the code. In the VS Code file explorer, I will open up file_io.py in the hazelbean module and scroll to the function I want to edit.\n\nOn lines 778-779, you’ll see VS Code has greyed the variables out indicating they are not used, so I will remove them. Once gone, you’ll see a blue bar on the left side of the editor. This is Git and VS Code indicating to you you made a change. Blue indicates a modification where as green indicates a new addition and red indicates a deletion.\n\nTo illustrate a new addition, I will add a line at to space out our for loop. Once I do this, you’ll see the green bar on the left side of the editor. In the image you can also see that the file_io.py is now a different color and has an M next to it. This indicates that the file has been modified.\n\nAnother useful thing you can do is click on the Blue or Green edit bar to see more details, as below.\n\nHere, you can see the exact changes you made. Additionally, there are buttons at the top of this new box that let you revert the change to go back to how it was before.\nBefore we commit our changes, look at the Git Graph view. You’ll see that we now have a new grey line coming from the feature_test branch, showing that we have uncommitted changes. You could click on the uncommitted changes link to see the edits we made.\n\nWe are now going to commit our changes. To do this, go to the Version Control Tab. You will now see that there is a change listed under the hazelbean_dev repository. Click on the change to see the details. You will see a more detailed “diff editor” that lets you understand (or change) what was edited. To accept these changes, we will click the commit button. But first write a short commit message. Click Commit (but don’t yet click the next button to Sync changes). After committing, look back at the Git Graph view. You’ll see that the grey line is now gone and the blue feature_test tag is at the top of our commit tree along with our comitt message.\n\nNotice though that the tag for origin/feature_test is not yet up at the new location. This is because we have not yet pushed our changes to the remote repository. To do this, click the Sync button in the bottom right of the VS Code window. This will push your changes to the remote repository and update the tag to the new location, like this.\n\nYour code is now on GitHub and other contributors with access could check it out and try it. But, this code will be different than their code and if you made non-trivial changes, it could be hard to keep straight what is going on. To clarify this, we are going to merge our feature_test branch back into develop.\nTo do this, first you must make sure you have the most recent version of the develop branch. To do this, first we will use the command palette and search for git pull. This will pull the most recent changes from the remote repository and update your local develop branch. Next, we want to make sure that any changes in develop are merged with what we just did to our feature_test branch. If there were no changes, this is not strictly necessary, but it’s a good precation to take to avoid future merge conflicts. To do this, right click on the develop tag and select merge into current branch. A popup will ask you to confirm this, which you do want to (with the default options).\n\nNow that we know for sure we have the most up-to-date develop branch details, we can merge our new feature into the develop branch. However, we will protect the develop branch so that only features that pass unit tests can be merged in. Thus, you will make a pull request, as described above, to get me to merge your work in the develop branch. For completeness, here we discuss how one would to that. To do this, first right-click on the develop tag and select Checkout branch. Our git tree will now show we have develop checked out:\n\nWith develop checked out, now right click on feature_test and select merge into current branch. Select confirm in the popup box. Click Sync Changes in the Version Control Tab.\n\nthe feature_test and develop branches are now identical. You could now delete feature_test and nothing would be lost. Now, the develop branch is ready for a user to use and/or make new branches off of. # End"
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installation",
    "section": "",
    "text": "The Earth Economy Devstack is a specific way of organizing files, VS Code workspaces, local development repositories and preconfigured launch configurations that let a user contribute to NatCap TEEMs projects. This document contains:\n\nA quickstark installation guide for installing the Earth Economy Devstack which assumes a basic understanding of the command line and Python.\nA detailed step-by-step installation guide of Python, Git, QGIS and other related software.\n\nBoth sections will discuss putting files in specific locations relative to your user directory. If followed exactly, this will allow the multiple repositories to work together as intended. Other file organization schemes are possible, but will require manual configuration of the launch configurations and other settings.\n\n\n\n\n\nInstall and run the installer for your operating system at &lt;Git-scm.com/downloads&gt;\n\n\n\n\n\nInstall Miniforge from https://github.com/conda-forge/miniforge?tab=readme-ov-file#miniforge3 - Be sure to select the correct version for your operating system (Windows, Mac, Linux)\n\nIf you have an Apple “m1 or m2” chip (a relatively new apple chip, make sure you select the Apple Silicon option). - Install in C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge3 (PC) or ~/miniconda3 (Mac) - During installation, select yes for “Add Mambaforge/Miniforge to my PATH environment Variable” - If you get a “Windows Protected your PC”, click more info then Run Anyway.\n\nOpen the Miniforge Prompt (PC only, search for it in the start menu) or just type “mamba init” in a new terminal/command line\nCreate a new mamba environment with the following commands (putting your desired environment name in place of &lt;env_name&gt;):\n\nmamba create -n &lt;env_name&gt; -c conda-forge\n\nActivate the environment\n\nmamba activate &lt;env_name&gt;\n\nInstall libraries using the following mamba command:\n\nmamba install -c conda-forge natcap.invest geopandas rasterstats netCDF4 cartopy xlrd markdown qtpy qtawesome plotly descartes pygeoprocessing taskgraph cython rioxarray dask google-cloud-datastore google-cloud-storage aenum anytree statsmodels openpyxl seaborn twine pyqt ipykernel imageio pandoc conda numba intake more-itertools google-api-python-client google-auth google-auth-oauthlib google-auth-httplib2 gdown tqdm\n\nIf you don’t add mamba to your path, you can do this manually. On PC, you could use the command\n\nSETX PATH \"%PATH%;C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge;C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge;\"\n\n\n\n\n\n\nCreate a directory for the Earth Economy Devstack at C:\\Users\\&lt;YOUR_USERNAME&gt;\\Files (PC) or ~/Files (Mac)\n\nWe add a Files directory to keep the root directory clean and to make it easier to find the Earth Economy Devstack in the file explorer.\n\nOpen a terminal or command prompt and navigate to the directory you created\nRun the git clone command to clone the Earth Economy Devstack repository\n\ngit clone https://github.com/jandrewjohnson/earth_economy_devstack\n\n\n\n\n\n\nInstall from &lt;code.visualstudio.com/download&gt;\n\nFor PC, I recommend selecting the “User Installer”, 64-bit option for windows.\nUse the default install options with the exception of the two that start with “Add Open with Code” action…\n\n\n\n\n\n\nNavigate to the directory where you cloned the Earth Economy Devstack\n\nIn the repository, there is a file called earth_economy_devstack.code-workspace. Launch this.\n\nIf configured correctly, the full path would be C:\\Users\\&lt;YOUR_USERNAME&gt;\\Files\\earth_economy_devstack\\earth_economy_devstack.code-workspace (PC) or ~/Files/earth_economy_devstack/earth_economy_devstack.code-workspace (Mac)\n\n\nThe workspace file adds a bunch of different repositories to the workspace (which you will need to git clone in to where the workspace says they should be)\nThe workspace also sets up a bunch of launch configurations for debugging and running code using the respositories in your workspace\n\n\n\n\n\nInstall required extensions\n\nInstall the Python extension in VS Code\n\nClick on the extensions icon in the left sidebar\nSearch for Python and install the one by Microsoft\n\nInstall Quarto extension in VS Code\nInstall GitGraph extension in VS Code\n\nSign in to VS Code with your GitHub account\n\nClick on the account icon in the bottom left corner of the window and follow prompts\n\n\n\n\n\n\n\n\n\nGet the git software\n\nInstall and run the installer for your operating system at &lt;Git-scm.com/downloads&gt;\n\n\nGit vs. GitHub\n\nGithub is a website that hosts code and connects a community of coders.\nGit is a “version control” software tool that records the history of files in a Git repository.\nNearly every coder uses Git to push their repository of code to GitHub.\n\nUse the default options for everything\n\nUnless you REALLY know what you’re doing.\n\n\n\n\n\nOpen up the command prompt and type git to test that it’s installed\n\n(PC) Search for cmd in the start menu\n\nThis is the OG way of working on computers\n\nAll version control tasks can be done via git commands here, but we will be using VS Code instead\n\n\n\n\n\n\nInstall Miniforge from https://github.com/conda-forge/miniforge?tab=readme-ov-file#miniforge3 - Be sure to select the correct version for your operating system (Windows, Mac, Linux)\n\n\nIf you have an Apple “m1 or m2” chip (a relatively new apple chip, make sure you select the Apple Silicon option).\nInstall in C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge3 (PC) or ~/miniconda3 (Mac)\n\n\n\nDuring installation, select yes for “Add Mambaforge/Miniforge to my PATH environment Variable”\n\n\n\nIf you get a “Windows Protected your PC”, click more info then Run Anyway.\n\nOpen the Miniforge Prompt (PC only, search for it in the start menu) or just type “mamba init” in a new terminal/command line\n\n\n\nYou’ll know it worked if you see (base) in front of your path\n\nBase is the default “environment” that you will use.\n\n\n\nCreate a new mamba environment with the following commands (putting your desired environment name in place of &lt;env_name&gt;):\n\nmamba create -n &lt;env_name&gt; -c conda-forge\n- When you have lots of projects, most people create multiple environments specific to each project. \n- For now, we’re going to install everything to the base environment\n\nActivate the environment\n\nmamba activate &lt;env_name&gt;\n- You’ll know it worked if you see (env_name) in front of your path\n- You can deactivate the environment with `conda deactivate`\n- ![](images/2024-02-02-10-39-21.png)\n- You can list all environments with `conda env list`\n\nInstall libraries using the following mamba command:\n\nmamba install -c conda-forge natcap.invest geopandas rasterstats netCDF4 cartopy xlrd markdown qtpy qtawesome plotly descartes pygeoprocessing taskgraph cython rioxarray dask google-cloud-datastore google-cloud-storage aenum anytree statsmodels openpyxl seaborn twine pyqt ipykernel imageio pandoc conda numba intake more-itertools google-api-python-client google-auth google-auth-oauthlib google-auth-httplib2 gdown tqdm\n\nIf you don’t add mamba to your path, you can do this manually. On PC, you could use the command\n\nSETX PATH \"%PATH%;C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge;C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge;\"\n\nThis step may take a long time because you are downloading and installing hundreds of libraries\n\n\n\nWhen you’re done, it should look like the image here.\n\n\n\nSuccess! You now have a modern scientific computing environment (sometimes called a scientific computing stack) on your computer!\n\n\n\n\n\nCreate a directory for the Earth Economy Devstack at C:\\Users\\&lt;YOUR_USERNAME&gt;\\Files (PC) or ~/Files (Mac)\n\nWe add a Files directory to keep the root directory clean and to make it easier to find the Earth Economy Devstack in the file explorer.\n\nOpen a terminal or command prompt and navigate to the directory you created\n\nBy default, your terminal will open in your user directory, so C:\\Users\\&lt;YOUR_USERNAME&gt;\nYou can navigate to the directory you created with the command cd Files\n\nSee image below.\n\n\nRun the git clone command to clone the Earth Economy Devstack repository\n\ngit clone https://github.com/jandrewjohnson/earth_economy_devstack\n\n\n\n\n\n\n\nInstall from &lt;code.visualstudio.com/download&gt;\n\nFor PC, I recommend selecting the “User Installer”, 64-bit option for windows.\n\n\n\nUse the default install options with the exception of the two that start with “Add Open with Code” action…\n\n\n\n\nYou could just open VS Code now, but we’re going to open it up with a specific Worspace Configuration file below\n\n\n\n\n\nNavigate to the directory where you cloned the Earth Economy Devstack\n\nIn the repository, there is a file called earth_economy_devstack.code-workspace. Launch this.\n\nIf configured correctly, the full path would be C:\\Users\\&lt;YOUR_USERNAME&gt;\\Files\\earth_economy_devstack\\earth_economy_devstack.code-workspace (PC) or ~/Files/earth_economy_devstack/earth_economy_devstack.code-workspace (Mac)\n\n\nThe workspace file adds a bunch of different repositories to the workspace (which you will need to git clone in to where the workspace says they should be)\n\n\n\nThe workspace also sets up a bunch of launch configurations for debugging and running code using the respositories in your workspace\n\n\nWe’ll describe debugging more in future sections\n\nFor now, just know that these launch configurations make sure you’re using the repositories that we’ve added to your workspace\n\n\nAlso note that the other repositories shown in the VS Code file explorer will be empty until you git clone them (described below)\n\n\n\n\n\nSign in to VS Code with your GitHub account\n\nClick on the account icon in the bottom left corner of the window and follow prompts\n\nInstall required extensions\n\nInstall the Python extension\n\nClick on the extensions icon in the left sidebar\nSearch for Python and install the one by Microsoft\n\n\nInstall Quarto extension in VS Code\n\nWe use Quarto to create reports and documents on .qmd, .md and .ipynb files\n\nYou can edit in source mode:\n\n\n\nOr you can press ctrl+shift+f4 to use the visual editor\n\n\n\n\n\nInstall Git Graph extension in VS Code\n\nOnce installed, click the Git Graph button on the bottom-left status bar to see a visual representation of your git history\n\n\n\n\n\n\n\n\nThe Earth Economy Devstack organizes repositories in a specific way, described here.\n\nRecall that our workspace links to e.g. the hazelbean_dev repository, but it currently points to a directory that doesn’t yet exist.\n\nIn our Files directory, create a directory named hazelbean (not hazelbean_dev as that will be the repository’s name)\n\n\n\nBelow, we will use git to clone the hazelbean_dev into this directory we just created\n\n\n\n\n\n\n\n\n\n\nInstead of using the command line, we will use Git via VS Code’s “Command Pallate”\n\nThe Command Pallate is accessed via \nIt is a search bar that you can use to run commands in VS Code\n\nOnce you’ve opened the Command Pallate, type “git clone” and it will search for the command\n\n\n\nOnce you select the command, it will prompt you if you want to write in your Repo’s GitHub URL manually, or you can use VS Code to search the different repositories you have access to\n\n\n\nSearching via GitHub found, for instance, NatCapTEEMs/gep repo\n\n\n\nOnce you select that, it will ask you what local directory you want to clone the repository to.\n\nBy default it assumes you want to clone it to your user directory as below\n\n\n\nWe instead want to clone it to the hazelbean directory we created earlier, which will put a new folder hazelbean_dev in that directory\nTo do this, navigate to the hazelbean directory and select it\n\n\n\n\nAfter you’ve cloned a repository, you can now access it in the file explorer\n\n\n\n\n\n\n\n\nIf you are a member of NatCap TEEMs or the JohnsonPolaskyLab, you should have access to these repositories via our GitHub organization\nIf you’re not a member, you will still be able to clone all of the public repositories (which are all documented in various journal articles)"
  },
  {
    "objectID": "installation.html#quickstart-installation",
    "href": "installation.html#quickstart-installation",
    "title": "Installation",
    "section": "",
    "text": "Install and run the installer for your operating system at &lt;Git-scm.com/downloads&gt;\n\n\n\n\n\nInstall Miniforge from https://github.com/conda-forge/miniforge?tab=readme-ov-file#miniforge3 - Be sure to select the correct version for your operating system (Windows, Mac, Linux)\n\nIf you have an Apple “m1 or m2” chip (a relatively new apple chip, make sure you select the Apple Silicon option). - Install in C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge3 (PC) or ~/miniconda3 (Mac) - During installation, select yes for “Add Mambaforge/Miniforge to my PATH environment Variable” - If you get a “Windows Protected your PC”, click more info then Run Anyway.\n\nOpen the Miniforge Prompt (PC only, search for it in the start menu) or just type “mamba init” in a new terminal/command line\nCreate a new mamba environment with the following commands (putting your desired environment name in place of &lt;env_name&gt;):\n\nmamba create -n &lt;env_name&gt; -c conda-forge\n\nActivate the environment\n\nmamba activate &lt;env_name&gt;\n\nInstall libraries using the following mamba command:\n\nmamba install -c conda-forge natcap.invest geopandas rasterstats netCDF4 cartopy xlrd markdown qtpy qtawesome plotly descartes pygeoprocessing taskgraph cython rioxarray dask google-cloud-datastore google-cloud-storage aenum anytree statsmodels openpyxl seaborn twine pyqt ipykernel imageio pandoc conda numba intake more-itertools google-api-python-client google-auth google-auth-oauthlib google-auth-httplib2 gdown tqdm\n\nIf you don’t add mamba to your path, you can do this manually. On PC, you could use the command\n\nSETX PATH \"%PATH%;C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge;C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge;\"\n\n\n\n\n\n\nCreate a directory for the Earth Economy Devstack at C:\\Users\\&lt;YOUR_USERNAME&gt;\\Files (PC) or ~/Files (Mac)\n\nWe add a Files directory to keep the root directory clean and to make it easier to find the Earth Economy Devstack in the file explorer.\n\nOpen a terminal or command prompt and navigate to the directory you created\nRun the git clone command to clone the Earth Economy Devstack repository\n\ngit clone https://github.com/jandrewjohnson/earth_economy_devstack\n\n\n\n\n\n\nInstall from &lt;code.visualstudio.com/download&gt;\n\nFor PC, I recommend selecting the “User Installer”, 64-bit option for windows.\nUse the default install options with the exception of the two that start with “Add Open with Code” action…\n\n\n\n\n\n\nNavigate to the directory where you cloned the Earth Economy Devstack\n\nIn the repository, there is a file called earth_economy_devstack.code-workspace. Launch this.\n\nIf configured correctly, the full path would be C:\\Users\\&lt;YOUR_USERNAME&gt;\\Files\\earth_economy_devstack\\earth_economy_devstack.code-workspace (PC) or ~/Files/earth_economy_devstack/earth_economy_devstack.code-workspace (Mac)\n\n\nThe workspace file adds a bunch of different repositories to the workspace (which you will need to git clone in to where the workspace says they should be)\nThe workspace also sets up a bunch of launch configurations for debugging and running code using the respositories in your workspace\n\n\n\n\n\nInstall required extensions\n\nInstall the Python extension in VS Code\n\nClick on the extensions icon in the left sidebar\nSearch for Python and install the one by Microsoft\n\nInstall Quarto extension in VS Code\nInstall GitGraph extension in VS Code\n\nSign in to VS Code with your GitHub account\n\nClick on the account icon in the bottom left corner of the window and follow prompts"
  },
  {
    "objectID": "installation.html#step-by-step-installation",
    "href": "installation.html#step-by-step-installation",
    "title": "Installation",
    "section": "",
    "text": "Get the git software\n\nInstall and run the installer for your operating system at &lt;Git-scm.com/downloads&gt;\n\n\nGit vs. GitHub\n\nGithub is a website that hosts code and connects a community of coders.\nGit is a “version control” software tool that records the history of files in a Git repository.\nNearly every coder uses Git to push their repository of code to GitHub.\n\nUse the default options for everything\n\nUnless you REALLY know what you’re doing.\n\n\n\n\n\nOpen up the command prompt and type git to test that it’s installed\n\n(PC) Search for cmd in the start menu\n\nThis is the OG way of working on computers\n\nAll version control tasks can be done via git commands here, but we will be using VS Code instead\n\n\n\n\n\n\nInstall Miniforge from https://github.com/conda-forge/miniforge?tab=readme-ov-file#miniforge3 - Be sure to select the correct version for your operating system (Windows, Mac, Linux)\n\n\nIf you have an Apple “m1 or m2” chip (a relatively new apple chip, make sure you select the Apple Silicon option).\nInstall in C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge3 (PC) or ~/miniconda3 (Mac)\n\n\n\nDuring installation, select yes for “Add Mambaforge/Miniforge to my PATH environment Variable”\n\n\n\nIf you get a “Windows Protected your PC”, click more info then Run Anyway.\n\nOpen the Miniforge Prompt (PC only, search for it in the start menu) or just type “mamba init” in a new terminal/command line\n\n\n\nYou’ll know it worked if you see (base) in front of your path\n\nBase is the default “environment” that you will use.\n\n\n\nCreate a new mamba environment with the following commands (putting your desired environment name in place of &lt;env_name&gt;):\n\nmamba create -n &lt;env_name&gt; -c conda-forge\n- When you have lots of projects, most people create multiple environments specific to each project. \n- For now, we’re going to install everything to the base environment\n\nActivate the environment\n\nmamba activate &lt;env_name&gt;\n- You’ll know it worked if you see (env_name) in front of your path\n- You can deactivate the environment with `conda deactivate`\n- ![](images/2024-02-02-10-39-21.png)\n- You can list all environments with `conda env list`\n\nInstall libraries using the following mamba command:\n\nmamba install -c conda-forge natcap.invest geopandas rasterstats netCDF4 cartopy xlrd markdown qtpy qtawesome plotly descartes pygeoprocessing taskgraph cython rioxarray dask google-cloud-datastore google-cloud-storage aenum anytree statsmodels openpyxl seaborn twine pyqt ipykernel imageio pandoc conda numba intake more-itertools google-api-python-client google-auth google-auth-oauthlib google-auth-httplib2 gdown tqdm\n\nIf you don’t add mamba to your path, you can do this manually. On PC, you could use the command\n\nSETX PATH \"%PATH%;C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge;C:\\Users\\&lt;YOUR_USERNAME&gt;\\miniforge;\"\n\nThis step may take a long time because you are downloading and installing hundreds of libraries\n\n\n\nWhen you’re done, it should look like the image here.\n\n\n\nSuccess! You now have a modern scientific computing environment (sometimes called a scientific computing stack) on your computer!\n\n\n\n\n\nCreate a directory for the Earth Economy Devstack at C:\\Users\\&lt;YOUR_USERNAME&gt;\\Files (PC) or ~/Files (Mac)\n\nWe add a Files directory to keep the root directory clean and to make it easier to find the Earth Economy Devstack in the file explorer.\n\nOpen a terminal or command prompt and navigate to the directory you created\n\nBy default, your terminal will open in your user directory, so C:\\Users\\&lt;YOUR_USERNAME&gt;\nYou can navigate to the directory you created with the command cd Files\n\nSee image below.\n\n\nRun the git clone command to clone the Earth Economy Devstack repository\n\ngit clone https://github.com/jandrewjohnson/earth_economy_devstack\n\n\n\n\n\n\n\nInstall from &lt;code.visualstudio.com/download&gt;\n\nFor PC, I recommend selecting the “User Installer”, 64-bit option for windows.\n\n\n\nUse the default install options with the exception of the two that start with “Add Open with Code” action…\n\n\n\n\nYou could just open VS Code now, but we’re going to open it up with a specific Worspace Configuration file below\n\n\n\n\n\nNavigate to the directory where you cloned the Earth Economy Devstack\n\nIn the repository, there is a file called earth_economy_devstack.code-workspace. Launch this.\n\nIf configured correctly, the full path would be C:\\Users\\&lt;YOUR_USERNAME&gt;\\Files\\earth_economy_devstack\\earth_economy_devstack.code-workspace (PC) or ~/Files/earth_economy_devstack/earth_economy_devstack.code-workspace (Mac)\n\n\nThe workspace file adds a bunch of different repositories to the workspace (which you will need to git clone in to where the workspace says they should be)\n\n\n\nThe workspace also sets up a bunch of launch configurations for debugging and running code using the respositories in your workspace\n\n\nWe’ll describe debugging more in future sections\n\nFor now, just know that these launch configurations make sure you’re using the repositories that we’ve added to your workspace\n\n\nAlso note that the other repositories shown in the VS Code file explorer will be empty until you git clone them (described below)\n\n\n\n\n\nSign in to VS Code with your GitHub account\n\nClick on the account icon in the bottom left corner of the window and follow prompts\n\nInstall required extensions\n\nInstall the Python extension\n\nClick on the extensions icon in the left sidebar\nSearch for Python and install the one by Microsoft\n\n\nInstall Quarto extension in VS Code\n\nWe use Quarto to create reports and documents on .qmd, .md and .ipynb files\n\nYou can edit in source mode:\n\n\n\nOr you can press ctrl+shift+f4 to use the visual editor\n\n\n\n\n\nInstall Git Graph extension in VS Code\n\nOnce installed, click the Git Graph button on the bottom-left status bar to see a visual representation of your git history\n\n\n\n\n\n\n\n\nThe Earth Economy Devstack organizes repositories in a specific way, described here.\n\nRecall that our workspace links to e.g. the hazelbean_dev repository, but it currently points to a directory that doesn’t yet exist.\n\nIn our Files directory, create a directory named hazelbean (not hazelbean_dev as that will be the repository’s name)\n\n\n\nBelow, we will use git to clone the hazelbean_dev into this directory we just created\n\n\n\n\n\n\n\n\n\n\nInstead of using the command line, we will use Git via VS Code’s “Command Pallate”\n\nThe Command Pallate is accessed via \nIt is a search bar that you can use to run commands in VS Code\n\nOnce you’ve opened the Command Pallate, type “git clone” and it will search for the command\n\n\n\nOnce you select the command, it will prompt you if you want to write in your Repo’s GitHub URL manually, or you can use VS Code to search the different repositories you have access to\n\n\n\nSearching via GitHub found, for instance, NatCapTEEMs/gep repo\n\n\n\nOnce you select that, it will ask you what local directory you want to clone the repository to.\n\nBy default it assumes you want to clone it to your user directory as below\n\n\n\nWe instead want to clone it to the hazelbean directory we created earlier, which will put a new folder hazelbean_dev in that directory\nTo do this, navigate to the hazelbean directory and select it\n\n\n\n\nAfter you’ve cloned a repository, you can now access it in the file explorer\n\n\n\n\n\n\n\n\nIf you are a member of NatCap TEEMs or the JohnsonPolaskyLab, you should have access to these repositories via our GitHub organization\nIf you’re not a member, you will still be able to clone all of the public repositories (which are all documented in various journal articles)"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Earth-Economy Devstack",
    "section": "",
    "text": "Earth-Economy Devstack\nThis is the documentation for the Earth-Economy Devstack, which is the set of repositories and code tools used by the Johnson-Polasky Lab and NatCap TEEMs. This documentation starts with overall organization of the Earth-Economy Devstack and discusses common coding practices used among the multiple releated repositories."
  },
  {
    "objectID": "organization.html",
    "href": "organization.html",
    "title": "Overall Organization",
    "section": "",
    "text": "Assuming you have already setup your python environment and installed VS Code, the first step is to clone all of the relevant repositories into the exact-right location. Throughout this documentation, we refer to “EE Spec”, or Earth-Economy Specification. This is simply the common conventions for things like naming files and organizing them so that it works for all participants. The EE Spec organization is to clone the earth_economy_devstack repository, available at https://github.com/jandrewjohnson/earth_economy_devstack, into your Users directory in a subdirectory called Files. The PC version is shown below.\n\nTo clone here, you can use the command line, navigate to the Files directory and use git clone https://github.com/jandrewjohnson/earth_economy_devstack . Alternatively you could use VS Code’s Command Pallate &lt;ctrl-shift-s&gt; Git Clone command and navigate to this repo. This repository configures VS Code to work well with the other EE repos and, for instance, defines launch-configurations that will always use the latest from github.\n\n\n\nNext, create a folder for each of the five repositories in the Files directory, as in the picture above.\n\nhazelbean\nseals\ngtappy\ngtap_invest\nglobal_invest\n\nInside each of these folders, you will clone the corresponding repositories:\n\nhttps://github.com/jandrewjohnson/hazelbean_dev\nhttps://github.com/jandrewjohnson/seals_dev\nhttps://github.com/jandrewjohnson/gtappy_dev\nhttps://github.com/jandrewjohnson/gtap_invest_dev\nhttps://github.com/jandrewjohnson/global_invest_dev\n\nIf successful, you will have a new folder with _dev postpended, indicating that it is the repository itself (and is the dev, i.e., not yet public, version of it). For GTAPPy it should look like this:\n\nAll code will be stored in the _dev repository. All files that you will generate when running the libraries, conversely, will be in a different Projects directory as above (This will be discussed more in the ProjectFlow section).\n\n\n\nNavigate to the earth_economy_devstack directory. In there, you will find a file earth_economy_devstack.code-workspace (pictured below).\n\nDouble click it to load a preconfigured VS Code Workspace. You will know you have it all working if the explorer tab in VS Code shows all all SIX of the repositories.\n\n\n\n\nThe earth_economy_devstack.code-workspace configures VS Code so that it includes all of the necessary repositories for EE code to run. Specifically, this means that if you pull the most recent version of each repository from Github, your code will all work together seamlessly. In addition to this file, you will see in the .vs_code directory there is a launch.json file. This file defines how to launch the python Debugger so that it uses the correct versions of the repositories. To launch a specific python file, first make it the active editor window, then open the debugger tab in VS Code’s left navbar, and in the Run and Debug dropdown box (pictured below) select the first run configuration, then hit the green Play triangle to launch the python file you had open. If all is setup correctly, your file you ran should be able to import all of the libraries in the devstack."
  },
  {
    "objectID": "organization.html#getting-the-earth_economy_devstack-repository",
    "href": "organization.html#getting-the-earth_economy_devstack-repository",
    "title": "Overall Organization",
    "section": "",
    "text": "Assuming you have already setup your python environment and installed VS Code, the first step is to clone all of the relevant repositories into the exact-right location. Throughout this documentation, we refer to “EE Spec”, or Earth-Economy Specification. This is simply the common conventions for things like naming files and organizing them so that it works for all participants. The EE Spec organization is to clone the earth_economy_devstack repository, available at https://github.com/jandrewjohnson/earth_economy_devstack, into your Users directory in a subdirectory called Files. The PC version is shown below.\n\nTo clone here, you can use the command line, navigate to the Files directory and use git clone https://github.com/jandrewjohnson/earth_economy_devstack . Alternatively you could use VS Code’s Command Pallate &lt;ctrl-shift-s&gt; Git Clone command and navigate to this repo. This repository configures VS Code to work well with the other EE repos and, for instance, defines launch-configurations that will always use the latest from github."
  },
  {
    "objectID": "organization.html#get-the-other-repositories",
    "href": "organization.html#get-the-other-repositories",
    "title": "Overall Organization",
    "section": "",
    "text": "Next, create a folder for each of the five repositories in the Files directory, as in the picture above.\n\nhazelbean\nseals\ngtappy\ngtap_invest\nglobal_invest\n\nInside each of these folders, you will clone the corresponding repositories:\n\nhttps://github.com/jandrewjohnson/hazelbean_dev\nhttps://github.com/jandrewjohnson/seals_dev\nhttps://github.com/jandrewjohnson/gtappy_dev\nhttps://github.com/jandrewjohnson/gtap_invest_dev\nhttps://github.com/jandrewjohnson/global_invest_dev\n\nIf successful, you will have a new folder with _dev postpended, indicating that it is the repository itself (and is the dev, i.e., not yet public, version of it). For GTAPPy it should look like this:\n\nAll code will be stored in the _dev repository. All files that you will generate when running the libraries, conversely, will be in a different Projects directory as above (This will be discussed more in the ProjectFlow section)."
  },
  {
    "objectID": "organization.html#launching-the-devstack-in-vs-code",
    "href": "organization.html#launching-the-devstack-in-vs-code",
    "title": "Overall Organization",
    "section": "",
    "text": "Navigate to the earth_economy_devstack directory. In there, you will find a file earth_economy_devstack.code-workspace (pictured below).\n\nDouble click it to load a preconfigured VS Code Workspace. You will know you have it all working if the explorer tab in VS Code shows all all SIX of the repositories."
  },
  {
    "objectID": "organization.html#launch-and-vs-code-configurations",
    "href": "organization.html#launch-and-vs-code-configurations",
    "title": "Overall Organization",
    "section": "",
    "text": "The earth_economy_devstack.code-workspace configures VS Code so that it includes all of the necessary repositories for EE code to run. Specifically, this means that if you pull the most recent version of each repository from Github, your code will all work together seamlessly. In addition to this file, you will see in the .vs_code directory there is a launch.json file. This file defines how to launch the python Debugger so that it uses the correct versions of the repositories. To launch a specific python file, first make it the active editor window, then open the debugger tab in VS Code’s left navbar, and in the Run and Debug dropdown box (pictured below) select the first run configuration, then hit the green Play triangle to launch the python file you had open. If all is setup correctly, your file you ran should be able to import all of the libraries in the devstack."
  },
  {
    "objectID": "project_flow.html",
    "href": "project_flow.html",
    "title": "Make consistent with projecflow_quickstart",
    "section": "",
    "text": "In many case, such as a standard GTAPPy run, we will iterate over different aggergations and scenarios (now renamed counterfactuals). This is done as expected with code like this:\n\nfor aggregation_label in p.aggregation_labels:\n     \n    for experiment_label in p.experiment_labels:\n        \n        for n_years_counter, ending_year in enumerate(p.years):\n            \n            if n_years_counter == 0:\n                starting_year = p.base_year\n            else:\n                starting_year = p.years[n_years_counter - 1]\n                \n            output_dir = p.get_path(os.path.join(aggregation_label, experiment_label, str(ending_year)))\n\nBut sometimes, this becomes DEEPLY nested and confusing. SEALS implements an API that reads these nested layers from a CSV. This is defined more fully in the SEALS user guide.\n\nfor index, row in p.scenarios_df.iterrows():\n    seals_utils.assign_df_row_to_object_attributes(p, row)\n    \n    if p.scenario_type != 'baseline':\n                            \n        for n_years_counter, ending_year in enumerate(p.years):\n\n            if n_years_counter == 0:\n                starting_year = p.base_year\n            else:\n                starting_year = p.years[n_years_counter - 1]\n                \n            current_run_dirs = os.path.join(p.exogenous_label, p.climate_label, p.model_label, p.counterfactual_label)\n            output_dir = p.get_path(current_run_dirs, str(ending_year))\n            expected_sl4_path = os.path.join(output_dir, p.counterfactual_label + '_Y' + str(ending_year) + '.sl4')\n\nIn this scenarios_df, which was loaded from scenarios_csv_path, there are multiple nested for loops implied, for p.exogenous_label, p.climate_label, p.model_label, p.counterfactual_label, each row has a unique value that would have been iterated over with the for loop above. Now, however, we are iterating just over scenario_df rows. Within each row pass, a project-level attribute is assigned via seals_utils.assign_df_row_to_object_attributes(p, row). This is used instead of the nested for loop."
  },
  {
    "objectID": "project_flow.html#iterating-over-many-model-assumptions",
    "href": "project_flow.html#iterating-over-many-model-assumptions",
    "title": "Make consistent with projecflow_quickstart",
    "section": "",
    "text": "In many case, such as a standard GTAPPy run, we will iterate over different aggergations and scenarios (now renamed counterfactuals). This is done as expected with code like this:\n\nfor aggregation_label in p.aggregation_labels:\n     \n    for experiment_label in p.experiment_labels:\n        \n        for n_years_counter, ending_year in enumerate(p.years):\n            \n            if n_years_counter == 0:\n                starting_year = p.base_year\n            else:\n                starting_year = p.years[n_years_counter - 1]\n                \n            output_dir = p.get_path(os.path.join(aggregation_label, experiment_label, str(ending_year)))\n\nBut sometimes, this becomes DEEPLY nested and confusing. SEALS implements an API that reads these nested layers from a CSV. This is defined more fully in the SEALS user guide.\n\nfor index, row in p.scenarios_df.iterrows():\n    seals_utils.assign_df_row_to_object_attributes(p, row)\n    \n    if p.scenario_type != 'baseline':\n                            \n        for n_years_counter, ending_year in enumerate(p.years):\n\n            if n_years_counter == 0:\n                starting_year = p.base_year\n            else:\n                starting_year = p.years[n_years_counter - 1]\n                \n            current_run_dirs = os.path.join(p.exogenous_label, p.climate_label, p.model_label, p.counterfactual_label)\n            output_dir = p.get_path(current_run_dirs, str(ending_year))\n            expected_sl4_path = os.path.join(output_dir, p.counterfactual_label + '_Y' + str(ending_year) + '.sl4')\n\nIn this scenarios_df, which was loaded from scenarios_csv_path, there are multiple nested for loops implied, for p.exogenous_label, p.climate_label, p.model_label, p.counterfactual_label, each row has a unique value that would have been iterated over with the for loop above. Now, however, we are iterating just over scenario_df rows. Within each row pass, a project-level attribute is assigned via seals_utils.assign_df_row_to_object_attributes(p, row). This is used instead of the nested for loop."
  },
  {
    "objectID": "project_flow.html#creating-scenarios-spreadsheets",
    "href": "project_flow.html#creating-scenarios-spreadsheets",
    "title": "Make consistent with projecflow_quickstart",
    "section": "Creating scenarios spreadsheets",
    "text": "Creating scenarios spreadsheets\nHere. Explain why it writes the scenarios_csv FROM CODE rather than downloading it (keeps it up to date as code changes quickly). However, this gets convoluted when you also have to initialize the attributes before you write?!?\n\n    # If you want to run SEALS with the run.py file in a different directory (ie in the project dir)\n    # then you need to add the path to the seals directory to the system path.\n    custom_seals_path = None\n    if custom_seals_path is not None: # G:/My Drive/Files/Research/seals/seals_dev/seals\n        sys.path.insert(0, custom_seals_path)\n\n    # SEALS will run based on the scenarios defined in a scenario_definitions.csv\n    # If you have not run SEALS before, SEALS will generate it in your project's input_dir.\n    # A useful way to get started is to to run SEALS on the test data without modification\n    # and then edit the scenario_definitions.csv to your project needs.\n    # Some of the other test files use different scenario definition csvs \n    # to illustrate the technique. If you point to one of these \n    # (or any one CSV that already exists), SEALS will not generate a new one.\n    # The avalable example files in the default_inputs include:\n    # - test_three_scenario_defininitions.csv\n    # - test_scenario_defininitions_multi_coeffs.csvs\n    \n    p.scenario_definitions_path = os.path.join(p.input_dir, 'scenario_defininitions.csv')\n\n    # Set defaults and generate the scenario_definitions.csv if it doesn't exist.\n    if not hb.path_exists(p.scenario_definitions_path):\n        # There are several possibilities for what you might want to set as the default.\n        # Choose accordingly by uncommenting your desired one. The set of\n        # supported options are\n        # - set_attributes_to_dynamic_default (primary one)\n        # - set_attributes_to_dynamic_many_year_default\n        # - set_attributes_to_default # Deprecated\n\n        gtap_invest_utils.set_attributes_to_dynamic_gtap_default(p) # Default option\n\n\n        # # Optional overrides for us in intitla scenarios\n        # p.aoi = 'RWA'\n\n        # gtap_invest_utils.set_attributes_to_dynamic_default(p)\n        # Once the attributes are set, generate the scenarios csv and put it in the input_dir.\n        gtap_invest_utils.generate_gtap_invest_scenarios_csv_and_put_in_input_dir(p)\n        p.scenarios_df = pd.read_csv(p.scenario_definitions_path)\n    else:\n        # Read in the scenarios csv and assign the first row to the attributes of this object (in order to setup additional \n        # project attributes like the resolutions of the fine scale and coarse scale data)\n        p.scenarios_df = pd.read_csv(p.scenario_definitions_path)\n\n        # Because we've only read the scenarios file, set the attributes\n        # to what is in the first row.\n        for index, row in p.scenarios_df.iterrows():\n            seals_utils.assign_df_row_to_object_attributes(p, row)\n            break # Just get first for initialization."
  },
  {
    "objectID": "project_flow.html#task-format",
    "href": "project_flow.html#task-format",
    "title": "Make consistent with projecflow_quickstart",
    "section": "Task Format",
    "text": "Task Format\nProject flow requires a consistent format for tasks. The following is an example of a task that creates a correspondence file from gtap11 regions to gtapaez11 regions. The task itself defined as a function that takes a p object as an argument. This p object is a ProjectFlow object that contains all the project-level variables, manages folders and files, and manages tasks and parallelization. p also includes documentation, which will be written directly into the task directory.\nAlso note that any project-level attribute defined in between the function start and the if p.run_this: component are the “project level variables” that are fair-game for use in other tasks. These paths are critical for high performance because they enable quick-skipping of completed tasks and determiniation of which parts of the task tree need rerunning.\nTasks should be named as a noun (this breaks Python pep8 style) referencing what will be stored in the tasks output dir. This might feel awkward at first, but it means that the resultant file structure is easier to interpret by a non-EE outsider.\n\ndef gtap_aez_seals_correspondences(p):\n    p.current_task_documentation = \"\"\"\n    Create correspondence CSVs from ISO3 countries to GTAPv11 160\n    regions, and then to gtapaezv11 50ish regions, also put the classification\n    for seals simplification and luh.  \n    \"\"\"\n    p.gtap11_region_correspondence_input_path = os.path.join(p.base_data_dir, 'gtappy', 'aggregation_mappings', 'GTAP-ctry2reg.xlsx')\n    p.gtap11_region_names_path = os.path.join(p.base_data_dir, 'gtappy', 'aggregation_mappings', 'gtap11_region_names.csv')\n    p.gtap11_gtapaez11_region_correspondence_path = os.path.join(p.base_data_dir, 'gtappy', 'aggregation_mappings', 'gtap11_gtapaez11_region_correspondance.csv')    \n\n    if p.run_this:\n        \n        \"logic here\""
  },
  {
    "objectID": "project_flow.html#automatic-directory-organization-via-tasks",
    "href": "project_flow.html#automatic-directory-organization-via-tasks",
    "title": "Make consistent with projecflow_quickstart",
    "section": "Automatic Directory Organization via Tasks",
    "text": "Automatic Directory Organization via Tasks\nHazelbean automatically defines directory organization as a function of the task tree. When the ProjectFlow object is created, it takes a directory as its only required input. This directory defines the root of the project. The other directory that needs to be referenced is the base_data_dir. When you initialize the p object, it notes this:\nCreated ProjectFlow object at C:\\Users\\jajohns\\Files\\gtap_invest\\projects\\cwon     from script C:\\Users\\jajohns\\Files\\gtap_invest\\gtap_invest_dev\\gtap_invest\\run_cwon.py     with base_data set at C:\\Users\\jajohns\\Files/base_data\nIn the run file, the following line generates the task tree:\ngtap_invest_initialize_project.build_extract_and_run_aez_seals_task_tree(p)\nWhich points to a builder function in the initialize file, looking something like this:\n\nThis would generate the following task tree:\n\nTwo notations are especially useful within this task tree.\n\nWithin the function that defines a task, p.cur_dir points to the directory of that task. So for instance, the last task defined in the image above, in its code, you could reference p.cur_dir, and it would point to &lt;project_root&gt;/econ_visualization/econ_lcovercom\nOutside of a given function’s code, you can still refer to paths that were defined from within the functions code, but now (because you are outside the function) it is given a new reference. Using the example above, you could reference the same directory with p.econ_lcovercom_dir where the p attribute is named exactly as &lt;function_name&gt;_dir\n\nAll of this setup enable another useful feature: automatic management of file generation, storage and downloading. This is done via the hazelbean function:\n\nuseful_path = hb.get_path(relative_path)\n\nThis function will iteratively search multiple locations and return the most “useful” one. By default, the relative_path variable will first joined with the p.cur_dir. If the file exists, it returns it. If not, it checks the next location, which is p.input_dir, and then p.base_data_dir. If it doesn’t find it anywhere, it will attempt to download it from google cloud (NYI) and save it in the p.cur_dir. If it is not available to download on google cloud, then it treats the path as something we will be generating within the task, and thus, get_path returns the first option above, namely joining the relative_path with p.cur_dir.\nOne important use-case that needs explaining is for tasks that generate files that will eventually be placed in the base_data_dir. The goal is to enable easy generation of it to the intermediate directory in the appropriate task_dir, but then have the ability to copy the files with the exact same relative path to the base_data_dir and have it still be found by p.get_path(). To do this, you will want to choose a path name relative to the tasks’ cur_dir that matches the desired directory relative to the base data dir. So, for example, we include 'gtappy', 'aggregation_mappings' at the beginning of the relative path for in the intermediate directory in the appropriate task_dir, but then we also will want to copy the files with the exact same relative path to the base_data_dir and have it still be found by p.get_path(). To do this, you will want to choose a path name relative to the tasks’ cur_dir that matches that in the base_data_dir, for example &lt;base_data_dir&gt;/'gtappy/aggregation_mappings/gadm_adm0.gpkg',\n\ntemplate_path = p.get_path(os.path.join('gtappy', 'aggregation_mappings', 'gadm_adm0.gpkg')) \n\nIt can be hard deciding what counts as a base_data_generating task or not, but generally if it is a file that will not be used by other projects, you should not treat it as a base_data_generating task. Instead, you should just make it relative to the cur_dir (or wahtever makes sense), as below:\n\noutput_path = p.get_path(os.path.join(aggregation_label + '_' + experiment_label + '_' + header + '_stacked_time_series.csv'))\n\nOne additional exception to the above is if you are calling get_path outside of a task/task_tree. One common example is in the run file before you build the task tree. In this case, the default_dirs will not make sense, and so you need to specify it manually as here:\n\np.countries_iso3_path = p.get_path(os.path.join('cartographic', 'gadm', 'gadm_adm0_10sec.gpkg'), possible_dirs=[p.input_dir, p.base_data_dir])"
  },
  {
    "objectID": "project_flow.html#validation-of-files",
    "href": "project_flow.html#validation-of-files",
    "title": "Make consistent with projecflow_quickstart",
    "section": "Validation of files",
    "text": "Validation of files\nProjectFlow is designed to calculate very fast while simultaneously validating that everything is approximately correct. It does this by checking for the existence of files (often combined with hb.get_path()). For example\n\np.gadm_r263_gtapv7_r251_r160_r50_correspondence_vector_path = p.get_path(os.path.join('gtap_invest', 'region_boundaries', 'gadm_r263_gtapv7_r251_r160_r50_regions.gpkg'))     \nif not hb.path_exists(p.gadm_r263_gtapv7_r251_r160_r50_correspondence_vector_path):         \n    hb.log('Creating ' + p.gadm_r263_gtapv7_r251_r160_r50_correspondence_vector_path)         \n    \"computationally expensive thing here.\"\n\nProjectFlow very carefully defines whether or not you should run something based on the existence of specific paths. Usually this is just checking for each written path and only executing the code if it’s missing, but in some cases where lots of files are created, it’s possible to take the shortcut of just checking for the existence of the last-created path.\n\nEliminating redundant calculation across projects\nIf you have a time consuming task that, or example, writes to\n\nbig_file_path = hb.get_path('lulc', 'esa', 'seals7', 'convolutions', '2017', 'convolution_esa_seals7_2017_cropland_gaussian_5.tif' )\n\nIn this example, suppose you needed to create this file via your create_convolutions() task or something. When you first do this, it obviously won’t exist yet, so get_path() will join that relative path in the p.cur_dir location. If you run the ProjectFlow again, it will see it’s there and then instantly skip recalcualting it.\nIn addition to the 5 repos plus the EE repo, there is a managed base data set stored in teh same location\n\nA ProjectFlow object must have a base_data_dir set (I think…). This is because the p.get_path() will look in this folder for it, and/or will download to it.\n\n\n\nPython Tips and Conventions\nFor large files that take a long time to load, use a string-&gt;dataframe/dataset substitution as below. Make a LOCAL variable to contain the loaded object, and have that be assigned to the correct project-level path string. In subsequent usages, check type and if it’s still a string, then it hasn’t been loaded yet, so do that. I’m debating making it a project level variable trick\n\ngadm = p.gadm_adm0_vector_input_path    \n\n# Just load it on first pass\nif type(gadm) is str:\n    gadm = hb.read_vector(p.gadm_adm0_vector_input_path)"
  },
  {
    "objectID": "qtap_invest.html",
    "href": "qtap_invest.html",
    "title": "GTAP-InVEST",
    "section": "",
    "text": "GTAP-InVEST\nNYI."
  },
  {
    "objectID": "seals.html",
    "href": "seals.html",
    "title": "SEALS",
    "section": "",
    "text": "SEALS is under active development and will change much as it moves from a personal research library to supported model. We have submitted this code for publication but are waiting on reviews. For installation and other details, see the SEALS documentation.\nTo run a minimal version of the model, open a terminal/console and navigate to the directory where run_test_seals.py is located. Then, simply run: &gt; python run_test_seals.py\nIn order for the above line to work, you will need to set the project directory and data directory lines in run_test_seals.py. To obtain the base_data necessary, see the SEALS manuscript for the download link.\nTo run a full version of the model, copy run_test_seals.py to a new file (i.e., run_seals.py) and set p.test_mode = False. You may also want to specify a new project directory to keep different runs separate.\n\nimages/2024-01-18-08-25-09.png"
  },
  {
    "objectID": "seals.html#from-readme-might-be-redundant",
    "href": "seals.html#from-readme-might-be-redundant",
    "title": "SEALS",
    "section": "",
    "text": "SEALS is under active development and will change much as it moves from a personal research library to supported model. We have submitted this code for publication but are waiting on reviews. For installation and other details, see the SEALS documentation.\nTo run a minimal version of the model, open a terminal/console and navigate to the directory where run_test_seals.py is located. Then, simply run: &gt; python run_test_seals.py\nIn order for the above line to work, you will need to set the project directory and data directory lines in run_test_seals.py. To obtain the base_data necessary, see the SEALS manuscript for the download link.\nTo run a full version of the model, copy run_test_seals.py to a new file (i.e., run_seals.py) and set p.test_mode = False. You may also want to specify a new project directory to keep different runs separate.\n\nimages/2024-01-18-08-25-09.png"
  },
  {
    "objectID": "seals.html#release-notes",
    "href": "seals.html#release-notes",
    "title": "SEALS",
    "section": "Release Notes",
    "text": "Release Notes\n\nUpdate v0.5.0\nDownloading of base data now works.\n\n\nUpdate v0.4.0\nNow all project flow objects can be set via a scenario_definitions.csv file, allowing for iteration over multiple projects.\nIf no scenario_definitions.csv is present, it will create the file based on the parameters set in the run file."
  },
  {
    "objectID": "seals.html#project-flow",
    "href": "seals.html#project-flow",
    "title": "SEALS",
    "section": "Project Flow",
    "text": "Project Flow\nOne key component of Hazelbean is that it manages directories, base_data, etc. using a concept called ProjectFlow. ProjectFlow defines a tree of tasks that can easily be run in parallel where needed and keeping track of task-dependencies. ProjectFlow borrows heavily in concept (though not in code) from the task_graph library produced by Rich Sharp but adds a predefined file structure suited to research and exploration tasks."
  },
  {
    "objectID": "seals.html#first-run-walkthrough-tutorial",
    "href": "seals.html#first-run-walkthrough-tutorial",
    "title": "SEALS",
    "section": "First run walkthrough tutorial",
    "text": "First run walkthrough tutorial\nThe simplest way to run SEALS is to clone the repository and then open run_test_seals.py in your preferred editor. Then, update the values in ENVIRONMENT SETTINGS near the top of run_test_seals.py for your local computer (ensuring this points to directories you have write-access for and is not a virtual/cloud directory).\n\n    ### ------- ENVIRONMENT SETTINGS -------------------------------\n\n    # Users should only need to edit lines in this ENVIRONMENT SETTINGS section\n    # Everything is relative to these (or the source code dir).\n    # Specifically,\n    # 1. ensure that the project_dir makes sense for your machine\n    # 2. ensure that the base_data_dir makes sense for your machine\n    # 3. ensure that the data_credentials_path points to a valid credentials file\n    # 4. ensure that the input_bucket_name points to a cloud bucket you have access to\n\n    # A ProjectFlow object is created from the Hazelbean library to organize directories and enable parallel processing.\n    # project-level variables are assigned as attributes to the p object (such as in p.base_data_dir = ... below)\n    # The only agrument for a project flow object is where the project directory is relative to the current_working_directory.\n    user_dir = os.path.expanduser('~')\n    script_dir = os.path.dirname(os.path.realpath(__file__))\n\n    project_name = 'test_seals_project'\n    project_dir = os.path.join(user_dir,  'seals', 'projects', project_name)\n    p = hb.ProjectFlow(project_dir)\n\nThe project name and the project dir will define the root directory where all files will be saved. This directory is given hb.ProjectFlow() to initalize the project (which will create the dirs). Once these are set, you should be able to run run_test_seals.py in your preferred way, ensuring that you are in the Conda environment discussed above. This could be achieved in VS Code by selecting the Conda environment in the bottom-right status bar and then selecting run. Alternatively, this could be done via the command line with the command python run_test_seals.py in the appropriate directory.\nWhen SEALS is run in this way, it will use the default values for a test run on a small country (Rawanda). All of these values are set (and documented) in the run file (run_test_seals.py) in the SET DEFAULT VARIABLES section. For your first run, it is recommended to use the defaults. When run, a configuration file will be written into your project’s input_dir named scenario_definitions.csv. This file is a table where each row is a “scenario” necessary to be defined for SEALS to run. In this minimal run, it must have 2 rows: one for the baseline condition (the starting LULC map) and one for a scenario of change that will indicate how much change of each LU class will happen in some coarse grid-cell or region/zone. Inspecting and/or modifying this file may give insights on how to customize a new run.\n\n### ------- SET DEFAULT VARIABLES --------------------------------\n\n# Set the path to the scenario definitions file. This is a CSV file that defines the scenarios to run.\n# If this file exists, it will load all of the attributes from this file and overwrite the attributes\n# set above. This is useful because adding new lines to to the scenario definitions file will allow\n# you to run many different scenarios easily. If this file does not exist, it will be created based\n# on the attributes set above and saved to the location in scenarios_definitions_path.\np.scenario_definitions_path = os.path.join(p.input_dir, 'scenario_defininitions.csv')\n\n# IMPORTANT NOTE: If you set a scenario_definitions_path, then the attributes set in this file (such as p.scenario_label below)\n# will be overwritten. Conversely, if you don't set a scenario_definitions_path, then the attributes set in this file will be used\n# and will be written to a CSV file in your project's input dir.\n\n# If you did not set a p.scenarios_definitions_path, the following default variables will be used\n# and will be written to a scenarios csv in your project's input_dir for later use/editing/expansion.\n\n# String that uniquely identifies the scenario. Will be referenced by other scenarios for comparison.\np.scenario_label = 'ssp2_rcp45_luh2-globio_bau'\n\n# Scenario type determines if it is historical (baseline) or future (anything else) as well\n# as what the scenario should be compared against. I.e., Policy minus BAU.\np.scenario_type = 'bau'\n\nThis computing stack also uses hazelbean to automatically download needed data at run time. In the code block below, notice the absolute path assigned to p.base_data_dir. Hazelbean will look here for certain files that are necessary and will download them from a cloud bucket if they are not present. This also lets you use the same base data across different projects.\nIn addition to defining a base_data_dir, you will need to For this to work, you need to also point SEALS to the correct data_credentials_path. If you don’t have a credentils file, email jajohns@umn.edu. The data are freely available but are very, very large (and thus expensive to host), so I limit access via credentials.\n\np.base_data_dir = os.path.join('G:/My Drive/Files/base_data')\n\np.data_credentials_path = '..\\\\api_key_credentials.json'\n\nNOTE THAT the final directory has to be named base_data to match the naming convention on the google cloud bucket."
  },
  {
    "objectID": "seals.html#running-the-model",
    "href": "seals.html#running-the-model",
    "title": "SEALS",
    "section": "Running the model",
    "text": "Running the model\nAfter doing the above steps, you should be ready to run run_test_seals.py. Upon starting, SEALS will report the “task tree” of steps that it will compute in the ProjectFlow environment. To understand SEALS in more depth, inspect each of the functions that define these tasks for more documention in the code.\nOnce the model is complete, go to your project directory, and then the intermediate directory. There you will see one directory for each of the tasks in the task tree. To get the final produce, go to the stitched_lulc_simplified_scenarios directory. There you will see the base_year lulc and the newly projected lulc map for the future year:\n [THIS IS NOT THE CORRECT IMAGE]\nOpen up the projected one (e.g., lulc_ssp2_rcp45_luh2-message_bau_2045.tif) in QGIS and enjoy your new, high-resolution land-use change projection!"
  }
]